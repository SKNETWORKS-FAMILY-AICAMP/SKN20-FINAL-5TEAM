# AI 코치 고도화 비교 분석

> 기준: `coach_view.py` (원본 v1) → 두 가지 고도화 방향 비교
> 작성일: 2026-02-24

---

## 1. 원본 (coach_view.py) 요약

| 항목 | 내용 |
|------|------|
| 패턴 | 단일 System Prompt + ReAct Loop (tool_choice="auto") |
| 도구 | 4개 (get_user_scores, get_weak_points, get_recent_activity, recommend_next_problem) |
| LLM 호출 | 1-Stage (하나의 프롬프트로 의도 파악 + 도구 호출 + 응답 생성 전부 수행) |
| 프롬프트 | 300~700자 범위의 단일 SYSTEM_PROMPT, 3파트 구조 강제 |
| 파일 구조 | 단일 파일 (501줄) — 프롬프트, 도구, 뷰 전부 한 파일 |
| 가드레일 | 없음 (모든 질문이 LLM으로 전달) |
| max_iterations | 5 |

---

## 2. 고도화 A: Enhanced 버전 (`coach_view_enhanced.py`)

### 핵심 전략: **"LLM이 의도를 분류하고, 의도에 맞는 전략으로 응답"**

| 항목 | 내용 |
|------|------|
| 패턴 | **Two-Stage LLM**: Intent Analysis → Response Strategy |
| 도구 | 4개 (원본과 동일) |
| LLM 호출 | 2-Stage — 1차: 의도 분류 (별도 LLM 호출), 2차: 전략별 프롬프트로 응답 생성 |
| 프롬프트 | **7개 전략별 System Prompt** (A~G, 각 300~800자) |
| 파일 구조 | 단일 파일 (1,169줄) — 원본 대비 2.3배 증가 |
| 가드레일 | 없음 (D유형을 LLM이 판정) |
| max_iterations | **10** (원본 대비 2배) |

### 추가된 핵심 기능

1. **Intent Analysis (7가지 유형)**
   - A: 데이터 조회형, B: 학습 방법형, C: 동기부여형, D: 범위 밖
   - E: 문제 풀이 지원, F: 개념 설명, G: 성과 비교 & 의사결정
   - LLM이 JSON으로 `intent_type`, `confidence`, `reasoning`, `key_indicators` 반환

2. **Intent별 도구 필터링 & 필수 도구 강제 호출**
   - `INTENT_TOOL_MAPPING`: 의도별로 허용/필수 도구 지정
   - 필수 도구 미호출 시 `tool_choice="required"` 강제
   - `ToolResultEvaluator` 클래스로 충분도 평가

3. **AI 피드백 통합 분석**
   - `_extract_feedback()`: submitted_data에서 AI 평가 피드백 추출
   - 점수 + 피드백을 함께 분석하여 설득력 강화
   - 유닛별 피드백 매핑 (키워드 기반 차원 분류)

4. **도구 결과 캐싱**
   - `called_tools_cache`: 동일 인자 중복 호출 방지
   - `validate_and_normalize_args()`: 인자 검증 및 기본값 적용

5. **향상된 SSE 이벤트**
   - `intent_detected`: 의도 분류 결과 클라이언트 전달 (투명성)
   - `final`: 응답 완료 시 메타정보 (intent_type, strategy) 전달
   - `warning`: 필수 도구 미호출 시 경고

---

## 3. 고도화 B: Compare 버전 (`compare/` 폴더)

### 핵심 전략: **"도구를 풍부하게, 프롬프트를 똑똑하게, 가드레일을 엄격하게"**

| 항목 | 내용 |
|------|------|
| 패턴 | **단일 System Prompt + ReAct Loop** (원본과 동일 구조) |
| 도구 | **6개** (원본 4 + get_unit_curriculum + get_study_guide) |
| LLM 호출 | 1-Stage (원본과 동일) |
| 프롬프트 | 단일 SYSTEM_PROMPT이지만, **질문 유형별 응답 전략(A~E)을 프롬프트 안에 기술** |
| 파일 구조 | **4개 파일 분리** — coach_view.py, coach_prompt.py, coach_tools.py, AICoach.vue |
| 가드레일 | **규칙 기반 사전 필터링** (is_off_topic: 키워드 매칭 + 정규식) |
| max_iterations | 5 (원본과 동일) |

### 추가된 핵심 기능

1. **규칙 기반 가드레일 (LLM 호출 전 차단)**
   - `_LEARNING_KEYWORDS` (37개): 학습 관련 키워드
   - `_OFF_TOPIC_KEYWORDS` (17개): 범위 밖 키워드
   - `_ABUSIVE_KEYWORDS` (11개): 욕설/비속어 차단
   - `_NONSENSE_RE`: 의미 없는 입력 정규식 필터
   - 효과: **불필요한 LLM 호출 비용 절감** + 즉시 응답

2. **2개의 새로운 도구**
   - `get_unit_curriculum`: 유닛별 학습 목표, 핵심 개념, 스킬, 공부 팁 반환
   - `get_study_guide`: 약점 기반 **맞춤 학습 가이드** 생성 (내부에서 get_weak_points 호출)

3. **정적 학습 데이터 (Knowledge Base)**
   - `UNIT_CURRICULUM`: 유닛별 핵심 개념, 스킬, 난이도, 공부 팁 (총 94줄)
   - `STUDY_GUIDE_MAP`: 메트릭별 개념 + 구체적 가이드 (총 63줄)
   - `UNIT_OVERALL_TIPS`: 유닛별 한줄 요약 팁
   - 효과: LLM이 **학습 방법 질문에 도메인 지식을 주입받아** 더 정확한 답변 가능

4. **개선된 프롬프트 설계**
   - 질문 유형 A~E를 프롬프트 안에 명시 (LLM이 스스로 판단)
   - **핵심 규칙 추가**: "도구를 호출할 필요가 없는 질문이라도 최소 get_user_scores를 호출하여 수준 파악"
   - "3파트 강제 아님" — 유형별 자유로운 구조

5. **향상된 SSE + 프론트엔드**
   - `status` 이벤트 (fetching/ready/blocked 3종)
   - `content_started` 플래그로 답변 생성 시작 알림
   - AICoach.vue 포함: 프리셋 버튼 6개, 토큰 디스플레이 큐 (부드러운 출력)

6. **모듈화된 파일 구조**
   - `coach_prompt.py`: 프롬프트 + 가드레일 로직 (115줄)
   - `coach_tools.py`: 도구 정의 + 실행 함수 + 학습 데이터 (468줄)
   - `coach_view.py`: 뷰 로직만 (217줄)
   - `AICoach.vue`: 프론트엔드 컴포넌트 (833줄)

---

## 4. 핵심 차이점 비교표

| 비교 항목 | Enhanced (A) | Compare (B) |
|-----------|-------------|------------|
| **의도 분류 방식** | LLM 기반 (별도 호출) | 프롬프트 내 가이드 + 규칙 기반 가드레일 |
| **LLM 호출 횟수 (최소)** | 2회 (의도분석 + 응답) | 1회 |
| **도구 개수** | 4개 (원본 동일) | 6개 (+2 신규) |
| **학습 도메인 지식** | 없음 (LLM 일반 지식에 의존) | 풍부 (UNIT_CURRICULUM + STUDY_GUIDE_MAP) |
| **가드레일** | LLM 의존 (D유형 판정) | 규칙 기반 사전 차단 (비용 절감) |
| **프롬프트 전략** | 7개 분리된 전략별 프롬프트 | 1개 통합 프롬프트 (내부에 유형별 가이드) |
| **AI 피드백 활용** | O (feedback_samples 추출) | X |
| **도구 결과 캐싱** | O (called_tools_cache) | X |
| **인자 검증** | O (validate_and_normalize_args) | X |
| **필수 도구 강제** | O (tool_choice="required") | X |
| **충분도 평가** | O (ToolResultEvaluator) | X |
| **프론트엔드** | 미포함 (백엔드만) | 포함 (AICoach.vue) |
| **파일 구조** | 단일 파일 (1,169줄) | 4파일 분리 (총 1,633줄) |
| **max_iterations** | 10 | 5 |
| **코드 복잡도** | 높음 (상태 머신 + 캐시 + 검증) | 중간 (원본 구조 유지) |

---

## 5. 비판적 분석

### 5.1 Enhanced (A)에 대한 비판

**장점:**
- 의도 분류의 정확도가 높아 맞춤형 응답 가능 (85~95% 정확도 주장)
- 필수 도구 강제 호출로 "데이터 없는 답변" 방지
- AI 피드백 통합으로 "점수만 보는 분석"에서 "구체적 피드백 인용 분석"으로 진화
- 투명성: 클라이언트에 의도 분류 결과를 공개하여 신뢰 구축

**단점:**
1. **레이턴시 & 비용 증가**: 매 질문마다 의도 분석용 LLM 호출이 추가됨. 간단한 질문("내 점수 보여줘")에도 2회 호출 필수 → 응답 시간 1.5~2배 증가, API 비용 증가
2. **단일 파일 1,169줄**: 프롬프트 7개 + 도구 + 뷰 + 검증 + 캐시 + 평가 로직이 한 파일에 밀집 → 유지보수 극히 어려움. 프롬프트 하나 수정하려면 1,169줄 파일을 탐색해야 함
3. **도구 확장 없음**: 원본과 동일한 4개 도구만 사용. "학습 방법" 질문(B유형)에 대해 별도 도구가 없어 LLM 일반 지식에 의존 → 답변 품질이 도메인 특화되지 않음
4. **max_iterations=10 위험**: 최악의 경우 10회 연속 LLM 호출 → 타임아웃, 비용 폭발 가능. 실제로 필수 도구 강제 + 충분도 미달 반복 시 무한 루프에 가까운 상황 발생 가능
5. **가드레일 부재**: 욕설, 무의미한 입력, 명백한 범위 밖 질문도 LLM으로 전달 → 불필요한 비용 발생. D유형 판정을 LLM에 의존하므로 가끔 잘못 분류될 수 있음
6. **피드백 매핑의 취약성**: `_extract_feedback()`의 키워드 기반 차원 매핑이 하드코딩됨. "설계"라는 단어가 포함되면 무조건 "design" 차원으로 분류 → 오분류 가능성 있음. 매칭되지 않는 피드백은 모든 차원에 복제됨 (노이즈)
7. **의도 분류 실패 시 폴백**: JSON 파싱 실패 시 무조건 B(학습 방법형)로 가정 → 데이터 조회형 질문이 학습 방법형으로 처리될 수 있음

### 5.2 Compare (B)에 대한 비판

**장점:**
- 모듈화된 구조로 유지보수 용이 (각 파일이 명확한 책임)
- 규칙 기반 가드레일로 불필요한 LLM 호출 차단 (비용 절감)
- 2개 신규 도구(curriculum, study_guide)로 학습 방법 질문에 도메인 특화 답변 가능
- 정적 학습 데이터(STUDY_GUIDE_MAP)로 일관된 품질의 학습 가이드 보장
- 프론트엔드까지 포함한 완결성

**단점:**
1. **의도 분류의 정확도 한계**: LLM이 프롬프트 안의 A~E 가이드를 보고 스스로 판단 → Enhanced의 명시적 분류 대비 정확도 낮음. 특히 복합 질문("약점 분석하고 공부법도 알려줘")에서 어느 전략을 따를지 불분명
2. **가드레일의 경직성**: 키워드 매칭 기반이라 우회 가능. "학습이랑 관계없는데 날씨 좋다"처럼 학습 키워드가 포함된 범위 밖 질문은 통과시킴. 반대로, 새로운 학습 관련 용어가 키워드 목록에 없으면 차단될 수 있음
3. **AI 피드백 미활용**: submitted_data의 풍부한 AI 평가 피드백(improvements, senior_advice, step_feedbacks)을 전혀 활용하지 않음 → 점수만 보는 피상적 분석에 머무름
4. **도구 결과 캐싱/검증 부재**: get_study_guide가 내부에서 get_weak_points를 호출하므로, LLM이 get_weak_points → get_study_guide 순서로 호출하면 DB 쿼리 중복. 인자 검증 없이 잘못된 unit_id가 전달되면 빈 결과 반환
5. **정적 데이터의 한계**: STUDY_GUIDE_MAP은 사전 정의된 가이드이므로 개인화 부족. "디버깅_정확도가 낮다"는 모든 학생에게 동일한 가이드 제공 → 학생별 맞춤화 불가
6. **프롬프트 유형 5개 vs Enhanced 7개**: E(문제 풀이 지원), F(개념 설명), G(성과 비교)에 대한 명시적 전략 부재. 이 유형의 질문은 LLM 재량에 맡김
7. **max_iterations=5 제한**: 도구 6개를 5회 안에 모두 활용하기 어려울 수 있음. 특히 curriculum + study_guide + scores + weak_points를 순차 호출하면 거의 한계에 도달

---

## 6. 종합 평가

### 설계 철학의 차이

| | Enhanced (A) | Compare (B) |
|---|---|---|
| **철학** | "LLM에게 더 많은 판단을 맡기자" | "LLM에게 더 좋은 재료를 주자" |
| **복잡도 투자** | Agent 오케스트레이션 레이어 (의도분석, 충분도평가, 캐시) | 도메인 지식 레이어 (커리큘럼, 가이드맵, 가드레일) |
| **품질 향상 방식** | 프롬프트 분리로 각 유형 최적화 | 도구 확장으로 LLM에게 더 풍부한 컨텍스트 제공 |

### 어떤 방향이 더 나은가?

**Enhanced (A)가 더 나은 상황:**
- 질문 유형이 매우 다양하고 각 유형에 근본적으로 다른 톤/구조가 필요할 때
- 의도 분류 정확도가 응답 품질에 결정적 영향을 미칠 때
- API 비용보다 응답 품질이 더 중요할 때

**Compare (B)가 더 나은 상황:**
- 레이턴시와 비용이 중요할 때 (1-Stage로 더 빠름)
- 학습 도메인 지식이 풍부할수록 답변 품질이 올라가는 경우
- 팀 작업 시 코드 모듈화/유지보수가 중요할 때
- 프론트엔드까지 포함한 완결된 기능이 필요할 때

### 이상적인 방향 (A + B 결합)

두 고도화의 장점을 결합하면 가장 이상적:

1. **Compare의 모듈화 구조** + **Enhanced의 Two-Stage 패턴**
2. **Compare의 가드레일** (규칙 기반 사전 필터링) → 비용 절감
3. **Compare의 확장된 도구** (curriculum, study_guide) → 도메인 지식 주입
4. **Enhanced의 의도 분류** → 맞춤형 프롬프트 선택
5. **Enhanced의 AI 피드백 통합** → 깊이 있는 분석
6. **Enhanced의 캐싱/검증** → 안정성 확보

```
[입력] → [규칙 기반 가드레일 (B)] → [의도 분석 LLM (A)]
       → [의도별 프롬프트 (A) + 확장 도구 6개 (B)]
       → [AI 피드백 통합 (A) + 학습 가이드 데이터 (B)]
       → [캐싱/검증 (A)] → [SSE 스트리밍]
```

---

## 7. 코드 품질 비교

| 항목 | Enhanced (A) | Compare (B) |
|------|-------------|------------|
| 단일 책임 원칙 (SRP) | X (1파일에 모든 것) | O (3파일 분리) |
| 테스트 용이성 | 낮음 (밀결합) | 높음 (각 모듈 독립 테스트 가능) |
| 확장성 | 의도/전략 추가 시 1파일 수정 | 도구/프롬프트 독립 확장 |
| 에러 핸들링 | 상세 (ValueError 분리, 경고 이벤트) | 기본 (try/except만) |
| 로깅 | 상세 (debug/info/warning 레벨 분리) | 기본 (error만) |
| 코드 중복 | 도구 함수 원본 복사 | 도구 함수 원본 복사 (동일) |
