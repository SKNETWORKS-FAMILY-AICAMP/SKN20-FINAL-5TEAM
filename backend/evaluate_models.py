import os
import time
import json
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    print("âŒ í™˜ê²½ ë³€ìˆ˜ 'OPENAI_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)

client = OpenAI(api_key=api_key)

# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡
# ì°¸ê³ : o1-preview, o1-mini ë“±ì€ OpenAIì˜ ìµœì‹  ì¶”ë¡ (Reasoning) íŠ¹í™” ëª¨ë¸ë¡œ ì†Œìœ„ 'GPT-5ê¸‰' ì§€ëŠ¥ì„ ê°€ì§‘ë‹ˆë‹¤.
# ë‹¨, o1 ê³„ì—´ ëª¨ë¸ì€ í˜„ì¬ streaming íŒŒë¼ë¯¸í„° ì œì•½ì´ ìˆê±°ë‚˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì§€ì› ë°©ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´
# ì¼ë°˜ì ì¸ ChatCompletion ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ì—ëŠ” gpt-4o ê³„ì—´ì„ ìš°ì„  ì‚¬ìš©í•˜ê³ ,
# o1 ê³„ì—´ì€ ì§€ì› ì—¬ë¶€ì— ë”°ë¼ ë³„ë„ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
MODELS_TO_TEST = [
    "gpt-4o-mini",       # í˜„ì¬ ì €í¬ê°€ ì“°ëŠ” ì´ˆê³ ì†/ê°€ì„±ë¹„ ëª¨ë¸
    "gpt-4o",            # ë†’ì€ ì§€ëŠ¥ê³¼ ì¤€ìˆ˜í•œ ì†ë„ë¥¼ ê°€ì§„ í”Œë˜ê·¸ì‹­ ëª¨ë¸
    "gpt-5-mini",           # GPT-5ê¸‰ ì¶”ë¡  ëª¨ë¸ (ì‹¬ì¸µ ì‚¬ê³ , ê²½ëŸ‰)
    "gpt-5.2",        # GPT-5ê¸‰ ì¶”ë¡  ëª¨ë¸ (ìµœê³  ìˆ˜ì¤€ ì§€ëŠ¥)
]

# í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ í”„ë¡¬í”„íŠ¸ (ê°€í˜¹í•œ ì¡°ê±´)
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ìˆ˜ì„ ê¸°ìˆ  ë©´ì ‘ê´€ 'ë„ë•'ì…ë‹ˆë‹¤. 
ë‹¨ í•œ ê°œì˜ ê¼¬ë¦¬ ì§ˆë¬¸ë§Œ ì˜ˆë¦¬í•˜ê²Œ ìƒëµ ì—†ì´ ë˜ì§€ì„¸ìš”.
[ì§€ì›ì ê³¼ê±° ì½”ë“œ]: íŒŒì´ì¬ìœ¼ë¡œ BFS íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ì§°ìœ¼ë‚˜, ë°©ë¬¸ ì²˜ë¦¬(visited) ë¡œì§ì´ ì™„ì „ ëˆ„ë½ë˜ì–´ ë¬´í•œ ë£¨í”„ ìœ„í—˜ì´ ìˆìŒ.
[ì§€ì› íšŒì‚¬ ìš”ê±´]: ëŒ€ê·œëª¨ íŠ¸ë˜í”½ ì²˜ë¦¬ ê²½í—˜ê³¼ ë©”ëª¨ë¦¬ ìµœì í™” ì—­ëŸ‰.

ìœ„ ì½”ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”."""

USER_PROMPT = "ì§€ì›ìê°€ ë°©ê¸ˆ ì…ì¥í–ˆìŠµë‹ˆë‹¤. ê°€ë³ê²Œ ì¸ì‚¬í•˜ê³  ì¦‰ì‹œ ê³¼ê±° ì½”ë“œì˜ ì•½ì ì„ ì§€ì í•˜ëŠ” ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”."

def run_benchmark():
    print("=" * 60)
    print("ğŸš€ ì‹¤ì‹œê°„ ìŒì„± ë©´ì ‘ íƒ€ë‹¹ì„± ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ğŸš€")
    print("=" * 60)
    print("ì¸¡ì • ì§€í‘œ:")
    print("1. TTFT (Time To First Token): ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ ë‚˜ì˜¤ê¸°ê¹Œì§€ ê±¸ë¦° ì‹œê°„ (ê°€ì¥ ì¤‘ìš”!)")
    print("2. Total Time: ì „ì²´ ë‹µë³€ì´ ì™„ì„±ë˜ê¸°ê¹Œì§€ ê±¸ë¦° ì‹œê°„")
    print("3. Response Quality: ë°˜í™˜ëœ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©\n")

    for model_name in MODELS_TO_TEST:
        print(f"â–¶ í…ŒìŠ¤íŠ¸ ì‹œì‘: [{model_name}]")
        try:
            start_time = time.time()
            first_token_time = None
            full_response = ""

            is_o1 = model_name.startswith("o1") or model_name.startswith("gpt-5")

            # o1/gpt-5 ê³„ì—´ ëª¨ë¸ì€ system prompt ëŒ€ì‹  userë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë©°, temperature íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            # ë˜í•œ ê³µì‹ì ìœ¼ë¡œ ì‹¤ì‹œê°„ streaming ì„±ëŠ¥ ì¸¡ì • ì§€í‘œê°€ ë¬´ì˜ë¯¸í•˜ë¯€ë¡œ(ë‚´ë¶€ì ìœ¼ë¡œ ê¸´ ì‹œê°„ ì‚¬ê³ í•¨) ë¶„ê¸° ì²˜ë¦¬í•©ë‹ˆë‹¤.
            kwargs = {
                "model": model_name,
                "messages": [
                    {"role": "user" if is_o1 else "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": USER_PROMPT}
                ]
            }
            if not is_o1:
                kwargs["stream"] = True
                kwargs["temperature"] = 0.7

            # API í˜¸ì¶œ
            response = client.chat.completions.create(**kwargs)

            if is_o1:
                # o1 ëª¨ë¸ë“¤ì€ ë‚´ë¶€ "Thinking" í”„ë¡œì„¸ìŠ¤ ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ ìŠ¤íŠ¸ë¦¬ë° ì²´ê°ì´ ì•ˆ ë©ë‹ˆë‹¤.
                first_token_time = time.time()  # ëŒ€ë‹µì´ ë°˜í™˜ëœ ì‹œì ì„ ì²« í† í°ìœ¼ë¡œ ê°„ì£¼
                if hasattr(response.choices[0].message, 'content'):
                    full_response = response.choices[0].message.content or ""
            else:
                for chunk in response:
                    content = chunk.choices[0].delta.content
                    if content is not None:
                        # ì²« í† í°ì´ ë„ë‹¬í•œ ì‹œê°„ ê¸°ë¡
                        if first_token_time is None:
                            first_token_time = time.time()
                        full_response += content

            end_time = time.time()

            # ì‹œê°„ ê³„ì‚°
            if first_token_time:
                ttft = first_token_time - start_time
                total_duration = end_time - start_time
                chars_per_sec = len(full_response) / total_duration if total_duration > 0 else 0

                print(f"   â±ï¸ TTFT (ì‘ë‹µ ì‹œì‘ ì†ë„): {ttft:.3f} ì´ˆ")
                print(f"   â³ ì „ì²´ ì†Œìš” ì‹œê°„: {total_duration:.3f} ì´ˆ")
                print(f"   âš¡ ìƒì„± ì†ë„: ì•½ {chars_per_sec:.1f} ê¸€ì/ì´ˆ")
                print(f"   ğŸ“„ ìƒì„±ëœ ë‹µë³€ (ì¼ë¶€): {full_response.strip()[:100]}...\n")
            else:
                print("   âŒ ì‘ë‹µ í† í°ì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n")

        except Exception as e:
            print(f"   âŒ {model_name} í˜¸ì¶œ ì‹¤íŒ¨: {e}\n")

if __name__ == "__main__":
    run_benchmark()
