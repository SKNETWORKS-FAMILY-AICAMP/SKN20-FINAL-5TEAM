import json
import statistics
import math

# Kendall's Tau ìˆ˜ë™ êµ¬í˜„
def kendalltau(x, y):
    n = len(x)
    concordant = 0
    discordant = 0

    for i in range(n):
        for j in range(i+1, n):
            if (x[i] - x[j]) * (y[i] - y[j]) > 0:
                concordant += 1
            elif (x[i] - x[j]) * (y[i] - y[j]) < 0:
                discordant += 1

    tau = (concordant - discordant) / (0.5 * n * (n - 1)) if n > 1 else 0
    return tau, 0  # p_valueëŠ” ìƒëµ

models = ['gpt-4o-mini', 'gpt-5-mini', 'gpt-5', 'gpt-5.2', 'gemini-2.5-flash', 'gemini-2.5-pro']
base_path = '/app/evaluation/data/validation/model_comparison'

# í’ˆì§ˆ ë ˆë²¨ ìˆœì„œ (ì •ë‹µ)
quality_order = {'excellent': 5, 'good': 4, 'average': 3, 'poor': 2, 'very_poor': 1}

print("=" * 120)
print("ğŸ¯ ëª¨ë¸ í‰ê°€ ì„±ëŠ¥ ë¹„êµ (ì§„ì§œ ì¤‘ìš”í•œ ì§€í‘œ)")
print("=" * 120)
print()

results_summary = {}

for model in models:
    print(f"\n{'='*120}")
    print(f"ğŸ“Š ëª¨ë¸: {model}")
    print(f"{'='*120}")

    with open(f'{base_path}/{model}_results.json', 'r') as f:
        data = json.load(f)

    all_results = data['results']
    stats = data['stats']

    # í’ˆì§ˆ ë ˆë²¨ë³„ ì ìˆ˜ ìˆ˜ì§‘
    quality_scores = {
        'excellent': [],
        'good': [],
        'average': [],
        'poor': [],
        'very_poor': []
    }

    # ì¼ê´€ì„± ê³„ì‚°ì„ ìœ„í•œ ìƒ˜í”Œë³„ ì ìˆ˜ ìˆ˜ì§‘
    sample_score_variances = []

    # ìˆœìœ„ ì •í™•ë„ ê³„ì‚°ìš©
    sample_quality_order = []
    sample_avg_scores = []

    for sample in all_results:
        quality = sample['quality_level']
        scores = []

        for trial in sample['trials']:
            if not trial.get('error', False):
                score = trial.get('thinking_score', 0)
                quality_scores[quality].append(score)
                scores.append(score)

        # ì¼ê´€ì„± (í‘œì¤€í¸ì°¨)
        if len(scores) >= 2:
            sample_score_variances.append(statistics.variance(scores))

        # ìˆœìœ„ ì •í™•ë„ìš© ë°ì´í„°
        if scores:
            sample_quality_order.append(quality_order[quality])
            sample_avg_scores.append(sum(scores) / len(scores))

    # 1. ì¼ê´€ì„± (Consistency) - ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    avg_consistency = statistics.mean([math.sqrt(v) for v in sample_score_variances]) if sample_score_variances else 0

    # 2. êµ¬ë¶„ë ¥ (Discrimination) - ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
    excellent_avg = statistics.mean(quality_scores['excellent']) if quality_scores['excellent'] else 0
    very_poor_avg = statistics.mean(quality_scores['very_poor']) if quality_scores['very_poor'] else 0
    discrimination = excellent_avg - very_poor_avg

    # 3. ìˆœìœ„ ì •í™•ë„ (Ranking Accuracy) - Kendall's Tau
    if len(sample_quality_order) > 0:
        tau, p_value = kendalltau(sample_quality_order, sample_avg_scores)
    else:
        tau = 0

    # 4. ì†ë„ (Speed)
    avg_speed = stats['avg_time']

    # 5. ë¹„ìš© (Cost)
    total_cost = stats['total_cost']

    # 6. ì‹ ë¢°ì„± (Reliability)
    total_trials = stats['total_evaluations']
    # ì‹¤íŒ¨í•œ trial ê³„ì‚°
    error_count = 0
    for sample in all_results:
        for trial in sample['trials']:
            if trial.get('error', False):
                error_count += 1

    reliability = ((total_trials - error_count) / (total_trials + error_count) * 100) if (total_trials + error_count) > 0 else 0

    # í’ˆì§ˆ ë ˆë²¨ë³„ í‰ê·  ì ìˆ˜
    print(f"\nğŸ“ˆ í’ˆì§ˆ ë ˆë²¨ë³„ í‰ê·  ì ìˆ˜:")
    print("-" * 120)
    for quality in ['excellent', 'good', 'average', 'poor', 'very_poor']:
        if quality_scores[quality]:
            avg = statistics.mean(quality_scores[quality])
            std = statistics.stdev(quality_scores[quality]) if len(quality_scores[quality]) > 1 else 0
            count = len(quality_scores[quality])
            print(f"   {quality:15s}: {avg:6.1f}ì  (Â±{std:4.1f}, n={count})")
        else:
            print(f"   {quality:15s}: ë°ì´í„° ì—†ìŒ")

    print(f"\n1ï¸âƒ£  ì¼ê´€ì„± (Consistency) - ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ")
    print("-" * 120)
    print(f"   í‘œì¤€í¸ì°¨ í‰ê· : {avg_consistency:.2f}ì ")
    if avg_consistency <= 5:
        print(f"   âœ… ëª©í‘œ ë‹¬ì„± (â‰¤5ì )")
    elif avg_consistency <= 10:
        print(f"   âš ï¸  ê°œì„  í•„ìš” (5-10ì )")
    else:
        print(f"   âŒ ë¶ˆì•ˆì • (>10ì )")

    print(f"\n2ï¸âƒ£  êµ¬ë¶„ë ¥ (Discrimination) - ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ")
    print("-" * 120)
    print(f"   Excellent í‰ê· : {excellent_avg:.1f}ì ")
    print(f"   Very Poor í‰ê· : {very_poor_avg:.1f}ì ")
    print(f"   êµ¬ë¶„ë ¥: {discrimination:.1f}ì ")
    if discrimination >= 30:
        print(f"   âœ… ëª©í‘œ ë‹¬ì„± (â‰¥30ì ) - ìš°ìˆ˜/ë¯¸í¡ì„ ì˜ êµ¬ë¶„í•¨")
    elif discrimination >= 20:
        print(f"   âš ï¸  ë³´í†µ (20-30ì )")
    else:
        print(f"   âŒ ë‚®ìŒ (<20ì ) - í’ˆì§ˆ êµ¬ë¶„ ëŠ¥ë ¥ ë¶€ì¡±")

    print(f"\n3ï¸âƒ£  ìˆœìœ„ ì •í™•ë„ (Ranking Accuracy) - ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ")
    print("-" * 120)
    print(f"   Kendall's Tau: {tau:.3f}")
    if tau >= 0.8:
        print(f"   âœ… ëª©í‘œ ë‹¬ì„± (â‰¥0.8) - í’ˆì§ˆ ìˆœì„œë¥¼ ë§¤ìš° ì •í™•íˆ í‰ê°€")
    elif tau >= 0.6:
        print(f"   âš ï¸  ë³´í†µ (0.6-0.8)")
    else:
        print(f"   âŒ ë‚®ìŒ (<0.6) - í’ˆì§ˆ ìˆœì„œ í‰ê°€ ë¶€ì •í™•")

    print(f"\n4ï¸âƒ£  ì†ë„ (Speed)")
    print("-" * 120)
    print(f"   í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_speed:.1f}ì´ˆ")

    print(f"\n5ï¸âƒ£  ë¹„ìš© (Cost)")
    print("-" * 120)
    print(f"   ì´ ë¹„ìš©: ${total_cost:.2f}")

    print(f"\n6ï¸âƒ£  ì‹ ë¢°ì„± (Reliability)")
    print("-" * 120)
    print(f"   ì„±ê³µë¥ : {reliability:.1f}%")
    print(f"   ì˜¤ë¥˜: {error_count}ê°œ")
    if reliability >= 95:
        print(f"   âœ… ëª©í‘œ ë‹¬ì„± (â‰¥95%)")
    elif reliability >= 90:
        print(f"   âš ï¸  ê°œì„  í•„ìš” (90-95%)")
    else:
        print(f"   âŒ ë¶ˆì•ˆì • (<90%)")

    # ê²°ê³¼ ì €ì¥
    results_summary[model] = {
        'consistency': avg_consistency,
        'discrimination': discrimination,
        'ranking_tau': tau,
        'speed': avg_speed,
        'cost': total_cost,
        'reliability': reliability,
        'excellent_avg': excellent_avg,
        'very_poor_avg': very_poor_avg
    }

print("\n" + "=" * 120)
print("ğŸ† ì¢…í•© ë¹„êµ")
print("=" * 120)

# ì •ê·œí™” í•¨ìˆ˜ (0-100 ìŠ¤ì¼€ì¼)
def normalize(value, min_val, max_val, reverse=False):
    if max_val == min_val:
        return 50
    normalized = (value - min_val) / (max_val - min_val) * 100
    return 100 - normalized if reverse else normalized

# ê° ì§€í‘œì˜ min/max ê³„ì‚°
consistency_vals = [r['consistency'] for r in results_summary.values()]
discrimination_vals = [r['discrimination'] for r in results_summary.values()]
ranking_vals = [r['ranking_tau'] for r in results_summary.values()]
speed_vals = [r['speed'] for r in results_summary.values()]
cost_vals = [r['cost'] for r in results_summary.values()]
reliability_vals = [r['reliability'] for r in results_summary.values()]

# ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
weights = {
    'consistency': 2.0,
    'discrimination': 2.0,
    'ranking': 1.5,
    'speed': 1.0,
    'cost': 1.5,
    'reliability': 2.0
}

total_weight = sum(weights.values())

for model, results in results_summary.items():
    # ì •ê·œí™” (ì¼ê´€ì„±, ì†ë„, ë¹„ìš©ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ - reverse=True)
    norm_consistency = normalize(results['consistency'], min(consistency_vals), max(consistency_vals), reverse=True)
    norm_discrimination = normalize(results['discrimination'], min(discrimination_vals), max(discrimination_vals))
    norm_ranking = normalize(results['ranking_tau'], min(ranking_vals), max(ranking_vals))
    norm_speed = normalize(results['speed'], min(speed_vals), max(speed_vals), reverse=True)
    norm_cost = normalize(results['cost'], min(cost_vals), max(cost_vals), reverse=True)
    norm_reliability = normalize(results['reliability'], min(reliability_vals), max(reliability_vals))

    # ê°€ì¤‘ í‰ê· 
    overall_score = (
        norm_consistency * weights['consistency'] +
        norm_discrimination * weights['discrimination'] +
        norm_ranking * weights['ranking'] +
        norm_speed * weights['speed'] +
        norm_cost * weights['cost'] +
        norm_reliability * weights['reliability']
    ) / total_weight

    results_summary[model]['overall_score'] = overall_score

# ì •ë ¬ ë° ì¶œë ¥
print(f"\n{'ëª¨ë¸':<25} {'ì¼ê´€ì„±':<10} {'êµ¬ë¶„ë ¥':<10} {'ìˆœìœ„':<10} {'ì†ë„':<10} {'ë¹„ìš©':<10} {'ì‹ ë¢°ì„±':<10} {'ì¢…í•©ì ìˆ˜':<10}")
print("-" * 120)

sorted_models = sorted(models, key=lambda m: results_summary[m]['overall_score'], reverse=True)

for model in sorted_models:
    r = results_summary[model]
    print(f"{model:<25} {r['consistency']:>6.1f}ì    {r['discrimination']:>6.1f}ì    {r['ranking_tau']:>6.3f}    {r['speed']:>6.1f}ì´ˆ   ${r['cost']:>6.2f}   {r['reliability']:>6.1f}%   {r['overall_score']:>6.1f}")

print("\n" + "=" * 120)
print("ğŸ“Œ ìµœì¢… ê²°ë¡ ")
print("=" * 120)

best_model = sorted_models[0]
best_discrimination = max(models, key=lambda m: results_summary[m]['discrimination'])
best_ranking = max(models, key=lambda m: results_summary[m]['ranking_tau'])
best_consistency = min(models, key=lambda m: results_summary[m]['consistency'])

print(f"\nğŸ† ìµœìš°ìˆ˜ ëª¨ë¸: {best_model}")
print(f"   ì¢…í•© ì ìˆ˜: {results_summary[best_model]['overall_score']:.1f}/100")
print(f"   ì´ ëª¨ë¸ì´ Bug Hunt í‰ê°€ ì‹œìŠ¤í…œì— ìµœì ì…ë‹ˆë‹¤.")

print(f"\nâœ¨ êµ¬ë¶„ë ¥ ìµœê³ : {best_discrimination}")
print(f"   êµ¬ë¶„ë ¥: {results_summary[best_discrimination]['discrimination']:.1f}ì ")
print(f"   (ìš°ìˆ˜ ì½”ë“œì™€ ë¯¸í¡ ì½”ë“œë¥¼ ê°€ì¥ ì˜ êµ¬ë¶„í•¨)")

print(f"\nâœ¨ ìˆœìœ„ ì •í™•ë„ ìµœê³ : {best_ranking}")
print(f"   Kendall's Tau: {results_summary[best_ranking]['ranking_tau']:.3f}")
print(f"   (í’ˆì§ˆ ë ˆë²¨ ìˆœì„œë¥¼ ê°€ì¥ ì •í™•íˆ í‰ê°€í•¨)")

print(f"\nâœ¨ ì¼ê´€ì„± ìµœê³ : {best_consistency}")
print(f"   í‘œì¤€í¸ì°¨: {results_summary[best_consistency]['consistency']:.2f}ì ")
print(f"   (ê°€ì¥ ì•ˆì •ì ì´ê³  ì¼ê´€ëœ í‰ê°€)")

print("\n" + "=" * 120)
print("\nğŸ’¡ ì§€í‘œ ì„¤ëª…:")
print("   - ì¼ê´€ì„±: ê°™ì€ ìƒ˜í”Œì„ ì—¬ëŸ¬ ë²ˆ í‰ê°€í–ˆì„ ë•Œ ì ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
print("   - êµ¬ë¶„ë ¥: ìš°ìˆ˜í•œ ì½”ë“œì™€ ë¯¸í¡í•œ ì½”ë“œë¥¼ ì–¼ë§ˆë‚˜ ì˜ êµ¬ë¶„í•˜ëŠ”ì§€ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
print("   - ìˆœìœ„ ì •í™•ë„: í’ˆì§ˆ ë ˆë²¨ ìˆœì„œë¥¼ ì˜¬ë°”ë¥´ê²Œ í‰ê°€í•˜ëŠ”ì§€ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
print("   - ì¢…í•©ì ìˆ˜: ìœ„ 6ê°œ ì§€í‘œë¥¼ ê°€ì¤‘ í‰ê· í•œ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
print("=" * 120)
