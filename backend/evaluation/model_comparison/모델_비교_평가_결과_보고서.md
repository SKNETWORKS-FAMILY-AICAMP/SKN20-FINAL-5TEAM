# 모델 비교 평가 결과 보고서

여러 LLM 모델의 Bug Hunt 평가 성능을 비교한 결과입니다.

## 평가 개요

이 시스템은 Bug Hunt 평가를 6개 LLM 모델로 실행하고, 다음 지표를 비교했습니다:

1. **일관성 (Consistency)**: 동일 샘플 반복 평가의 표준편차 (낮을수록 좋음)
2. **구분력 (Discrimination)**: 우수 vs 매우 미흡 케이스 점수 차이 (높을수록 좋음)
3. **순위 정확도 (Ranking Accuracy)**: Kendall's Tau (높을수록 좋음)
4. **속도 (Speed)**: 평균 응답 시간 (낮을수록 좋음)
5. **비용 (Cost)**: 총 API 비용 (낮을수록 좋음)
6. **신뢰성 (Reliability)**: 오류율 (낮을수록 좋음)

## 평가 조건

- **평가 샘플**: 60개 Bug Hunt 검증 샘플
- **품질 레벨**: excellent (12개), good (12개), average (12개), poor (12개), very_poor (12개)
- **반복 횟수**: 각 샘플당 3회 시도 (trials)
- **총 평가 횟수**: 60 샘플 × 3 trials × 6 모델 = 1,080회 평가
- **평가 기간**: 2026년 2월 12일
- **총 소요 시간**: 약 6시간

## 평가 모델

### OpenAI 모델
- `gpt-4o-mini`: GPT-4 Optimized 경량 모델
- `gpt-5-mini`: GPT-5 경량 모델
- `gpt-5`: GPT-5 표준 모델
- `gpt-5.2`: GPT-5.2 최신 모델

### Google Gemini 모델
- `gemini-2.5-flash`: Gemini 2.5 Flash (무료)
- `gemini-2.5-pro`: Gemini 2.5 Pro

### 기타
- `llama-3.3-70b-versatile`: Groq API 사용 (Rate Limit으로 평가 실패)

## 종합 결과

### 🏆 종합 순위

| 순위 | 모델 | 일관성 | 구분력 | 순위 정확도 | 속도 | 비용 | 신뢰성 | 종합점수 |
|------|------|--------|--------|-------------|------|------|--------|----------|
| 1 🥇 | **gpt-5.2** | 1.4점 | 60.0점 | 0.755 | 13.2초 | $2.50 | 100.0% | **72.7** |
| 2 🥈 | **gemini-2.5-flash** | 4.7점 | **80.7점** | **0.793** | 18.7초 | **$0.00** | 95.7% | **71.8** |
| 3 🥉 | **gemini-2.5-pro** | 2.4점 | 64.4점 | 0.717 | 30.2초 | $0.90 | 100.0% | **64.8** |
| 4 | gpt-5-mini | 3.3점 | 54.6점 | 0.725 | 15.4초 | $0.65 | 98.9% | 59.8 |
| 5 | gpt-5 | 2.3점 | 57.8점 | 0.736 | 48.7초 | $5.10 | 97.8% | 45.1 |
| 6 | gpt-4o-mini | **0.7점** | 40.0점 | 0.685 | **8.3초** | **$0.08** | 84.6% | 44.8 |

### 🎯 부문별 1위

| 부문 | 1위 모델 | 수치 |
|------|----------|------|
| **종합 점수** | gpt-5.2 | 72.7/100 |
| **구분력** | gemini-2.5-flash | 80.7점 ⭐ |
| **순위 정확도** | gemini-2.5-flash | 0.793 ⭐ |
| **일관성** | gpt-4o-mini | 0.7점 |
| **신뢰성** | gpt-5.2, gemini-2.5-pro | 100% |
| **속도** | gpt-4o-mini | 8.3초 |
| **비용** | gemini-2.5-flash | $0 (무료) |

## 상세 평가 결과

### 1. gpt-5.2 (종합 1위)

**종합 점수**: 72.7/100

**품질 레벨별 평균 점수**:
- Excellent: 68.8점 (±7.6)
- Good: 64.5점 (±7.3)
- Average: 27.3점 (±8.3)
- Poor: 12.9점 (±2.0)
- Very Poor: 8.8점 (±1.8)

**성능 지표**:
- ✅ 일관성: 1.4점 (목표 ≤5점 달성)
- ✅ 구분력: 60.0점 (목표 ≥30점 달성)
- ⚠️ 순위 정확도: 0.755 (목표 0.8 근접)
- 속도: 13.2초
- 비용: $2.50
- ✅ 신뢰성: 100% (오류 0개)

**강점**:
- 완벽한 신뢰성 (오류 없음)
- 우수한 구분력
- 빠른 속도

**약점**:
- 비용이 비쌈 ($2.50)
- Pass/Fail 판단 불일치 6건

### 2. gemini-2.5-flash (구분력 & 순위 정확도 1위)

**종합 점수**: 71.8/100

**품질 레벨별 평균 점수**:
- Excellent: 86.4점 (±7.1) ⭐
- Good: 74.4점 (±6.8)
- Average: 45.9점 (±17.5)
- Poor: 11.4점 (±3.7)
- Very Poor: 5.7점 (±3.7)

**성능 지표**:
- ✅ 일관성: 4.7점 (목표 ≤5점 달성)
- ✅ 구분력: 80.7점 (전체 1위) ⭐
- ⚠️ 순위 정확도: 0.793 (전체 1위) ⭐
- 속도: 18.7초
- 비용: $0.00 (무료) ⭐
- ✅ 신뢰성: 95.7% (오류 4개)

**강점**:
- **최고의 구분력**: 우수/미흡 코드를 가장 잘 구분
- **최고의 순위 정확도**: 품질 레벨 순서를 가장 정확히 평가
- **완전 무료**
- 합리적인 속도

**약점**:
- 일관성이 다소 낮음 (4.7점)
- Average 품질 샘플 점수 변동이 큼 (±17.5)
- JSON 파싱 오류 4건

**추천 대상**:
- 비용을 최소화하면서도 우수한 평가 성능을 원하는 경우
- 코드 품질 구분 능력이 중요한 경우

### 3. gemini-2.5-pro

**종합 점수**: 64.8/100

**품질 레벨별 평균 점수**:
- Excellent: 69.6점 (±3.0)
- Good: 70.8점 (±3.1)
- Average: 28.1점 (±13.4)
- Poor: 9.4점 (±3.1)
- Very Poor: 5.2점 (±1.2)

**성능 지표**:
- ✅ 일관성: 2.4점 (목표 ≤5점 달성)
- ✅ 구분력: 64.4점 (목표 ≥30점 달성)
- ⚠️ 순위 정확도: 0.717 (목표 0.8 근접)
- 속도: 30.2초 (느림)
- 비용: $0.90
- ✅ 신뢰성: 100% (오류 0개)

**강점**:
- 완벽한 신뢰성
- 우수한 일관성
- 좋은 구분력

**약점**:
- 속도가 느림 (30.2초)
- 비용 대비 성능이 gemini-flash보다 낮음

### 4. gpt-5-mini

**종합 점수**: 59.8/100

**품질 레벨별 평균 점수**:
- Excellent: 75.9점 (±4.5)
- Good: 75.0점 (±3.6)
- Average: 43.4점 (±11.0)
- Poor: 25.9점 (±3.5)
- Very Poor: 21.2점 (±3.5)

**성능 지표**:
- ✅ 일관성: 3.3점 (목표 ≤5점 달성)
- ✅ 구분력: 54.6점 (목표 ≥30점 달성)
- ⚠️ 순위 정확도: 0.725
- 속도: 15.4초
- 비용: $0.65
- ✅ 신뢰성: 98.9% (오류 1개)

**강점**:
- 매우 높은 신뢰성
- 좋은 일관성
- 합리적인 속도와 비용

**약점**:
- 구분력이 중간 수준
- Excellent/Good 샘플 점수가 비슷함

### 5. gpt-5

**종합 점수**: 45.1/100

**품질 레벨별 평균 점수**:
- Excellent: 73.3점 (±3.8)
- Good: 72.6점 (±3.5)
- Average: 37.2점 (±11.4)
- Poor: 22.1점 (±3.2)
- Very Poor: 15.5점 (±3.3)

**성능 지표**:
- ✅ 일관성: 2.3점 (목표 ≤5점 달성)
- ✅ 구분력: 57.8점 (목표 ≥30점 달성)
- ⚠️ 순위 정확도: 0.736
- 속도: 48.7초 (매우 느림) ❌
- 비용: $5.10 (매우 비쌈) ❌
- ✅ 신뢰성: 97.8% (오류 2개)

**강점**:
- 좋은 일관성
- 양호한 구분력
- 높은 신뢰성

**약점**:
- **매우 느린 속도** (48.7초)
- **매우 비싼 비용** ($5.10)
- 성능 대비 비용/속도 효율성 최저

**결론**: 성능은 괜찮지만 비용과 속도 때문에 추천하지 않음

### 6. gpt-4o-mini (일관성 & 속도 1위)

**종합 점수**: 44.8/100

**품질 레벨별 평균 점수**:
- Excellent: 70.0점 (±0.0)
- Good: 70.0점 (±0.0)
- Average: 55.9점 (±11.0)
- Poor: 37.4점 (±4.4)
- Very Poor: 30.0점 (±0.0)

**성능 지표**:
- ✅ 일관성: 0.7점 (전체 1위) ⭐
- ✅ 구분력: 40.0점 (목표 ≥30점 달성, 하지만 가장 낮음)
- ❌ 순위 정확도: 0.685 (목표 미달)
- 속도: 8.3초 (전체 1위) ⭐
- 비용: $0.08 (매우 저렴) ⭐
- ❌ 신뢰성: 84.6% (오류 15개)

**강점**:
- **최고의 일관성** (0.7점)
- **최고 속도** (8.3초)
- **매우 저렴** ($0.08)

**약점**:
- **낮은 구분력** (40.0점) - 코드 품질 구분 능력 부족
- **낮은 순위 정확도** (0.685)
- **낮은 신뢰성** (84.6%, JSON 파싱 오류 15개)
- Excellent/Good/Average 샘플 점수가 비슷함

**결론**: 일관성은 높지만 실제 코드 평가 능력은 부족함. **추천하지 않음**.

### 7. llama-3.3-70b-versatile (평가 실패)

**종합 점수**: N/A

**평가 결과**:
- 총 시도: 180개
- 성공: 1개
- 실패: 179개
- 실패 원인: Groq API Rate Limit (일일 토큰 제한 초과)

**상세**:
- Groq 무료 티어: 100,000 tokens/day
- 평가 시작 시점에 이미 98,732 토큰 사용됨
- 첫 번째 샘플만 성공 후 전부 실패

**결론**: 평가 불가

## 평가 지표 상세

### 1. 일관성 (Consistency)

**측정 방법**: 동일 샘플을 3회 반복 평가했을 때의 표준편차 평균
**목표**: ≤ 5점
**의미**: 낮을수록 평가가 일관적이고 안정적임

**결과**:
- 🥇 gpt-4o-mini: 0.7점 (매우 우수)
- gpt-5.2: 1.4점 (우수)
- gpt-5: 2.3점 (우수)
- gemini-2.5-pro: 2.4점 (우수)
- gpt-5-mini: 3.3점 (양호)
- gemini-2.5-flash: 4.7점 (양호)

**전체 평가**: 모든 모델이 목표 달성 ✅

### 2. 구분력 (Discrimination)

**측정 방법**: Excellent 케이스 평균 점수 - Very Poor 케이스 평균 점수
**목표**: ≥ 30점
**의미**: 높을수록 품질 차이를 잘 구분함

**결과**:
- 🥇 gemini-2.5-flash: 80.7점 (86.4 - 5.7) ⭐
- gemini-2.5-pro: 64.4점 (69.6 - 5.2)
- gpt-5.2: 60.0점 (68.8 - 8.8)
- gpt-5: 57.8점 (73.3 - 15.5)
- gpt-5-mini: 54.6점 (75.9 - 21.2)
- gpt-4o-mini: 40.0점 (70.0 - 30.0)

**전체 평가**: 모든 모델이 목표 달성, gemini-2.5-flash가 압도적 1위 ✅

**핵심 발견**:
- gemini-2.5-flash는 Excellent 코드를 86.4점으로 평가하고 Very Poor 코드를 5.7점으로 평가하여 80.7점 차이를 보임
- gpt-4o-mini는 Very Poor 코드도 30점을 주어 구분력이 가장 낮음

### 3. 순위 정확도 (Ranking Accuracy)

**측정 방법**: Kendall's Tau (품질 레벨 순서 일치도)
**목표**: ≥ 0.8
**의미**: 높을수록 excellent > good > average > poor > very_poor 순서를 정확히 평가함

**결과**:
- 🥇 gemini-2.5-flash: 0.793 ⚠️
- gpt-5: 0.736 ⚠️
- gpt-5.2: 0.755 ⚠️
- gpt-5-mini: 0.725 ⚠️
- gemini-2.5-pro: 0.717 ⚠️
- gpt-4o-mini: 0.685 ❌

**전체 평가**: 모든 모델이 목표 미달 (0.8 미만)

**분석**:
- gemini-2.5-flash가 0.793으로 가장 근접했으나 목표에는 미달
- 품질 레벨 순서를 완벽히 지키는 모델은 없음
- 특히 Excellent와 Good 샘플의 점수가 비슷하게 나오는 경향

### 4. 속도 (Speed)

**측정 방법**: 평균 응답 시간 (초)
**의미**: 낮을수록 빠름

**결과**:
- 🥇 gpt-4o-mini: 8.3초
- gpt-5.2: 13.2초
- gpt-5-mini: 15.4초
- gemini-2.5-flash: 18.7초
- gemini-2.5-pro: 30.2초
- gpt-5: 48.7초 (매우 느림)

### 5. 비용 (Cost)

**측정 방법**: 총 API 비용 (180회 평가 기준, USD)
**의미**: 낮을수록 경제적

**결과**:
- 🥇 gemini-2.5-flash: $0.00 (무료) ⭐
- gpt-4o-mini: $0.08
- gpt-5-mini: $0.65
- gemini-2.5-pro: $0.90
- gpt-5.2: $2.50
- gpt-5: $5.10 (매우 비쌈)

**1,000회 평가 시 예상 비용**:
- gemini-2.5-flash: $0.00
- gpt-4o-mini: $0.44
- gpt-5-mini: $3.61
- gemini-2.5-pro: $5.00
- gpt-5.2: $13.89
- gpt-5: $28.33

### 6. 신뢰성 (Reliability)

**측정 방법**: 오류 없이 성공한 비율 (%)
**목표**: ≥ 95%
**의미**: 높을수록 안정적

**결과**:
- 🥇 gpt-5.2: 100.0% (오류 0개) ✅
- 🥇 gemini-2.5-pro: 100.0% (오류 0개) ✅
- gpt-5-mini: 98.9% (오류 1개) ✅
- gpt-5: 97.8% (오류 2개) ✅
- gemini-2.5-flash: 95.7% (오류 4개) ✅
- gpt-4o-mini: 84.6% (오류 15개) ❌

**오류 유형**:
- 대부분 JSON 파싱 오류
- 모델이 반환한 JSON 형식이 잘못됨

## 종합 점수 계산 방법

각 지표를 0-100으로 정규화하고 다음 가중치를 적용:

```python
가중치:
- 일관성: 2.0
- 구분력: 2.0
- 순위 정확도: 1.5
- 속도: 1.0
- 비용: 1.5
- 신뢰성: 2.0

총 가중치: 10.0
```

**정규화 방법**:
- 일관성, 속도, 비용: 낮을수록 좋음 (역정규화)
- 구분력, 순위 정확도, 신뢰성: 높을수록 좋음

## 최종 권장사항

### 💰 비용 최소화 시나리오

**추천 모델**: `gemini-2.5-flash`

**이유**:
- 완전 무료 ($0.00)
- 최고의 구분력 (80.7점)
- 최고의 순위 정확도 (0.793)
- 높은 신뢰성 (95.7%)
- 합리적인 속도 (18.7초)

**단점**:
- 일관성이 다소 낮음 (4.7점, 하지만 목표 달성)

**결론**: 비용 대비 성능이 가장 우수함 ⭐⭐⭐

### ⚡ 성능 최우선 시나리오

**추천 모델**: `gpt-5.2`

**이유**:
- 종합 점수 1위 (72.7/100)
- 완벽한 신뢰성 (100%, 오류 0개)
- 우수한 구분력 (60.0점)
- 빠른 속도 (13.2초)
- 매우 낮은 일관성 (1.4점)

**단점**:
- 비용이 비쌈 ($2.50)
- gemini-2.5-flash보다 구분력과 순위 정확도가 낮음

**결론**: 신뢰성이 중요하고 비용 부담이 적다면 선택 가능

### 🚫 권장하지 않는 모델

**gpt-4o-mini**:
- 일관성과 속도는 최고지만
- 구분력 최하위 (40.0점)
- 순위 정확도 최하위 (0.685)
- 낮은 신뢰성 (84.6%)
- **코드 품질 평가 능력이 부족**

**gpt-5**:
- 성능은 중간 수준이지만
- 매우 느림 (48.7초)
- 매우 비쌈 ($5.10)
- **비용/속도 효율성 최악**

## 결론

### 🏆 최종 추천

**1순위: gemini-2.5-flash** ⭐⭐⭐
- 무료이면서도 최고의 코드 평가 능력
- 구분력과 순위 정확도 1위
- Bug Hunt 평가 시스템에 최적

**2순위: gpt-5.2**
- 완벽한 신뢰성 필요 시
- 비용 부담 가능 시
- 종합 점수 1위

**3순위: gemini-2.5-pro**
- gemini-flash의 백업용
- 100% 신뢰성 필요 시

### 주요 발견사항

1. **무료 모델이 유료 모델보다 우수**: gemini-2.5-flash가 대부분의 유료 모델보다 성능이 좋음

2. **비싼 모델 ≠ 좋은 성능**: gpt-5 ($5.10)가 가장 비싸지만 성능은 중간 수준

3. **일관성 ≠ 평가 능력**: gpt-4o-mini는 일관성 1위지만 구분력과 순위 정확도는 최하위

4. **구분력이 가장 중요**: 코드 품질을 정확히 평가하려면 우수/미흡 코드를 잘 구분해야 함

5. **모든 모델이 순위 정확도 목표 미달**: 품질 레벨 순서를 완벽히 지키는 모델은 없음 (0.8 미만)

## 향후 개선 방안

1. **프롬프트 최적화**: 순위 정확도 0.8 이상 달성을 위한 프롬프트 개선
2. **앙상블 평가**: 여러 모델의 결과를 조합하여 정확도 향상
3. **추가 모델 테스트**: Claude, GPT-4o 등 다른 모델 평가
4. **비용 최적화**: gemini-2.5-flash를 기본으로 사용하고, 중요 평가만 유료 모델 사용

---

**평가 실행 일시**: 2026년 2월 12일
**평가 도구**: `run_model_comparison.py`, `proper_model_comparison.py`
**총 평가 시간**: 약 6시간
**총 비용**: $9.23 (llama 제외)
