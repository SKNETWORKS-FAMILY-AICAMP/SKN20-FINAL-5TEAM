"""
BugHunt ë©´ì ‘ê´€ LLM ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ (v2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ëŒ€ìƒ ëª¨ë¸: gpt-4o-mini, gpt-5.2, gemini-2.5-flash
ì‹œë‚˜ë¦¬ì˜¤ : S4 Step 1 (Gradient Bug - optimizer.zero_grad ëˆ„ë½)
ì¡°ê±´     : í”„ë¡¬í”„íŠ¸ ê³ ì •, ìœ ì € ë‹µë³€ ê³ ì •, 3íšŒ ë°˜ë³µ
"""

import os, sys, json, time, re, statistics
from pathlib import Path
from datetime import datetime

try:
    from dotenv import load_dotenv
    load_dotenv(Path(__file__).resolve().parent.parent / '.env')
except ImportError:
    pass

import openai
import google.generativeai as genai

OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '')
GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY', '')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ (S4 Step 1 - Gradient Bug)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP_CONTEXT = {
    "buggy_code": (
        "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n"
        "model = SimpleNet()\ncriterion = nn.CrossEntropyLoss()\n"
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n"
        "for epoch in range(10):\n"
        "    for batch_idx, (data, target) in enumerate(train_loader):\n"
        "        output = model(data)\n"
        "        loss = criterion(output, target)\n\n"
        "        loss.backward()\n"
        "        optimizer.step()\n\n"
        "        print(f\"Epoch {epoch}, Batch {batch_idx}: Loss = {loss.item():.4f}\")"
    ),
    "user_code": (
        "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n"
        "model = SimpleNet()\ncriterion = nn.CrossEntropyLoss()\n"
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n"
        "for epoch in range(10):\n"
        "    for batch_idx, (data, target) in enumerate(train_loader):\n"
        "        optimizer.zero_grad()\n\n"
        "        output = model(data)\n"
        "        loss = criterion(output, target)\n\n"
        "        loss.backward()\n"
        "        optimizer.step()\n\n"
        "        print(f\"Epoch {epoch}, Batch {batch_idx}: Loss = {loss.item():.4f}\")"
    ),
    "error_info": {
        "type": "Gradient Accumulation Bug",
        "description": "optimizer.zero_grad()ê°€ ëˆ„ë½ë˜ì–´ gradientê°€ ë§¤ ë°°ì¹˜ë§ˆë‹¤ ëˆ„ì ë˜ê³  ìˆìŠµë‹ˆë‹¤."
    },
    "interview_rubric": {
        "core_concepts": ["zero_grad() ëˆ„ë½", "gradient ëˆ„ì "],
        "mechanism_concepts": ["backward()ê°€ .gradì— += ì—°ì‚°", "ë°°ì¹˜ë§ˆë‹¤ gradientê°€ ì»¤ì§"],
        "application_concepts": [
            "í‘œì¤€ ë£¨í”„ ìˆœì„œ: zero_grad â†’ forward â†’ backward â†’ step",
            "ì˜ë„ì  gradient accumulationê³¼ì˜ ì°¨ì´"
        ],
        "first_question": "ë°©ê¸ˆ optimizer.zero_grad()ë¥¼ ì¶”ê°€í•˜ì…¨ëŠ”ë°, ì´ ì½”ë“œê°€ ì—†ìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ë‚˜ìš”?"
    }
}

# ê³ ì • ìœ ì € ë‹µë³€ (3í„´ + ìµœì¢… í‰ê°€ ì¬ì „ì†¡)
USER_ANSWERS = [
    "zero_gradê°€ ì—†ìœ¼ë©´ gradientê°€ ì´ˆê¸°í™” ì•ˆë¼ì„œ ì´ì „ ë°°ì¹˜ì˜ gradientê°€ ê³„ì† ìŒ“ì´ëŠ” ê±¸ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ lossê°€ ì ì  ì»¤ì§€ëŠ” ê²ƒ ê°™ì•„ìš”.",
    "backwardë¥¼ í˜¸ì¶œí•˜ë©´ ê° íŒŒë¼ë¯¸í„°ì˜ .gradì— ìƒˆë¡œìš´ gradientê°€ ë”í•´ì§€ëŠ”ë°, zero_gradê°€ ì—†ìœ¼ë©´ ì´ì „ ê°’ ìœ„ì— ê³„ì† += ë˜ë‹ˆê¹Œ gradient ê°’ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì§€ëŠ” ê±°ì£ .",
    "í‘œì¤€ ë£¨í”„ ìˆœì„œëŠ” zero_grad, forward, backward, step ìˆœì„œë¡œ í•˜ëŠ” ê²Œ ë§ê³ , ë§Œì•½ ì˜ë„ì ìœ¼ë¡œ gradientë¥¼ ëˆ„ì í•˜ê³  ì‹¶ìœ¼ë©´ në°°ì¹˜ë§ˆë‹¤ í•œë²ˆì”© zero_gradë¥¼ í˜¸ì¶œí•˜ëŠ” gradient accumulation ê¸°ë²•ì„ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
]

MAX_TURNS = 3
DISPLAY_NAME = "ì§€ì›ì"

# í„´ë³„ ê¸°ëŒ€ í‚¤ì›Œë“œ (ì§ˆë¬¸ í’ˆì§ˆ í‰ê°€ìš©)
TURN_KEYWORDS = {
    1: ["zero_grad", "gradient", "ì´ˆê¸°í™”", "ëˆ„ì ", "ì›ì¸"],
    2: ["backward", ".grad", "+=", "ë‚´ë¶€", "ë™ì‘", "ë©”ì»¤ë‹ˆì¦˜"],
    3: ["ìˆœì„œ", "zero_grad.*forward.*backward.*step", "accumulation", "ì‹¤ë¬´", "ë””ë²„ê¹…"],
}

# ë¹„ìš© ì¶”ì • (USD per 1M tokens, 2026-02 ê¸°ì¤€ ì¶”ì •)
COST_PER_1M = {
    "gpt-4o-mini":       {"input": 0.15,  "output": 0.60},
    "gpt-5.2":           {"input": 2.50,  "output": 10.00},
    "gemini-2.5-flash":  {"input": 0.15,  "output": 0.60},
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. í”„ë¡¬í”„íŠ¸ ë¹Œë” (ai_view.pyì™€ 100% ë™ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _rubric_text():
    r = STEP_CONTEXT['interview_rubric']
    return (
        f"í•µì‹¬ ê°œë… (core): {', '.join(r['core_concepts'])}\n"
        f"ë©”ì»¤ë‹ˆì¦˜ ê°œë… (mechanism): {', '.join(r['mechanism_concepts'])}\n"
        f"ì‘ìš© ê°œë… (application): {', '.join(r['application_concepts'])}"
    )

def build_stream_prompt(turn):
    ei = STEP_CONTEXT['error_info']
    remaining = MAX_TURNS - turn
    return f"""ë„ˆëŠ” ì£¼ë‹ˆì–´ AI ì—”ì§€ë‹ˆì–´ë¥¼ ë©´ì ‘í•˜ëŠ” ê¸°ìˆ  ë©´ì ‘ê´€ì´ë‹¤. í•œêµ­ì–´ë¡œ ëŒ€í™”í•œë‹¤.
{DISPLAY_NAME}ë‹˜ì´ ì•„ë˜ ì½”ë“œì˜ ë²„ê·¸ë¥¼ ìˆ˜ì •í–ˆë‹¤. ìˆ˜ì • ì´ìœ ì™€ ì´í•´ë„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì§ˆë¬¸í•œë‹¤.

[ëŒ€ìƒ ìˆ˜ì¤€ - ë§¤ìš° ì¤‘ìš”]
ìƒëŒ€ë°©ì€ AI/MLì„ ë°°ìš°ê³  ìˆëŠ” ì£¼ë‹ˆì–´ ì—”ì§€ë‹ˆì–´ë‹¤.
- ë¬¼ì–´ë´ë„ ë˜ëŠ” ê²ƒ: ê°œë…ì˜ "ì™œ", ë‚´ë¶€ ë™ì‘ ì›ë¦¬, ì½”ë“œ ë™ì‘ ìˆœì„œ, í•´ë‹¹ ë²„ê·¸ì™€ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©
- ì ˆëŒ€ ë¬¼ì–´ë³´ë©´ ì•ˆ ë˜ëŠ” ê²ƒ: gradient accumulation êµ¬í˜„, loss scaling, learning rate scheduling ì „ëµ, ë¶„ì‚° í•™ìŠµ, ì»¤ìŠ¤í…€ ì˜µí‹°ë§ˆì´ì € ë“± ì‹œë‹ˆì–´ ë ˆë²¨ ì£¼ì œ
- ë£¨ë¸Œë¦­ì— ìˆëŠ” ê°œë… ë²”ìœ„ ì•ˆì—ì„œë§Œ ì§ˆë¬¸í•˜ë¼. ë£¨ë¸Œë¦­ì— ì—†ëŠ” ì‹¬í™” ì£¼ì œë¡œ ë„˜ì–´ê°€ì§€ ë§ˆë¼.

[í˜„ì¬ ì§„í–‰ ìƒí™©]
í˜„ì¬ {turn}/{MAX_TURNS}í„´ (ë‚¨ì€ ì§ˆë¬¸ ê¸°íšŒ: {remaining}íšŒ)

í„´ë³„ ì§ˆë¬¸ ë°©í–¥:
- 1í„´ (ì²« ë‹µë³€ í›„): core ê°œë…ì„ ì •í™•íˆ ì´í•´í–ˆëŠ”ì§€ í™•ì¸. í‹€ë¦° ë¶€ë¶„ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì§šì–´ë¼.
- 2í„´: mechanism ê°œë…ìœ¼ë¡œ ë„˜ì–´ê°€ë¼. "ë‚´ë¶€ì ìœ¼ë¡œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€" ë¬¼ì–´ë¼.
- 3í„´ (ë§ˆì§€ë§‰): application ê°œë…ì„ ë¬¼ì–´ë¼. ë‹¨, ì£¼ë‹ˆì–´ ìˆ˜ì¤€ì˜ ì‹¤ë¬´ (ë””ë²„ê¹… ë°©ë²•, í™•ì¸ ë°©ë²•) í•œì •.

[ë²„ê·¸ ì½”ë“œ]
{STEP_CONTEXT['buggy_code']}

[ìœ ì €ê°€ ìˆ˜ì •í•œ ì½”ë“œ]
{STEP_CONTEXT['user_code']}

[ë²„ê·¸ ì •ë³´]
íƒ€ì…: {ei['type']}
ì„¤ëª…: {ei['description']}

[í‰ê°€ ê¸°ì¤€ - ì±„ì  ë£¨ë¸Œë¦­]
{_rubric_text()}

[ì ì‘í˜• ì§ˆë¬¸ ì „ëµ - ìœ ì €ì˜ ì§ì „ ë‹µë³€ì„ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ë¼]

1) ë‹µë³€ì´ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ê²½ìš°:
   â†’ "ì˜ ì´í•´í•˜ê³  ê³„ì‹œë„¤ìš”"ë¥¼ ì§§ê²Œ ì¸ì •í•œ ë’¤, ë£¨ë¸Œë¦­ì˜ ë‹¤ìŒ ë‹¨ê³„ ê°œë…ì„ ë¬¼ì–´ë¼.

2) ë°©í–¥ì€ ë§ì§€ë§Œ ë¶€ì •í™•í•˜ê±°ë‚˜ ë¹ ì§„ ë¶€ë¶„ì´ ìˆëŠ” ê²½ìš°:
   â†’ í‹€ë¦° ë¶€ë¶„ì„ ë¶€ë“œëŸ½ê²Œ ì§šì–´ë¼.

3) "ëª¨ë¥´ê² ë‹¤" ë˜ëŠ” ë§¤ìš° ëª¨í˜¸í•œ ë‹µë³€ì¸ ê²½ìš°:
   â†’ ë‚œì´ë„ë¥¼ í™• ë‚®ì¶°ë¼.

4) ì™„ì „íˆ ë°©í–¥ì´ í‹€ë¦° ê²½ìš°:
   â†’ í‹€ë¦° ë¶€ë¶„ì„ ì •ì¤‘í•˜ê²Œ ì•Œë ¤ì£¼ê³ , ì˜¬ë°”ë¥¸ ë°©í–¥ì˜ ë‹¨ì„œë¥¼ ì¤€ ë’¤ ë” ì‰¬ìš´ ì§ˆë¬¸ì„ í•˜ë¼.

[ê·œì¹™]
- ì •ë‹µì„ ì§ì ‘ ì•Œë ¤ì£¼ì§€ ë§ˆë¼. ìœ ë„ ì§ˆë¬¸ë§Œ í•˜ë¼.
- ì§ˆë¬¸ì€ 1~2ë¬¸ì¥ìœ¼ë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ í•˜ë¼.
- ë°˜ë“œì‹œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë¼.
- ìœ ì €ë¥¼ ë¶€ë¥¼ ë•ŒëŠ” ë°˜ë“œì‹œ "{DISPLAY_NAME}ë‹˜" í˜¸ì¹­ì„ ì‚¬ìš©í•˜ë¼.
- ì¶œë ¥ì€ JSONì´ ì•„ë‹Œ, ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ "ì§ˆë¬¸ ë¬¸ì¥ë§Œ" ì¶œë ¥í•˜ë¼.
"""


def build_eval_prompt():
    ei = STEP_CONTEXT['error_info']
    return f"""ë„ˆëŠ” ì£¼ë‹ˆì–´ AI ì—”ì§€ë‹ˆì–´ ê¸°ìˆ  ë©´ì ‘ê´€ì´ë‹¤. í•œêµ­ì–´ë¡œ ëŒ€í™”í•œë‹¤.
{DISPLAY_NAME}ë‹˜ì´ ì•„ë˜ ì½”ë“œì˜ ë²„ê·¸ë¥¼ ìˆ˜ì •í–ˆê³ , ì§€ê¸ˆê¹Œì§€ ëŒ€í™”ë¥¼ ë‚˜ëˆ´ë‹¤.
ì´ë²ˆì´ ë§ˆì§€ë§‰ í„´ì´ë‹¤. {DISPLAY_NAME}ë‹˜ì˜ ë§ˆì§€ë§‰ ë‹µë³€ì„ í‰ê°€í•˜ê³  ì¢…í•© í‰ê°€ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•˜ë¼.

[ë²„ê·¸ ì½”ë“œ]
{STEP_CONTEXT['buggy_code']}

[ìœ ì €ê°€ ìˆ˜ì •í•œ ì½”ë“œ]
{STEP_CONTEXT['user_code']}

[ë²„ê·¸ ì •ë³´]
íƒ€ì…: {ei['type']}
ì„¤ëª…: {ei['description']}

[í‰ê°€ ê¸°ì¤€ - ì±„ì  ë£¨ë¸Œë¦­]
{_rubric_text()}

[ì±„ì  ë°©ë²• - ì£¼ë‹ˆì–´ ì—”ì§€ë‹ˆì–´ ê¸°ì¤€ìœ¼ë¡œ ê´€ëŒ€í•˜ê²Œ ì±„ì í•˜ë¼]
ëŒ€í™” ì „ì²´ë¥¼ ì¢…í•©í•´ì„œ ì±„ì í•˜ë¼ (ë§ˆì§€ë§‰ ë‹µë³€ë§Œì´ ì•„ë‹˜).
í”¼ë“œë°± ë¬¸ì¥ì—ì„œëŠ” ë°˜ë“œì‹œ "{DISPLAY_NAME}ë‹˜" í˜¸ì¹­ì„ ì‚¬ìš©í•˜ë¼.

1) core (40ì  ë§Œì ):
   - í•µì‹¬ ì›ì¸ì„ ìê¸° ë§ë¡œ ì„¤ëª…í–ˆìœ¼ë©´ 30~40ì 
   - ë°©í–¥ì€ ë§ì§€ë§Œ ë¶€ì •í™•í•˜ë©´ 15~25ì 
   - ì „í˜€ ëª¨ë¥´ë©´ 0~10ì 

2) mechanism (35ì  ë§Œì ):
   - ë‚´ë¶€ ë™ì‘ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í–ˆìœ¼ë©´ 25~35ì 
   - ê°œë…ì€ ì•Œì§€ë§Œ ì„¤ëª…ì´ ëª¨í˜¸í•˜ë©´ 10~20ì 
   - ì–¸ê¸‰ ì—†ìœ¼ë©´ 0~5ì 

3) application (25ì  ë§Œì ):
   - ì‹¤ë¬´ ì ìš© ë°©ë²•ì„ 1ê°€ì§€ë¼ë„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ë©´ 15~25ì 
   - ì¶”ìƒì ìœ¼ë¡œë§Œ ì–¸ê¸‰í•˜ë©´ 5~12ì 
   - ì–¸ê¸‰ ì—†ìœ¼ë©´ 0ì 

[understanding_level ê¸°ì¤€]
- 90ì  ì´ìƒ: "Excellent"
- 70~89ì : "Good"
- 40~69ì : "Surface"
- 39ì  ì´í•˜: "Poor"

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼:
{{
  "type": "evaluation",
  "message": "2~3ë¬¸ì¥ì˜ ì¢…í•© í”¼ë“œë°±",
  "score": 0ì—ì„œ 100 ì‚¬ì´ ì •ìˆ˜,
  "understanding_level": "Excellent|Good|Surface|Poor",
  "matched_concepts": ["ìœ ì €ê°€ ë³´ì—¬ì¤€ ê°œë…ë“¤"],
  "weak_point": "ë¶€ì¡±í•œ ë¶€ë¶„ (ì—†ìœ¼ë©´ null)"
}}"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ì§ˆë¬¸ í’ˆì§ˆ ìë™ í‰ê°€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def evaluate_question_quality(text, turn):
    """ì§ˆë¬¸ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ 0~100ìœ¼ë¡œ í‰ê°€"""
    score = 0
    details = {}

    # (1) ì¡´ëŒ“ë§ ì‚¬ìš© ì—¬ë¶€ (20ì )
    honorific_patterns = ["ë‚˜ìš”", "ê¹Œìš”", "ì„¸ìš”", "ìŠµë‹ˆë‹¤", "ì‹œê² ", "ë“œë¦¬"]
    honorific_count = sum(1 for p in honorific_patterns if p in text)
    honorific_score = min(20, honorific_count * 7)
    score += honorific_score
    details["ì¡´ëŒ“ë§"] = f"{honorific_score}/20"

    # (2) í˜¸ì¹­ ì‚¬ìš© (10ì )
    has_name = DISPLAY_NAME in text
    name_score = 10 if has_name else 0
    score += name_score
    details["í˜¸ì¹­"] = f"{name_score}/10"

    # (3) ì§ˆë¬¸ ê¸¸ì´ ì ì •ì„± (15ì ) â€” 50~300ìê°€ ì ì •
    length = len(text)
    if 30 <= length <= 400:
        len_score = 15
    elif 15 <= length <= 600:
        len_score = 10
    else:
        len_score = 5
    score += len_score
    details["ê¸¸ì´"] = f"{len_score}/15 ({length}ì)"

    # (4) í„´ë³„ í‚¤ì›Œë“œ ë¶€í•©ë„ (30ì )
    keywords = TURN_KEYWORDS.get(turn, [])
    if keywords:
        matched = sum(1 for k in keywords if re.search(k, text, re.IGNORECASE))
        kw_score = min(30, int(matched / len(keywords) * 30))
    else:
        kw_score = 15  # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’
    score += kw_score
    details["ì£¼ì œë¶€í•©"] = f"{kw_score}/30"

    # (5) í•œêµ­ì–´ ë¹„ìœ¨ (15ì )
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    kr_ratio = korean_chars / max(len(text), 1)
    kr_score = 15 if kr_ratio > 0.3 else (10 if kr_ratio > 0.15 else 5)
    score += kr_score
    details["í•œêµ­ì–´"] = f"{kr_score}/15 ({kr_ratio:.0%})"

    # (6) ì§ˆë¬¸í˜• ì¢…ê²° (10ì ) â€” ?ë¡œ ëë‚˜ëŠ”ì§€
    has_question = "?" in text or "ìš”?" in text
    q_score = 10 if has_question else 3
    score += q_score
    details["ì§ˆë¬¸í˜•"] = f"{q_score}/10"

    return score, details


def validate_eval_json(parsed):
    """ìµœì¢… í‰ê°€ JSON ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
    required = ["type", "message", "score", "understanding_level", "matched_concepts", "weak_point"]
    valid_levels = ["Excellent", "Good", "Surface", "Poor"]
    issues = []

    for key in required:
        if key not in parsed:
            issues.append(f"missing: {key}")

    if "score" in parsed:
        s = parsed["score"]
        if not isinstance(s, (int, float)) or s < 0 or s > 100:
            issues.append(f"score out of range: {s}")

    if "understanding_level" in parsed:
        lvl = parsed["understanding_level"]
        if lvl not in valid_levels:
            issues.append(f"invalid level: {lvl}")

    if "matched_concepts" in parsed:
        if not isinstance(parsed["matched_concepts"], list):
            issues.append("matched_concepts not array")

    if "message" in parsed:
        msg = parsed["message"]
        if DISPLAY_NAME not in msg:
            issues.append("í˜¸ì¹­ ë¯¸ì‚¬ìš©")

    return len(issues) == 0, issues


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def test_openai_model(model_name, num_runs=3):
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    results = {"model": model_name, "question_turns": [], "eval_turns": [], "errors": []}
    first_q = STEP_CONTEXT['interview_rubric']['first_question']

    for run in range(num_runs):
        print(f"  [Run {run+1}/{num_runs}]", end=" ", flush=True)
        conversation = []

        # â”€â”€ ì§ˆë¬¸ í„´ 1~3 (ìŠ¤íŠ¸ë¦¬ë°) â”€â”€
        for turn in range(1, MAX_TURNS + 1):
            if turn == 1:
                conversation.append({"role": "assistant", "content": first_q})
            conversation.append({"role": "user", "content": USER_ANSWERS[turn - 1]})

            messages = [{"role": "system", "content": build_stream_prompt(turn)}] + conversation

            try:
                t0 = time.time()
                stream = client.chat.completions.create(
                    model=model_name, messages=messages,
                    temperature=0.6, max_completion_tokens=400, stream=True,
                )
                ttft = None
                full_text = ""
                token_count = 0
                for chunk in stream:
                    if ttft is None:
                        ttft = time.time() - t0
                    delta = chunk.choices[0].delta if chunk.choices else None
                    tok = getattr(delta, "content", None) or ""
                    full_text += tok
                    if tok:
                        token_count += 1
                total_time = time.time() - t0

                conversation.append({"role": "assistant", "content": full_text})
                q_score, q_details = evaluate_question_quality(full_text, turn)

                results["question_turns"].append({
                    "run": run+1, "turn": turn,
                    "ttft": round(ttft, 3) if ttft else None,
                    "total_time": round(total_time, 3),
                    "response_length": len(full_text),
                    "token_count": token_count,
                    "quality_score": q_score,
                    "quality_details": q_details,
                    "response_text": full_text,
                })
                print(f"T{turn}âœ“({q_score}ì )", end=" ", flush=True)

            except Exception as e:
                results["errors"].append({"run": run+1, "turn": turn, "type": "question", "error": str(e)})
                print(f"T{turn}âœ—", end=" ", flush=True)
                conversation.append({"role": "assistant", "content": "(error)"})

        # â”€â”€ ìµœì¢… í‰ê°€ í„´ (ë¹„ìŠ¤íŠ¸ë¦¬ë°, JSON) â”€â”€
        conversation.append({"role": "user", "content": USER_ANSWERS[-1]})
        messages = [{"role": "system", "content": build_eval_prompt()}] + conversation

        try:
            t0 = time.time()
            response = client.chat.completions.create(
                model=model_name, messages=messages,
                temperature=0.6, max_completion_tokens=800,
                response_format={"type": "json_object"},
            )
            eval_time = time.time() - t0
            raw = response.choices[0].message.content
            parsed = json.loads(raw)
            schema_ok, schema_issues = validate_eval_json(parsed)

            results["eval_turns"].append({
                "run": run+1,
                "time": round(eval_time, 3),
                "json_valid": True,
                "schema_valid": schema_ok,
                "schema_issues": schema_issues,
                "score": parsed.get("score"),
                "understanding_level": parsed.get("understanding_level"),
                "matched_concepts": parsed.get("matched_concepts", []),
                "weak_point": parsed.get("weak_point"),
                "message": parsed.get("message", ""),
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                },
            })
            print(f"Evalâœ“(score={parsed.get('score')})", flush=True)

        except json.JSONDecodeError:
            results["eval_turns"].append({
                "run": run+1, "time": round(time.time()-t0, 3),
                "json_valid": False, "schema_valid": False, "score": None,
                "raw_preview": (raw[:300] if 'raw' in dir() else "N/A"),
            })
            results["errors"].append({"run": run+1, "type": "eval_json", "error": "JSON parse failed"})
            print("Evalâœ—(JSON)", flush=True)
        except Exception as e:
            results["errors"].append({"run": run+1, "type": "eval", "error": str(e)})
            print(f"Evalâœ—({str(e)[:40]})", flush=True)

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. Gemini ëª¨ë¸ í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def test_gemini_model(model_name="gemini-2.5-flash", num_runs=3):
    genai.configure(api_key=GOOGLE_API_KEY)
    results = {"model": model_name, "question_turns": [], "eval_turns": [], "errors": []}
    first_q = STEP_CONTEXT['interview_rubric']['first_question']

    for run in range(num_runs):
        print(f"  [Run {run+1}/{num_runs}]", end=" ", flush=True)
        conversation = []

        for turn in range(1, MAX_TURNS + 1):
            if turn == 1:
                conversation.append({"role": "model", "parts": [first_q]})
            conversation.append({"role": "user", "parts": [USER_ANSWERS[turn - 1]]})

            model = genai.GenerativeModel(
                model_name=model_name,
                system_instruction=build_stream_prompt(turn),
                generation_config=genai.types.GenerationConfig(temperature=0.6, max_output_tokens=400),
            )

            try:
                t0 = time.time()
                response = model.generate_content(conversation, stream=True)
                ttft = None
                full_text = ""
                token_count = 0
                for chunk in response:
                    if ttft is None:
                        ttft = time.time() - t0
                    txt = chunk.text if hasattr(chunk, 'text') else ""
                    full_text += txt
                    if txt:
                        token_count += 1
                total_time = time.time() - t0

                conversation.append({"role": "model", "parts": [full_text]})
                q_score, q_details = evaluate_question_quality(full_text, turn)

                results["question_turns"].append({
                    "run": run+1, "turn": turn,
                    "ttft": round(ttft, 3) if ttft else None,
                    "total_time": round(total_time, 3),
                    "response_length": len(full_text),
                    "token_count": token_count,
                    "quality_score": q_score,
                    "quality_details": q_details,
                    "response_text": full_text,
                })
                print(f"T{turn}âœ“({q_score}ì )", end=" ", flush=True)

            except Exception as e:
                results["errors"].append({"run": run+1, "turn": turn, "type": "question", "error": str(e)})
                print(f"T{turn}âœ—", end=" ", flush=True)
                conversation.append({"role": "model", "parts": ["(error)"]})

        # â”€â”€ ìµœì¢… í‰ê°€ í„´ â”€â”€
        conversation.append({"role": "user", "parts": [USER_ANSWERS[-1]]})
        model = genai.GenerativeModel(
            model_name=model_name,
            system_instruction=build_eval_prompt(),
            generation_config=genai.types.GenerationConfig(
                temperature=0.6, max_output_tokens=800,
                response_mime_type="application/json",
            ),
        )

        try:
            t0 = time.time()
            response = model.generate_content(conversation)
            eval_time = time.time() - t0
            raw = response.text
            parsed = json.loads(raw)
            schema_ok, schema_issues = validate_eval_json(parsed)

            usage_meta = response.usage_metadata
            results["eval_turns"].append({
                "run": run+1,
                "time": round(eval_time, 3),
                "json_valid": True,
                "schema_valid": schema_ok,
                "schema_issues": schema_issues,
                "score": parsed.get("score"),
                "understanding_level": parsed.get("understanding_level"),
                "matched_concepts": parsed.get("matched_concepts", []),
                "weak_point": parsed.get("weak_point"),
                "message": parsed.get("message", ""),
                "usage": {
                    "prompt_tokens": getattr(usage_meta, 'prompt_token_count', 0),
                    "completion_tokens": getattr(usage_meta, 'candidates_token_count', 0),
                },
            })
            print(f"Evalâœ“(score={parsed.get('score')})", flush=True)

        except json.JSONDecodeError:
            results["eval_turns"].append({
                "run": run+1, "time": round(time.time()-t0, 3),
                "json_valid": False, "schema_valid": False, "score": None,
                "raw_preview": (raw[:300] if 'raw' in dir() else "N/A"),
            })
            results["errors"].append({"run": run+1, "type": "eval_json", "error": "JSON parse failed"})
            print("Evalâœ—(JSON)", flush=True)
        except Exception as e:
            results["errors"].append({"run": run+1, "type": "eval", "error": str(e)})
            print(f"Evalâœ—({str(e)[:50]})", flush=True)

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. ê²°ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def safe_stdev(values):
    return statistics.stdev(values) if len(values) >= 2 else 0.0

def analyze_results(all_results):
    separator = "=" * 80
    print(f"\n{separator}")
    print("  BugHunt ë©´ì ‘ê´€ LLM ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸")
    print(f"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(separator)

    summaries = []  # ì¢…í•©í‘œìš©

    for res in all_results:
        model = res["model"]
        q_turns = res["question_turns"]
        e_turns = res["eval_turns"]
        errors = res["errors"]

        print(f"\n{'â”' * 80}")
        print(f"  â–  ëª¨ë¸: {model}")
        print(f"{'â”' * 80}")

        summary = {"model": model}

        # â”€â”€ A. ì§ˆë¬¸ í„´ ì„±ëŠ¥ â”€â”€
        if q_turns:
            ttfts = [t["ttft"] for t in q_turns if t.get("ttft")]
            totals = [t["total_time"] for t in q_turns]
            lengths = [t["response_length"] for t in q_turns]
            q_scores = [t["quality_score"] for t in q_turns]

            print(f"\n  [A. ì§ˆë¬¸ í„´ ì„±ëŠ¥] (ì´ {len(q_turns)}íšŒ = {len(q_turns)//3}runs Ã— 3turns)")
            print(f"    â”Œ{'â”€'*50}")
            print(f"    â”‚ TTFT (ì²« í† í°)  : í‰ê·  {sum(ttfts)/len(ttfts):.3f}s  (min {min(ttfts):.3f}s / max {max(ttfts):.3f}s)")
            print(f"    â”‚ ì´ ì‘ë‹µ ì‹œê°„    : í‰ê·  {sum(totals)/len(totals):.3f}s  (min {min(totals):.3f}s / max {max(totals):.3f}s)")
            print(f"    â”‚ ì‘ë‹µ ê¸¸ì´       : í‰ê·  {sum(lengths)//len(lengths)}ì  (min {min(lengths)}ì / max {max(lengths)}ì)")
            print(f"    â”‚ ì§ˆë¬¸ í’ˆì§ˆ ì ìˆ˜  : í‰ê·  {sum(q_scores)/len(q_scores):.1f}/100  (stdev {safe_stdev(q_scores):.1f})")
            print(f"    â””{'â”€'*50}")

            summary["avg_ttft"] = round(sum(ttfts)/len(ttfts), 3)
            summary["avg_q_time"] = round(sum(totals)/len(totals), 3)
            summary["avg_q_quality"] = round(sum(q_scores)/len(q_scores), 1)

            # í„´ë³„ ìƒ˜í”Œ (Run 1)
            print(f"\n  [B. ì§ˆë¬¸ ìƒ˜í”Œ (Run 1)]")
            for t in q_turns:
                if t["run"] == 1:
                    text = t["response_text"].replace("\n", " ").strip()
                    print(f"    Turn {t['turn']} (í’ˆì§ˆ:{t['quality_score']}ì ): {text[:200]}")
                    for k, v in t["quality_details"].items():
                        print(f"      {k}: {v}", end="  ")
                    print()

        # â”€â”€ C. ìµœì¢… í‰ê°€ ì„±ëŠ¥ â”€â”€
        if e_turns:
            scores = [t["score"] for t in e_turns if t.get("score") is not None]
            times = [t["time"] for t in e_turns]
            json_ok = sum(1 for t in e_turns if t.get("json_valid"))
            schema_ok = sum(1 for t in e_turns if t.get("schema_valid"))

            print(f"\n  [C. ìµœì¢… í‰ê°€ ì„±ëŠ¥] ({len(e_turns)}íšŒ)")
            print(f"    â”Œ{'â”€'*55}")
            print(f"    â”‚ JSON íŒŒì‹± ì„±ê³µ   : {json_ok}/{len(e_turns)} ({json_ok/len(e_turns)*100:.0f}%)")
            print(f"    â”‚ ìŠ¤í‚¤ë§ˆ ì™„ì „ ì¤€ìˆ˜  : {schema_ok}/{len(e_turns)} ({schema_ok/len(e_turns)*100:.0f}%)")
            if scores:
                avg = sum(scores)/len(scores)
                sd = safe_stdev(scores)
                print(f"    â”‚ ì ìˆ˜ í‰ê·        : {avg:.1f}ì ")
                print(f"    â”‚ ì ìˆ˜ ë²”ìœ„       : {min(scores)} ~ {max(scores)} (í¸ì°¨ {max(scores)-min(scores)}ì )")
                print(f"    â”‚ ì ìˆ˜ í‘œì¤€í¸ì°¨   : {sd:.2f}")
                levels = [t.get("understanding_level", "?") for t in e_turns]
                print(f"    â”‚ ì´í•´ë„ ë“±ê¸‰     : {levels}")
                summary["avg_score"] = round(avg, 1)
                summary["score_stdev"] = round(sd, 2)
                summary["score_range"] = f"{min(scores)}~{max(scores)}"
            print(f"    â”‚ ì‘ë‹µ ì‹œê°„ í‰ê·    : {sum(times)/len(times):.3f}s")
            print(f"    â””{'â”€'*55}")

            summary["json_rate"] = f"{json_ok}/{len(e_turns)}"
            summary["schema_rate"] = f"{schema_ok}/{len(e_turns)}"
            summary["avg_eval_time"] = round(sum(times)/len(times), 3)

            # ìƒì„¸ í‰ê°€ ê²°ê³¼
            print(f"\n  [D. í‰ê°€ ìƒì„¸ (Runë³„)]")
            total_prompt = 0
            total_completion = 0
            for t in e_turns:
                print(f"    Run {t['run']}: score={t.get('score')} | level={t.get('understanding_level')} "
                      f"| json={t.get('json_valid')} | schema={t.get('schema_valid')}")
                if t.get("schema_issues"):
                    print(f"      âš  ìŠ¤í‚¤ë§ˆ ì´ìŠˆ: {t['schema_issues']}")
                if t.get("matched_concepts"):
                    print(f"      ë§¤ì¹­ ê°œë…: {t['matched_concepts']}")
                if t.get("weak_point"):
                    print(f"      ì•½ì : {t['weak_point']}")
                if t.get("message"):
                    print(f"      í”¼ë“œë°±: {t['message'][:150]}")
                if t.get("usage"):
                    u = t["usage"]
                    total_prompt += u.get("prompt_tokens", 0)
                    total_completion += u.get("completion_tokens", 0)
                    print(f"      í† í°: prompt={u.get('prompt_tokens',0)} / completion={u.get('completion_tokens',0)}")

            # ë¹„ìš© ì¶”ì •
            cost_info = COST_PER_1M.get(model, {})
            if cost_info and total_prompt > 0:
                est_cost = (total_prompt * cost_info["input"] + total_completion * cost_info["output"]) / 1_000_000
                per_session = est_cost / len(e_turns)
                print(f"\n    ğŸ’° ë¹„ìš© ì¶”ì • (í‰ê°€ í„´ë§Œ): ì´ ${est_cost:.6f} / ì„¸ì…˜ë‹¹ ${per_session:.6f}")
                summary["est_cost_per_session"] = f"${per_session:.6f}"

        # ì—ëŸ¬ ìš”ì•½
        if errors:
            print(f"\n  [E. ì—ëŸ¬] {len(errors)}ê±´")
            for e in errors:
                print(f"    Run {e.get('run')}: [{e.get('type')}] {e.get('error','')[:80]}")
        summary["error_count"] = len(errors)

        summaries.append(summary)

    # â•â•â• ì¢…í•© ë¹„êµí‘œ â•â•â•
    print(f"\n{'â”' * 80}")
    print("  â–  ì¢…í•© ë¹„êµí‘œ")
    print(f"{'â”' * 80}")

    col_w = 24
    header = f"  {'í•­ëª©':<22}"
    for s in summaries:
        header += f"â”‚ {s['model']:<{col_w}}"
    print(header)
    print("  " + "â”€" * (22 + (col_w + 2) * len(summaries)))

    rows = [
        ("í‰ê·  TTFT (ìŠ¤íŠ¸ë¦¬ë°)", "avg_ttft", "s"),
        ("í‰ê·  ì§ˆë¬¸ ì‘ë‹µì‹œê°„", "avg_q_time", "s"),
        ("í‰ê·  í‰ê°€ ì‘ë‹µì‹œê°„", "avg_eval_time", "s"),
        ("ì§ˆë¬¸ í’ˆì§ˆ ì ìˆ˜", "avg_q_quality", "/100"),
        ("JSON íŒŒì‹± ì„±ê³µë¥ ", "json_rate", ""),
        ("ìŠ¤í‚¤ë§ˆ ì™„ì „ ì¤€ìˆ˜", "schema_rate", ""),
        ("í‰ê·  ì ìˆ˜", "avg_score", "ì "),
        ("ì ìˆ˜ í‘œì¤€í¸ì°¨", "score_stdev", ""),
        ("ì ìˆ˜ ë²”ìœ„", "score_range", ""),
        ("ì¶”ì • ë¹„ìš©/ì„¸ì…˜", "est_cost_per_session", ""),
        ("ì—ëŸ¬ ê±´ìˆ˜", "error_count", "ê±´"),
    ]

    for label, key, unit in rows:
        row = f"  {label:<22}"
        for s in summaries:
            val = s.get(key, "N/A")
            if val != "N/A" and unit:
                val = f"{val}{unit}"
            row += f"â”‚ {str(val):<{col_w}}"
        print(row)

    print()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. ë©”ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    NUM_RUNS = 3

    print("=" * 80)
    print("  BugHunt ë©´ì ‘ê´€ LLM ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ v2")
    print(f"  ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  ì‹œë‚˜ë¦¬ì˜¤: S4 Step 1 (Gradient Bug - zero_grad ëˆ„ë½)")
    print(f"  ë°˜ë³µ: {NUM_RUNS}íšŒ/ëª¨ë¸ | í”„ë¡¬í”„íŠ¸: ê³ ì • | ìœ ì € ë‹µë³€: ê³ ì •")
    print(f"  í‰ê°€ í•­ëª©: TTFT, ì‘ë‹µì‹œê°„, ì§ˆë¬¸í’ˆì§ˆ, JSONì¤€ìˆ˜, ì±„ì ì¼ê´€ì„±, ë¹„ìš©")
    print("=" * 80)

    all_results = []

    print(f"\nâ–¶ [1/3] gpt-4o-mini í…ŒìŠ¤íŠ¸...")
    all_results.append(test_openai_model("gpt-4o-mini", NUM_RUNS))

    print(f"\nâ–¶ [2/3] gpt-5.2 í…ŒìŠ¤íŠ¸...")
    all_results.append(test_openai_model("gpt-5.2", NUM_RUNS))

    print(f"\nâ–¶ [3/3] gemini-2.5-flash í…ŒìŠ¤íŠ¸...")
    all_results.append(test_gemini_model("gemini-2.5-flash", NUM_RUNS))

    analyze_results(all_results)

    # JSON ì €ì¥
    output_path = Path(__file__).parent / "model_comparison_results.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
    print(f"ğŸ“ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_path}")
