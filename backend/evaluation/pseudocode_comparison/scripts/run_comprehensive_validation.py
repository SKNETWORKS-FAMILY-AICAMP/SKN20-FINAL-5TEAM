"""
[ì˜ì‚¬ì½”ë“œ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸] 3-Option í‰ê°€ ì‹œìŠ¤í…œ ì „ìš© í…ŒìŠ¤íŠ¸
ìˆ˜ì •ì¼: 2026-02-15
ìˆ˜ì •ë‚´ìš©: ì˜ì‚¬ì½”ë“œ í‰ê°€ 3ê°€ì§€ ì˜µì…˜(Multimodel, GPT-only, Hybrid)ì„ ì •êµí•˜ê²Œ ê²€ì¦
"""

import os
import sys
import json
import time
import django
from pathlib import Path

# Django ì„¤ì • ë¡œë“œ
current_dir = Path(__file__).resolve().parent
backend_dir = current_dir.parent.parent.parent
sys.path.insert(0, str(backend_dir))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
django.setup()

from core.services.pseudocode_evaluator import PseudocodeEvaluator, EvaluationRequest, EvaluationMode

class PseudocodeThreeOptionValidator:
    def __init__(self):
        self.pseudocode_evaluator = PseudocodeEvaluator()
        self.results_dir = backend_dir / 'evaluation' / 'pseudocode_comparison' / 'results'
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
    def test_option_1_multimodel(self):
        """ì˜µì…˜ 1: ë‹¤ì¤‘ ëª¨ë¸ (ë³‘ë ¬ í‰ê°€) ê²€ì¦"""
        print("\n" + "="*60)
        print("ğŸ” TEST: Option 1 (Always Multimodel)")
        print("="*60)
        return self._run_test(EvaluationMode.OPTION1_ALWAYS_MULTIMODEL)

    def test_option_2_gptonly(self):
        """ì˜µì…˜ 2: GPT-4o-mini ë‹¨ì¼ ëª¨ë¸ ê²€ì¦"""
        print("\n" + "="*60)
        print("ğŸ” TEST: Option 2 (GPT-4o-mini Only)")
        print("="*60)
        return self._run_test(EvaluationMode.OPTION2_GPTONLY)

    def test_option_3_hybrid(self):
        """ì˜µì…˜ 3: í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ê²€ì¦"""
        print("\n" + "="*60)
        print("ğŸ” TEST: Option 3 (Hybrid Strategy)")
        print("="*60)
        return self._run_test(EvaluationMode.OPTION3_HYBRID)

    def _run_test(self, mode):
        test_case = {
            "quest_id": "1",
            "quest_title": "ë°ì´í„° ì „ì²˜ë¦¬ ëˆ„ìˆ˜ ì°¨ë‹¨",
            "pseudocode": "1. ë°ì´í„°ë¥¼ ë‚˜ëˆˆë‹¤.\n2. í•™ìŠµ ë°ì´í„°ë¡œë§Œ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ fit í•œë‹¤.\n3. ë‘˜ ë‹¤ transform í•œë‹¤."
        }
        
        start_time = time.time()
        request = EvaluationRequest(
            user_id="test_user",
            detail_id=test_case["quest_id"],
            pseudocode=test_case["pseudocode"],
            quest_title=test_case["quest_title"],
            mode=mode
        )
        
        try:
            result = self.pseudocode_evaluator.evaluate(request)
            elapsed = time.time() - start_time
            print(f"  âœ… Result: Score={result.final_score}, Grade={result.grade}")
            print(f"  â±ï¸ Time: {elapsed:.2f}s")
            print(f"  ğŸ“¦ Models: {', '.join(result.llm_evaluations.keys())}")
            
            return {
                "score": result.final_score,
                "grade": result.grade,
                "latency": elapsed,
                "models": list(result.llm_evaluations.keys())
            }
        except Exception as e:
            print(f"  âŒ Error: {str(e)}")
            return {"error": str(e)}

    def run_all(self):
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "results": {
                "option1": self.test_option_1_multimodel(),
                "option2": self.test_option_2_gptonly(),
                "option3": self.test_option_3_hybrid()
            }
        }
        
        # íŒŒì¼ëª…ë„ ë” ì •í™•í•˜ê²Œ ë³€ê²½
        output_file = self.results_dir / "PSEUDOCODE_3OPTION_VALIDATION.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        print("\n" + "="*60)
        print(f"ğŸ‰ Pseudocode 3-Option Test Complete!")
        print(f"Report: {output_file}")
        print("="*60)

if __name__ == "__main__":
    validator = PseudocodeThreeOptionValidator()
    validator.run_all()
