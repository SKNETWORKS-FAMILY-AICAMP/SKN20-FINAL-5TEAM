import os
import sys
import json
import time
import re
import django
from pathlib import Path
from typing import Dict, Any

# Django ì„¤ì • ë¡œë“œ
current_dir = Path(__file__).resolve().parent
backend_dir = current_dir.parent.parent.parent
sys.path.insert(0, str(backend_dir))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
django.setup()

try:
    from backend.evaluation.model_comparison.scripts.model_evaluator import get_evaluator
except ImportError:
    try:
        from evaluation.model_comparison.scripts.model_evaluator import get_evaluator
    except ImportError:
        def get_evaluator(model): raise ImportError("Evaluator not found")

# ì‹¤ì œ PSEUDO CODE í‰ê°€ ë¡œì§ì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (ê³ ë¦½ì„ ìœ„í•´ í•˜ë“œì½”ë”© ê¶Œì¥ë˜ë‚˜ ì—¬ê¸°ì„  ì°¸ì¡° ì‹œë„)
try:
    from core.views.pseudocode_evaluation import SYSTEM_PROMPT, MISSION_BLUEPRINTS
except ImportError:
    # ì°¸ì¡°ê°€ ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°±ì—… (ì‹¤ì œ íŒŒì¼ ë‚´ìš©ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
    SYSTEM_PROMPT = "ë‹¹ì‹ ì€ AI ê¸°ë°˜ ë°ì´í„° ê³¼í•™ ì„¤ê³„ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤..."
    MISSION_BLUEPRINTS = {"1": {"mission_goal": "ë°ì´í„° ì „ì²˜ë¦¬ ëˆ„ìˆ˜ ë°©ì§€", "critical_constraints": [], "required_keywords": []}}

class PseudocodeComparisonRunner:
    def __init__(self, samples_file, output_dir, models, num_trials=3):
        self.samples_file = Path(samples_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.models = models
        self.num_trials = num_trials
        self.evaluators = {}

        for model in models:
            try:
                self.evaluators[model] = get_evaluator(model)
                print(f"âœ… {model} evaluator ready")
            except Exception as e:
                print(f"âŒ {model} initialization failed: {e}")

    def create_prompt(self, sample):
        quest_id = sample['quest_id']
        quest_title = sample['quest_title']
        pseudocode = sample['pseudocode']
        
        blueprint = MISSION_BLUEPRINTS.get(quest_id, MISSION_BLUEPRINTS.get("1"))
        
        user_prompt = f"""
# [Evaluation Context: Mission Blueprint]
- Goal: {blueprint.get('mission_goal', 'ì „ì²˜ë¦¬')}
- Critical Constraints: {", ".join(blueprint.get('critical_constraints', []))}
- Required Keywords: {", ".join(blueprint.get('required_keywords', []))}

# [User Input]
- Title: {quest_title}
- Pseudocode: {pseudocode}
- Diagnostic Context: N/A

# [Task]
ìœ„ [Mission Blueprint]ì˜ ì œì•½ ì‚¬í•­ì„ ì–¼ë§ˆë‚˜ ì¶©ì‹¤íˆ ì„¤ê³„ì— ë°˜ì˜í–ˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
- AI ì ìˆ˜ëŠ” ì´ 85ì  ë§Œì ìœ¼ë¡œ ì±„ì í•©ë‹ˆë‹¤. (ì§€í‘œë³„ í•©ì‚°)
- ì ìˆ˜ ê²°ê³¼ì— ë”°ë¼ ë§ì¶¤í˜• MCQ(tail_question or deep_dive)ë¥¼ ìƒì„±í•˜ì„¸ìš”. 
- ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ Python ì½”ë“œë¡œ ë³€í™˜í•˜ì„¸ìš”.
"""
        return SYSTEM_PROMPT, user_prompt

    def run_all(self):
        with open(self.samples_file, 'r', encoding='utf-8') as f:
            samples = json.load(f)

        all_results = {}
        for model_name in self.models:
            print(f"\nğŸš€ Testing Model: {model_name}")
            model_evals = []
            evaluator = self.evaluators.get(model_name)
            if not evaluator: continue

            for sample in samples:
                print(f"  - Sample: {sample['sample_id']} ({sample['quality_level']})")
                trials = []
                system_p, user_p = self.create_prompt(sample)
                
                for i in range(self.num_trials):
                    res = evaluator.evaluate(system_p, user_p)
                    if res['success']:
                        score = res['result'].get('overall_score', 0)
                        print(f"    Trial {i+1}: Score={score} ({res['time']:.1f}s)")
                        trials.append(res)
                    else:
                        print(f"    Trial {i+1}: FAILED - {res.get('error')}")
                    time.sleep(1)
                
                model_evals.append({
                    "sample_id": sample['sample_id'],
                    "quality_level": sample['quality_level'],
                    "expected_range": sample['expected_score_range'],
                    "trials": trials
                })
            
            all_results[model_name] = {
                "model": model_name,
                "evaluations": model_evals,
                "stats": evaluator.get_stats()
            }
            
            # Save intermediate result
            model_filename = model_name.replace("/", "_").replace("-", "_")
            with open(self.output_dir / f"PSEUDOCODE_{model_filename}_results.json", 'w', encoding='utf-8') as f:
                json.dump(all_results[model_name], f, ensure_ascii=False, indent=2)

        # Final Summary
        with open(self.output_dir / "PSEUDOCODE_comparison_summary.json", 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… All tests complete. Results saved in {self.output_dir}")

if __name__ == "__main__":
    # [2026-02-15] GPT vs Gemini vs Llama 3ì ëŒ€ê²° ì„¤ì •
    models_to_test = [
        'gpt-4o-mini', 
        'gemini-1.5-flash', 
        'llama-3.3-70b-versatile'
    ]
    samples_path = backend_dir / 'evaluation' / 'pseudocode_comparison' / 'samples' / 'pseudocode_validation_samples.json'
    out_path = backend_dir / 'evaluation' / 'pseudocode_comparison' / 'results'
    
    runner = PseudocodeComparisonRunner(samples_path, out_path, models_to_test, num_trials=2)
    runner.run_all()
