"""
ì˜ì‚¬ì½”ë“œ í‰ê°€ ì—”ì§„ - í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ê°€ì´ë“œ
ì‘ì„±ì¼: 2026-02-15

ì´ íŒŒì¼ì€ pseudocode_evaluator.pyì˜ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³  
ì‹¤ì œ ì‚¬ìš© ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œë“¤ì…ë‹ˆë‹¤.
"""

import os
import sys
import json
from pathlib import Path

# Django ì„¤ì •
django_setup_path = Path(__file__).parent.parent.parent
sys.path.insert(0, str(django_setup_path))

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')

import django
django.setup()

from core.services.pseudocode_evaluator import (
    PseudocodeEvaluator,
    EvaluationRequest,
    EvaluationMode,
    RuleValidationEngine as LocalValidationEngine  # êµ¬ë²„ì „ í˜¸í™˜
)


# ============================================================================
# 1. í…ŒìŠ¤íŠ¸ ìƒ˜í”Œë“¤ (ë‹¤ì–‘í•œ í’ˆì§ˆì˜ ì˜ì‚¬ì½”ë“œ)
# ============================================================================

TEST_SAMPLES = {
    # ì •ë‹µ - ì™„ë²½í•œ ì˜ì‚¬ì½”ë“œ
    'perfect': """
1. Raw ë°ì´í„° ë¡œë“œ
2. Train/Test ë°ì´í„° ë¶„í•  (80/20)
3. Train ë°ì´í„°ë¡œë§Œ StandardScaler fit
4. Trainê³¼ Test ë°ì´í„° ëª¨ë‘ StandardScaler transform ì ìš©
5. DecisionTreeClassifier ëª¨ë¸ ì´ˆê¸°í™”
6. Train ë°ì´í„°ë¡œ ëª¨ë¸ fit
7. Test ë°ì´í„°ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
8. ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ê³„ì‚°
9. ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ 
""",
    
    # ì¢‹ì€ ì˜ì‚¬ì½”ë“œ - ì•½ê°„ì˜ ê°œì„  í•„ìš”
    'good': """
1. ë°ì´í„° ë¡œë“œ
2. Train/Test ë¶„í• 
3. Train ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ fit
4. ë°ì´í„° transform
5. ëª¨ë¸ ìƒì„±
6. ëª¨ë¸ fit
7. ì˜ˆì¸¡
8. ì ìˆ˜ ê³„ì‚°
""",
    
    # í‰ë²”í•œ ì˜ì‚¬ì½”ë“œ - ê°œë…ì€ ìˆì§€ë§Œ ë¶ˆëª…í™•
    'average': """
1. ë°ì´í„° ì¤€ë¹„
2. ë°ì´í„° ë¶„ë¦¬
3. ì „ì²˜ë¦¬
4. ëª¨ë¸ í•™ìŠµ
5. í‰ê°€
""",
    
    # ë°ì´í„° ëˆ„ìˆ˜ - ì¹˜ëª…ì  ì˜¤ë¥˜
    'data_leakage': """
1. ë°ì´í„° ë¡œë“œ
2. ì „ì²´ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ fit  <-- ì˜¤ë¥˜! í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´ í¬í•¨
3. ì „ì²´ ë°ì´í„° transform
4. Train/Test ë¶„í• 
5. ëª¨ë¸ fit
6. ì˜ˆì¸¡
7. í‰ê°€
""",
    
    # ìˆœì„œ ì˜¤ë¥˜ - fitì´ split ì „ì—
    'wrong_order': """
1. ë°ì´í„° ë¡œë“œ
2. ìŠ¤ì¼€ì¼ëŸ¬ fit (ë°ì´í„°)
3. Transform
4. Train/Test ë¶„í• 
5. ëª¨ë¸ í•™ìŠµ
6. ì˜ˆì¸¡
""",
    
    # ë„ˆë¬´ ì§§ì€ ì˜ì‚¬ì½”ë“œ - ë„ˆë¬´ ì¶”ìƒì 
    'too_short': """
1. ë°ì´í„° ì¤€ë¹„
2. í•™ìŠµ
3. ì˜ˆì¸¡
""",
    
    # ê°œë… ëˆ„ë½ - splitì´ ì—†ìŒ
    'missing_concepts': """
1. ë°ì´í„° ë¡œë“œ
2. ì „ì²˜ë¦¬ (ì •ê·œí™”)
3. ëª¨ë¸ fit
4. ì˜ˆì¸¡
5. í‰ê°€
""",
}


# ============================================================================
# 2. ê¸°ë³¸ ì„¤ëª…: í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ êµ¬ì„±
# ============================================================================

def explain_script_structure():
    """
    pseudocode_evaluator.pyì˜ êµ¬ì¡° ì„¤ëª…
    """
    print("\n" + "="*70)
    print("ğŸ“š pseudocode_evaluator.py êµ¬ì¡° ì„¤ëª…")
    print("="*70)
    
    explanation = """
ã€8ê°œ í•µì‹¬ ì—”ì§„ã€‘

1ï¸âƒ£ EvaluationMode (Enum)
   â€¢ STANDARD: GPT-4o-minië§Œ ì‚¬ìš© (ê¸°ë³¸, ë¹ ë¦„)
   â€¢ COMPARISON: 3ê°œ ëª¨ë¸ ë¹„êµ (ìƒì„¸, ëŠë¦¼)
   â€¢ DEEP_DIVE: ë¹„êµ + ì‹¬í™” ì§ˆë¬¸ (ë¯¸êµ¬í˜„)

2ï¸âƒ£ LocalValidationEngine
   â€¢ Rule ê¸°ë°˜ ê²€ì¦ (ë¹ ë¦„, ~50ms)
   â€¢ ì˜ì‚¬ì½”ë“œ í˜•ì‹, ê°œë…, ìˆœì„œ í™•ì¸
   â€¢ critic_patterns, required_concepts, dependencies ì •ì˜

3ï¸âƒ£ LLMEvaluationEngine
   â€¢ OpenAI (GPT-4o-mini)
   â€¢ Google Gemini (1.5 Flash)
   â€¢ Groq (Llama 3.3)
   â€¢ 3ë°•ì í”„ë¡¬í”„íŠ¸: System + Module-specific + User

4ï¸âƒ£ ScoringEngine
   â€¢ Self-Audit: ì¹˜ëª…ì  ì˜¤ë¥˜ ê°ì§€
   â€¢ Score Ceiling: ìƒí•œì„  ì ìš© (40ì )
   â€¢ Grade Normalization: ë“±ê¸‰ë³„ ì •ê·œí™”
   â€¢ Multi-model aggregation: ê°€ì¤‘í‰ê·  (GPT 50%, Gemini 30%, Llama 20%)

5ï¸âƒ£ FeedbackEngine
   â€¢ ìš”ì•½ ë¬¸ì¥ ìƒì„±
   â€¢ í˜ë¥´ì†Œë‚˜ ë¶„ë¥˜ (5ê°€ì§€): ì™„ë²½í•œ ì„¤ê³„ì, ì´ë¡ ê°€, ì½”ë”, ê¸°íšì, ì´ˆì‹¬ì
   â€¢ ë‹¤ìŒ ë‹¨ê³„ ì œì‹œ
   â€¢ í•™ìŠµ ìì› ì¶”ì²œ

6ï¸âƒ£ Model Selection
   â€¢ PRIMARY_MODEL = "gpt-4o-mini"
   â€¢ BACKUP_MODELS = ["gemini-1.5-flash", "llama-3.3-70b-versatile"]

7ï¸âƒ£ ë°ì´í„° í´ë˜ìŠ¤
   â€¢ EvaluationRequest: ì…ë ¥
   â€¢ LocalValidationResult: ë¡œì»¬ ê²€ì¦ ê²°ê³¼
   â€¢ LLMEvaluationResult: ê° ëª¨ë¸ ê²°ê³¼
   â€¢ FinalEvaluationResult: ìµœì¢… ê²°ê³¼

8ï¸âƒ£ PseudocodeEvaluator (ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°)
   â€¢ 5ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
   â€¢ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ í‰ê°€ ë°©ì‹ ì ìš©
   â€¢ ìë™ í´ë°± (í‘œì¤€ ì‹¤íŒ¨ ì‹œ ë¹„êµ ëª¨ë“œ ì „í™˜)

ã€5ë‹¨ê³„ í‰ê°€ íŒŒì´í”„ë¼ì¸ã€‘

[Step 1] ë¡œì»¬ ê²€ì¦ (~50ms)
   ì…ë ¥ê°’ â†’ í˜•ì‹/ê°œë…/ìˆœì„œ í™•ì¸ â†’ ë¡œì»¬ ì ìˆ˜

[Step 2] LLM í‰ê°€ (í‘œì¤€/ë¹„êµ ëª¨ë“œ)
   ì˜ì‚¬ì½”ë“œ + ë¡œì»¬ ê²°ê³¼ â†’ 3ë°•ì í”„ë¡¬í”„íŠ¸ â†’ LLM í˜¸ì¶œ â†’ ì ìˆ˜ + í”¼ë“œë°±

[Step 3] ì ìˆ˜ ì •ê·œí™”
   LLM ì ìˆ˜ â†’ ìê°€ì§„ë‹¨ â†’ ìƒí•œì„  â†’ ê·¸ë ˆì´ë“œ ì •ê·œí™” â†’ ìµœì¢… ì ìˆ˜

[Step 4] í”¼ë“œë°± ìƒì„±
   LLM ì‘ë‹µ + ì ìˆ˜ + ë‹¤ì¤‘ ëª¨ë¸ ê²°ê³¼ â†’ ìš”ì•½/ê°•ì /ê°œì„ /í˜ë¥´ì†Œë‚˜/ë‹¤ìŒë‹¨ê³„

[Step 5] ìµœì¢… ê²°ê³¼ í¬ì¥
   ëª¨ë“  ì •ë³´ í†µí•© â†’ JSON ë°˜í™˜ â†’ ì €ì¥/ì‘ë‹µ

ã€3ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°ã€‘

SYSTEM PROMPT (ê³ ì •)
â”œâ”€ ì—­í• : ì˜ì‚¬ì½”ë“œ í‰ê°€ ì „ë¬¸ê°€
â”œâ”€ 5ì°¨ì› ë£¨ë¸Œë¦­ ì •ì˜
â””â”€ JSON ì‘ë‹µ í˜•ì‹

MODULE-SPECIFIC PROMPT (ëª¨ë“ˆë³„)
â”œâ”€ ë¯¸ì…˜ ì„¤ëª…
â”œâ”€ í•„ìˆ˜ ê°œë…
â””â”€ í‰ê°€ ì¤‘ì 

USER PROMPT (ë™ì )
â”œâ”€ ì‚¬ìš©ì ì˜ì‚¬ì½”ë“œ
â”œâ”€ ë¡œì»¬ ê²€ì¦ ê²°ê³¼
â””â”€ í‰ê°€ ìš”ì²­
"""
    
    print(explanation)


# ============================================================================
# 3. ê¸°ë³¸ ì‚¬ìš©ë²•
# ============================================================================

def example_1_option1_always_multimodel():
    """
    ì˜ˆì œ 1: ì˜µì…˜1 - í•­ìƒ 3ê°œ ëª¨ë¸ ì‚¬ìš© (ìµœê³  ì‹ ë¢°ë„)
    """
    print("\n" + "="*70)
    print("ğŸ“ [ì˜µì…˜1] í•­ìƒ 3ê°œ ëª¨ë¸ ì‚¬ìš©")
    print("="*70)
    
    print("""
ì „ëµ: GPT (50%) + Gemini (30%) + Llama (20%) ê°€ì¤‘í‰ê· 

ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
  â€¢ êµìˆ˜/ê´€ë¦¬ì ìµœì¢… ê²€ìˆ˜ (ìµœê³  ì‹ ë¢°ë„ í•„ìš”)
  â€¢ í•™ìƒ ë¯¼ì› ì‘ì²˜ (3ê°œ ëª¨ë¸ í•©ì˜)
  â€¢ ì¤‘ìš” í‰ê°€ ê¸°ë¡ (ê°ì‚¬ ì¶”ì )
  â€¢ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„

íŠ¹ì§•:
  âœ… ìµœê³ ì˜ ì‹ ë¢°ë„ (3ê°œ ëª¨ë¸ ê²€ì¦)
  âœ… ëª¨ë¸ ê°„ í•©ì˜ ê°•í™”
  âœ… ê° ëª¨ë¸ì˜ ê°•ì  í™œìš©
  âŒ ëŠë¦¼ (~8-10ì´ˆ)
  âŒ ë†’ì€ ë¹„ìš© (3ë°°)
""")
    
    code_example = """
from core.services.pseudocode_evaluator import (
    PseudocodeEvaluator,
    EvaluationRequest,
    EvaluationMode
)

evaluator = PseudocodeEvaluator()

# ì˜µì…˜1: í•­ìƒ 3ê°œ ëª¨ë¸
request = EvaluationRequest(
    user_id='user_123',
    detail_id='unit0401',
    pseudocode='... ì˜ì‚¬ì½”ë“œ ...',
    mode=EvaluationMode.OPTION2_GPTONLY  # í˜„ì¬ ì§€ì› ëª¨ë“œ
)

result = evaluator.evaluate(request)

# ê²°ê³¼: ëª¨ë¸ë³„ ì ìˆ˜ + ê°€ì¤‘í‰ê· 
print(f"ìµœì¢… ì ìˆ˜: {result.final_score}ì  (3ëª¨ë¸ ê°€ì¤‘í‰ê· )")
print(f"ì‹ ë¢°ë„: {result.feedback.get('model_comparison', {}).get('confidence', 0):.0%}")

# ëª¨ë¸ë³„ ìƒì„¸ ë¹„êµ
for model, eval_result in result.llm_evaluations.items():
    print(f"{model}: {eval_result.raw_score}ì ")
"""
    
    print("\nğŸ’» ì½”ë“œ:")
    print(code_example)


def example_2_option2_gptonly():
    """
    ì˜ˆì œ 2: ì˜µì…˜2 - GPTë§Œ ì‚¬ìš© (ë¹ ë¥´ê³  ì €ë¹„ìš©)
    """
    print("\n" + "="*70)
    print("ğŸ“ [ì˜µì…˜2] GPT-4o-minië§Œ ì‚¬ìš©")
    print("="*70)
    
    print("""
ì „ëµ: ë‹¨ì¼ ëª¨ë¸ë¡œ ë¹ ë¥¸ í‰ê°€

ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
  â€¢ ì¼ë°˜ í•™ìƒ í‰ê°€ (í”¼ë“œë°± ì†ë„ ì¤‘ìš”)
  â€¢ ëŒ€ëŸ‰ í‰ê°€ (ë¹„ìš© ì ˆê°)
  â€¢ í”„ë¡œë•ì…˜ ê¸°ë³¸ í‰ê°€ (ì‹ ë¢°ë„ 95%+)
  â€¢ ì‹¤ì‹œê°„ í”¼ë“œë°±

íŠ¹ì§•:
  âœ… ë§¤ìš° ë¹ ë¦„ (~2-3ì´ˆ)
  âœ… ì €ë¹„ìš© (1/3)
  âœ… ë†’ì€ ì‹ ë¢°ë„ (95%+)
  âŒ ë‹¨ì¼ ëª¨ë¸ (ê²€ì¦ ë¶ˆê°€)
  âŒ ëª¨ë¸ ì‹¤íŒ¨ ì‹œ í´ë°± í•„ìš”
""")
    
    code_example = """
from core.services.pseudocode_evaluator import (
    PseudocodeEvaluator,
    EvaluationRequest,
    EvaluationMode
)

evaluator = PseudocodeEvaluator()

# ì˜µì…˜2: GPTë§Œ
request = EvaluationRequest(
    user_id='user_123',
    detail_id='unit0401',
    pseudocode='... ì˜ì‚¬ì½”ë“œ ...',
    mode=EvaluationMode.OPTION2_GPTONLY  # GPTë§Œ!
)

result = evaluator.evaluate(request)

print(f"ìµœì¢… ì ìˆ˜: {result.final_score}ì ")
print(f"ì‚¬ìš© ëª¨ë¸: GPT-4o-mini (ë‹¨ì¼)")
print(f"ì†Œìš” ì‹œê°„: {result.metadata['total_latency_ms']}ms")
"""
    
    print("\nğŸ’» ì½”ë“œ:")
    print(code_example)


def example_3_option3_hybrid():
    """
    ì˜ˆì œ 3: ì˜µì…˜3 - í•˜ì´ë¸Œë¦¬ë“œ (ë¡œì»¬ ê¸°ë°˜ ì„ íƒ)
    """
    print("\n" + "="*70)
    print("ğŸ“ [ì˜µì…˜3] í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ")
    print("="*70)
    
    print("""
ì „ëµ: ë¡œì»¬ ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ëª¨ë¸ ì„ íƒ

ë™ì‘ ë°©ì‹:
  1. ë¡œì»¬ ê²€ì¦ ì‹¤í–‰
  2. í†µê³¼ â†’ GPTë§Œ ì‚¬ìš© (ë¹ ë¦„)
  3. ì‹¤íŒ¨ â†’ 3ê°œ ëª¨ë¸ ì‚¬ìš© (ê²€ì¦)

ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
  â€¢ ìë™ í‰ê°€ ì‹œìŠ¤í…œ (ê· í˜•ì¡íŒ ë¹„ìš©/ì‹ ë¢°ë„)
  â€¢ ë¬¸ì œ ìˆëŠ” ë‹µë³€ë§Œ ì¬ê²€ì¦
  â€¢ ë™ì  ìì› í• ë‹¹

íŠ¹ì§•:
  âœ… íš¨ìœ¨ì  (í•„ìš”ì‹œì—ë§Œ 3ëª¨ë¸)
  âœ… í•©ë¦¬ì  ë¹„ìš©
  âœ… ë†’ì€ ì‹ ë¢°ë„
  âœ… ìë™ í´ë°±
""")
    
    code_example = """
from core.services.pseudocode_evaluator import (
    PseudocodeEvaluator,
    EvaluationRequest,
    EvaluationMode
)

evaluator = PseudocodeEvaluator()

# ì˜µì…˜3: í•˜ì´ë¸Œë¦¬ë“œ
request = EvaluationRequest(
    user_id='user_123',
    detail_id='unit0401',
    pseudocode='... ì˜ì‚¬ì½”ë“œ ...',
    mode=EvaluationMode.OPTION2_GPTONLY  # í˜„ì¬ ì§€ì› ëª¨ë“œ
)

result = evaluator.evaluate(request)

# ë¡œì»¬ ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©ë¨
if result.local_validation.passed:
    print("ë¡œì»¬ ê²€ì¦ âœ… â†’ GPTë§Œ ì‚¬ìš©")
else:
    print("ë¡œì»¬ ê²€ì¦ âŒ â†’ 3ê°œ ëª¨ë¸ ì¬ê²€ì¦")

print(f"ìµœì¢… ì ìˆ˜: {result.final_score}ì ")
print(f"ì‚¬ìš© ëª¨ë¸: {result.metadata['llm_models_successful']}")
"""
    
    print("\nğŸ’» ì½”ë“œ:")
    print(code_example)


# ============================================================================
# 4. ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ============================================================================

def run_test_with_sample(sample_name: str, pseudocode: str):
    """
    ìƒ˜í”Œ ì˜ì‚¬ì½”ë“œë¡œ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    """
    print("\n" + "-"*70)
    print(f"ğŸ“Œ í…ŒìŠ¤íŠ¸: {sample_name}")
    print("-"*70)
    print(f"\nì˜ì‚¬ì½”ë“œ ì…ë ¥:\n{pseudocode}")
    
    evaluator = PseudocodeEvaluator()
    
    # í‘œì¤€ ëª¨ë“œë¡œ í‰ê°€
    print(f"\nğŸ”„ í‘œì¤€ ëª¨ë“œ(GPT)ë¡œ í‰ê°€ ì¤‘...")
    request = EvaluationRequest(
        user_id='test_user',
        detail_id='test_001',
        pseudocode=pseudocode,
        mode=EvaluationMode.OPTION2_GPTONLY
    )
    
    try:
        result = evaluator.evaluate(request)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
        print(f"  â€¢ ìµœì¢… ì ìˆ˜: {result.final_score}ì ")
        print(f"  â€¢ ë“±ê¸‰: {result.grade}")
        print(f"  â€¢ í˜ë¥´ì†Œë‚˜: {result.persona}")
        print(f"  â€¢ í‰ê°€ ëª¨ë“œ: {result.mode.value}")
        
        # í”¼ë“œë°± ì¶œë ¥
        print(f"\nğŸ“‹ í”¼ë“œë°±:")
        print(f"  â€¢ ìš”ì•½: {result.feedback.get('summary', 'N/A')}")
        
        if result.feedback.get('strengths'):
            print(f"  â€¢ ê°•ì : {', '.join(result.feedback['strengths'][:2])}")
        
        if result.feedback.get('improvements'):
            print(f"  â€¢ ê°œì„ : {', '.join(result.feedback['improvements'][:2])}")
        
        print(f"  â€¢ ë‹¤ìŒ ë‹¨ê³„:")
        for step in result.feedback.get('next_steps', [])[:2]:
            print(f"    - {step}")
        
        # ë©”íƒ€ë°ì´í„°
        print(f"\nâ±ï¸ ì„±ëŠ¥:")
        print(f"  â€¢ ì´ ì†Œìš” ì‹œê°„: {result.metadata['total_latency_ms']}ms")
        print(f"  â€¢ ì‚¬ìš© ëª¨ë¸: {', '.join(result.metadata['llm_models_successful'])}")
        
        return result
    
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def run_all_tests():
    """
    ëª¨ë“  í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì‹¤í–‰
    """
    print("\n" + "="*70)
    print("ğŸ§ª ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("="*70)
    
    results = {}
    
    for sample_name, pseudocode in TEST_SAMPLES.items():
        print(f"\n\n{'='*70}")
        print(f"í…ŒìŠ¤íŠ¸: {sample_name.upper()}")
        print(f"{'='*70}")
        result = run_test_with_sample(sample_name, pseudocode)
        results[sample_name] = result
    
    # ì¢…í•© ë¹„êµ
    print("\n\n" + "="*70)
    print("ğŸ“Š ì¢…í•© ë¹„êµ")
    print("="*70)
    
    summary = {}
    for name, result in results.items():
        if result:
            summary[name] = {
                'score': result.final_score,
                'grade': result.grade,
                'persona': result.persona
            }
    
    print(json.dumps(summary, ensure_ascii=False, indent=2))


# ============================================================================
# 5. ë¡œì»¬ ê²€ì¦ë§Œ í…ŒìŠ¤íŠ¸
# ============================================================================

def test_local_validation_only():
    """
    ë¡œì»¬ ê²€ì¦ë§Œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸ (LLM í˜¸ì¶œ ì—†ìŒ)
    """
    print("\n" + "="*70)
    print("ğŸƒ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: ë¡œì»¬ ê²€ì¦ë§Œ (LLM ì œì™¸)")
    print("="*70)
    
    validator = LocalValidationEngine()
    
    for sample_name, pseudocode in TEST_SAMPLES.items():
        print(f"\nğŸ“Œ {sample_name}:")
        result = validator.validate(pseudocode)
        
        print(f"  â€¢ í†µê³¼: {result.passed}")
        print(f"  â€¢ ì ìˆ˜: {result.score}ì ")
        print(f"  â€¢ í”¼ë“œë°±: {', '.join(result.feedback)}")
        if result.warnings:
            print(f"  â€¢ ê²½ê³ : {', '.join(result.warnings)}")
        print(f"  â€¢ ì²˜ë¦¬ ì‹œê°„: {result.processing_time_ms}ms")


# ============================================================================
# 6. ë©”ì¸ ì‹¤í–‰
# ============================================================================

if __name__ == '__main__':
    print("\n" + "â–“"*70)
    print("â–“  ì˜ì‚¬ì½”ë“œ í‰ê°€ ì—”ì§„ - ì‚¬ìš© ê°€ì´ë“œ ë° í…ŒìŠ¤íŠ¸")
    print("â–“" * 70)
    
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
    else:
        command = 'explain'
    
    if command == 'explain':
        # 1. ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡° ì„¤ëª…
        explain_script_structure()
        
        # 2. 3ê°€ì§€ ì˜µì…˜ ì„¤ëª…
        example_1_option1_always_multimodel()
        example_2_option2_gptonly()
        example_3_option3_hybrid()
    
    elif command == 'local_only':
        # 3. ë¡œì»¬ ê²€ì¦ë§Œ (ë¹ ë¦„, LLM ë¶ˆí•„ìš”)
        test_local_validation_only()
    
    elif command == 'test_all':
        # 4. ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹œê°„ ê±¸ë¦¼)
        print("\nâš ï¸  ì „ì²´ í…ŒìŠ¤íŠ¸ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ (~30-50ì´ˆ)")
        print("LLM API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
        proceed = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if proceed.lower() == 'y':
            run_all_tests()
    
    elif command == 'test_single':
        # 5. ë‹¨ì¼ ìƒ˜í”Œ ì„ íƒ í…ŒìŠ¤íŠ¸
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ:")
        for i, name in enumerate(TEST_SAMPLES.keys(), 1):
            print(f"  {i}. {name}")
        
        choice = input("\nì„ íƒ (1-6): ")
        sample_names = list(TEST_SAMPLES.keys())
        
        try:
            idx = int(choice) - 1
            sample_name = sample_names[idx]
            pseudocode = TEST_SAMPLES[sample_name]
            run_test_with_sample(sample_name, pseudocode)
        except (ValueError, IndexError):
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤")
    
    elif command == 'test_option1':
        # 6. ì˜µì…˜1 í…ŒìŠ¤íŠ¸ (í•­ìƒ 3ê°œ ëª¨ë¸)
        print("\nğŸ§ª ì˜µì…˜1 í…ŒìŠ¤íŠ¸: í•­ìƒ 3ê°œ ëª¨ë¸ ì‚¬ìš©")
        pseudocode = TEST_SAMPLES['perfect']
        evaluator = PseudocodeEvaluator()
        request = EvaluationRequest(
            user_id='test_user',
            detail_id='option1_test',
            pseudocode=pseudocode,
            mode=EvaluationMode.OPTION2_GPTONLY
        )
        result = evaluator.evaluate(request)
        print(f"\nìµœì¢… ì ìˆ˜: {result.final_score}ì ")
        print(f"ì‚¬ìš© ëª¨ë¸: {result.metadata['llm_models_successful']}")
    
    elif command == 'test_option2':
        # 7. ì˜µì…˜2 í…ŒìŠ¤íŠ¸ (GPTë§Œ)
        print("\nğŸ§ª ì˜µì…˜2 í…ŒìŠ¤íŠ¸: GPTë§Œ ì‚¬ìš©")
        pseudocode = TEST_SAMPLES['perfect']
        evaluator = PseudocodeEvaluator()
        request = EvaluationRequest(
            user_id='test_user',
            detail_id='option2_test',
            pseudocode=pseudocode,
            mode=EvaluationMode.OPTION2_GPTONLY
        )
        result = evaluator.evaluate(request)
        print(f"\nìµœì¢… ì ìˆ˜: {result.final_score}ì ")
        print(f"ì‚¬ìš© ëª¨ë¸: {result.metadata['llm_models_successful']}")
    
    elif command == 'test_option3':
        # 8. ì˜µì…˜3 í…ŒìŠ¤íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ)
        print("\nğŸ§ª ì˜µì…˜3 í…ŒìŠ¤íŠ¸: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ")
        pseudocode = TEST_SAMPLES['data_leakage']  # ì‹¤íŒ¨í•˜ëŠ” ìƒ˜í”Œ
        evaluator = PseudocodeEvaluator()
        request = EvaluationRequest(
            user_id='test_user',
            detail_id='option3_test',
            pseudocode=pseudocode,
            mode=EvaluationMode.OPTION2_GPTONLY
        )
        result = evaluator.evaluate(request)
        print(f"\nìµœì¢… ì ìˆ˜: {result.final_score}ì ")
        print(f"ì‚¬ìš© ëª¨ë¸: {result.metadata['llm_models_successful']}")
        if result.local_validation.passed:
            print("ë¡œì»¬ ê²€ì¦: âœ… â†’ GPTë§Œ ì‚¬ìš©")
        else:
            print("ë¡œì»¬ ê²€ì¦: âŒ â†’ 3ê°œ ëª¨ë¸ ì¬ê²€ì¦")
    
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {command}")
        print("\nì‚¬ìš©ë²•:")
        print("  python test_pseudocode_evaluator.py explain       # 3ê°€ì§€ ì˜µì…˜ ì„¤ëª…")
        print("  python test_pseudocode_evaluator.py local_only    # ë¡œì»¬ ê²€ì¦ë§Œ")
        print("  python test_pseudocode_evaluator.py test_single   # ë‹¨ì¼ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸")
        print("  python test_pseudocode_evaluator.py test_all      # ì „ì²´ í…ŒìŠ¤íŠ¸")
        print("  python test_pseudocode_evaluator.py test_option1  # ì˜µì…˜1 í…ŒìŠ¤íŠ¸ (í•­ìƒ 3ëª¨ë¸)")
        print("  python test_pseudocode_evaluator.py test_option2  # ì˜µì…˜2 í…ŒìŠ¤íŠ¸ (GPTë§Œ)")
        print("  python test_pseudocode_evaluator.py test_option3  # ì˜µì…˜3 í…ŒìŠ¤íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ)")
    
    print("\n" + "â–“"*70)
    print("â–“ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("â–“" * 70 + "\n")
