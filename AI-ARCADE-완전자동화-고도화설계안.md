# AI-ARCADE 고도화 설계안 v2
> 작성일: 2026-02-20  
> 핵심 키워드: MCP + 멀티 에이전트 + 완전 자동화 AI 루프  
> 검토 요청: 이 설계의 문제점, 대안, 개선 방향에 대한 비판적 평가를 요청합니다.

---

## 1. 프로젝트 현황

### 플랫폼 개요
- **AI-ARCADE**: AI 엔지니어 초년생 대상 게임형 학습 플랫폼
- **타겟**: AI 엔지니어 초년생
- **기술 스택**: Vue 3, Django REST, PostgreSQL, OpenAI API (gpt-4o-mini)
- **현재 Unit 구성**
  - Unit 1: 의사코드 실습 (데이터 파이프라인 설계)
  - Unit 2: 버그헌트 (디버깅)
  - Unit 3: 시스템 아키텍처 설계

### 현재 구조의 한계
```
현재:
정적 문제 (하드코딩) → 사용자 풀이 → 단일 LLM 평가 → 결과 표시

한계:
- 문제가 고정되어 있어 트렌드 반영 불가
- 단일 LLM 평가 → 관점이 하나, 깊이 부족
- 평가 후 "그래서 어떻게 해야 하나"가 없음
- 사람이 계속 콘텐츠를 추가해야 하는 구조
```

---

## 2. 고도화 핵심 아이디어

### 완전 AI 자동화 루프

사람이 문제를 만들지 않아도, AI가 트렌드를 반영해 문제를 생성하고,
AI 면접관이 대화형으로 평가하며, 그 결과가 다시 문제 생성에 반영되는
**Self-improving 구조**.

```
[외부 트렌드 MCP]
        ↓
커리큘럼 생성 에이전트 팀
(문제 자동 생성)
        ↓
  동적 문제 풀 (DB)
        ↓
  사용자 풀이
        ↓
AI 면접관 에이전트 팀
(대화형 평가)
        ↓
  평가 결과 → DB 저장
        ↓
[AI-ARCADE DB MCP]
        ↓
커리큘럼 에이전트 팀
(문제 품질 개선 피드백 루프)
```

사람이 개입하는 포인트는 단 하나,
**관리자 승인 큐** (생성된 문제를 노출 전 간단 검토).

---

## 3. MCP 서버 구성

### 3.1 직접 구현할 MCP 서버

**서버 1: AI-ARCADE DB MCP 서버**
```python
@mcp_tool
def get_user_history(user_id: int):
    """사용자의 전체 풀이 이력 반환"""

@mcp_tool
def get_problem_pool(unit: str, difficulty: str):
    """현재 문제 풀 조회"""

@mcp_tool
def add_problem_to_pool(problem: dict):
    """에이전트가 생성한 문제를 풀에 추가"""

@mcp_tool
def save_evaluation_result(user_id: int, result: dict):
    """면접관 에이전트의 평가 결과 저장"""

@mcp_tool
def get_weak_problem_patterns():
    """전체 사용자 기준 정답률 낮은 문제 패턴 반환"""
```

**서버 2: 기술 트렌드 MCP 서버**
```python
@mcp_tool
def get_ai_engineering_trends():
    """최신 AI 엔지니어링 트렌드 키워드 반환
    (웹 검색 또는 큐레이션된 데이터)"""

@mcp_tool
def get_real_world_issues(topic: str):
    """실무에서 자주 발생하는 이슈 패턴 반환"""
```

### 3.2 MCP가 필요한 이유

단순 Django 내부 함수 호출로도 가능하지만, MCP를 쓰는 이유:
- 에이전트가 **어떤 도구를 쓸지 스스로 결정**하는 구조 가능
- 향후 GitHub, Notion, Slack 등 외부 서비스 연결 확장 용이
- "MCP 서버 설계/구현" 자체가 포트폴리오 어필 포인트

---

## 4. 커리큘럼 생성 에이전트 팀

### 4.1 구조

```
Orchestrator
"지금 어떤 문제가 필요한가 판단"
        ↓
병렬 실행:
┌─────────────────┐  ┌─────────────────┐
│   트렌드Agent   │  │   갭분석Agent   │
│                 │  │                 │
│ MCP →           │  │ MCP →           │
│ 기술트렌드서버  │  │ AI-ARCADE DB    │
│                 │  │                 │
│ "요즘 실무에서  │  │ "사용자들이     │
│  RAG, MCP,      │  │  어디서 막히고  │
│  멀티에이전트가 │  │  어떤 유형의    │
│  핫하다"        │  │  실수를 반복    │
│                 │  │  하는가"        │
└─────────────────┘  └─────────────────┘
        ↓
   문제설계Agent
   "두 결과를 합쳐 문제 초안 생성"
   Unit 1/2/3 형식에 맞게
        ↓
   검증Agent
   "난이도 적절한가"
   "학습 목표에 맞는가"
   "기존 문제와 중복인가"
        ↓
   관리자 승인 큐
   (승인된 문제만 사용자에게 노출)
```

### 4.2 Orchestrator 판단 로직

```python
def curriculum_orchestrator():
    # 문제 풀이 부족한 Unit 파악
    pool_status = mcp.get_problem_pool()
    
    if pool_status["unit1"] < 10:
        → unit1 문제 생성 우선
    
    # 정답률 낮은 유형 파악
    weak_patterns = mcp.get_weak_problem_patterns()
    
    if weak_patterns["edge_case"] > 60%_fail_rate:
        → edge_case 강화 문제 생성
    
    # 트렌드 반영 주기 (주 1회)
    if last_trend_update > 7days:
        → 트렌드Agent 실행
```

### 4.3 각 Agent 프롬프트 핵심

**트렌드Agent**
```
temperature: 0.3
"AI 엔지니어 초년생 교육 맥락에서
 아래 트렌드 키워드를 학습 문제로
 변환할 수 있는 주제를 추출하라.
 단, 초년생이 6개월 내 실무에서
 마주칠 수 있는 것만 포함."
```

**갭분석Agent**
```
temperature: 0.1
"전체 사용자 풀이 데이터에서
 반복적으로 나타나는 실수 패턴을
 추출하라. 개인 식별 정보 제외.
 JSON으로만 출력."
```

**검증Agent (Critic)**
```
temperature: 0.1
"이 문제가 아래 기준을 충족하는가:
 1. 단순 암기로 풀 수 없는가
 2. 실무 맥락이 있는가
 3. 기존 문제와 70% 이상 유사하지 않은가
 4. 초년생 수준에 적절한가
 
 통과/실패 + 이유를 JSON으로 출력."
```

---

## 5. AI 면접관 에이전트 팀

### 5.1 구조

```
사용자 풀이 제출
        ↓
Orchestrator
MCP → DB에서 이 사람 이전 답변 패턴 조회
"어떤 방향으로 면접을 진행할까"
        ↓
병렬 실행:
┌──────────────┐  ┌──────────────┐
│  기술검증    │  │   압박질문   │
│  Agent       │  │   Agent      │
│              │  │              │
│  "설계 의도  │  │  "이 반례는  │
│   를 확인"   │  │   어떻게?"   │
└──────────────┘  └──────────────┘
        ↓
   질문 우선순위 결정
   (두 Agent 결과 중 더 중요한 것부터)
        ↓
   대화 3~5턴 진행
   (사용자와 실제 대화)
        ↓
   최종평가Agent
   "대화 전체를 보고 종합 평가"
   - 표면적 이해도
   - 깊이 있는 이해도
   - 압박 상황 대응력
        ↓
   결과 저장 → 커리큘럼 피드백 루프
```

### 5.2 면접관 에이전트 프롬프트 핵심

**Orchestrator**
```
temperature: 0.2
입력: 사용자 풀이 + 이전 답변 이력

"이 풀이에서 가장 검증이 필요한
 부분 2가지를 선택하라.
 이전에 같은 실수를 한 적 있으면
 그 부분을 우선 질문하라."
```

**기술검증Agent**
```
temperature: 0.3
"시니어 엔지니어처럼 질문하라.
 답변의 '왜'를 파고드는 질문.
 유도하지 말고, 스스로 설명하게 유도."
```

**압박질문Agent**
```
temperature: 0.4
"이 설계의 가장 취약한 부분에
 반례를 들어 압박하라.
 단, 적대적이지 않게.
 실제 기술 면접 수준으로."
```

**최종평가Agent**
```
temperature: 0.2
"대화 전체를 보고 평가하라.
 점수가 아니라 서술로.
 
 - 이 사람이 진짜 이해한 것
 - 아직 모르는 것
 - 다음에 공부해야 할 것"
```

### 5.3 토큰 관리 전략

멀티 에이전트에서 토큰 폭증 방지:

```python
# 각 Agent 출력에 summary 필드 강제 포함
agent_output = {
    "full_result": "...",  # 내부용
    "summary": "한 줄 요약"  # 다음 Agent에게 전달
}

# 다음 Agent에는 summary만 전달
next_agent_input = {
    "prev_summary": agent_output["summary"],  # 100 토큰
    # full_result 전달 X (3,000 토큰 절약)
}
```

---

## 6. 피드백 루프 (Self-improving 구조)

```
면접관 에이전트 평가 완료
        ↓
피드백 데이터 DB 저장:
- 이 문제에서 몇 %가 막혔나
- 어떤 질문에서 대화가 끊겼나
- 어떤 개념 설명을 가장 못했나
        ↓
커리큘럼 에이전트 (주기적 실행)
MCP → DB에서 피드백 데이터 조회
        ↓
"이 문제는 너무 어렵다 → 난이도 조정"
"이 개념이 반복적으로 약하다 → 관련 문제 추가"
"이 유형은 다 잘 푼다 → 심화 문제로 교체"
        ↓
문제 풀 자동 업데이트
```

---

## 7. Human-in-the-loop 포인트

완전 자동화이지만 품질 리스크 관리를 위해 단 하나의 개입 포인트:

```
커리큘럼 에이전트 → 문제 생성
        ↓
관리자 승인 큐
(Slack MCP 연동 → 슬랙으로 알림)
관리자가 1분 내 승인/거절
        ↓
승인된 문제만 사용자에게 노출
```

완전 자동화의 장점은 살리면서 품질 리스크 최소화.

---

## 8. 포트폴리오 어필 포인트

### 이력서 한 줄 요약
```
"AI 교육 플랫폼에 MCP + 멀티 에이전트 기반
 Self-improving 커리큘럼 자동화 시스템 설계 및 구현"
```

### 기술 면접에서 설명할 수 있는 것들

**설계 결정:**
- "왜 MCP를 썼냐" → 에이전트의 도구 자율 선택 + 확장성
- "왜 멀티 에이전트냐" → 단일 LLM의 관점 한계, 역할 분리
- "토큰 관리를 어떻게 했냐" → summary 필드 강제, 다음 에이전트에 요약만 전달
- "품질 관리를 어떻게 했냐" → Critic Agent + Human-in-the-loop

**트레이드오프 설명:**
- "완전 자동화의 리스크는" → 품질 불안정, 관리자 승인으로 해소
- "비용은" → Agent 4~5개 × $0.01 = 분석당 $0.04~0.05
- "응답 시간은" → 병렬 실행으로 순차 대비 40% 단축

---

## 9. 구현 로드맵

```
1주차: MCP 서버 2개 구현
  - AI-ARCADE DB MCP 서버
  - 기술 트렌드 MCP 서버
  - 연결 테스트

2주차: 커리큘럼 생성 에이전트 팀
  - Orchestrator
  - 트렌드Agent + 갭분석Agent (병렬)
  - 검증Agent (Critic)
  - 관리자 승인 큐

3주차: AI 면접관 에이전트 팀
  - Orchestrator
  - 기술검증Agent + 압박질문Agent (병렬)
  - 최종평가Agent
  - 대화 UI 연동

4주차: 피드백 루프 연결
  - 평가 결과 → DB 저장
  - 커리큘럼 에이전트 주기적 실행
  - Self-improving 루프 검증
```

---

## 10. 미해결 문제 및 검토 요청

### 설계상 결정이 필요한 것들

**1. AI 면접관 대화 품질**
- 3~5턴 대화가 실제로 의미 있는 평가를 만들어내는가?
- 사용자가 면접관 AI에 거부감을 느끼지 않을까?

**2. 커리큘럼 자동 생성의 신뢰성**
- AI가 만든 문제의 품질을 관리자 1명이 검토하는 게 충분한가?
- 잘못된 문제가 다수 사용자에게 노출될 리스크는?

**3. MCP over-engineering 여부**
- 지금 규모에서 MCP가 Django 내부 함수보다 실질적 이점이 있는가?
- 아니면 포트폴리오용 복잡도인가?

**4. Self-improving 루프의 안정성**
- 피드백 루프가 잘못된 방향으로 강화될 가능성은?
- 예: 쉬운 문제만 계속 생성되는 방향으로 수렴하면?

**5. 비용 구조**
- 커리큘럼 생성: Agent 4개 × 주 1회 = 낮음
- 면접관 평가: Agent 3개 × 사용자 수 = 사용자 증가에 따라 급증
- 스케일업 시 비용 관리 방안은?

### 비판적으로 검토해주셨으면 하는 부분
- 이 구조가 AI 엔지니어 초년생 교육에 실제로 효과적인가?
- Self-improving 구조가 실제로 품질을 높이는가, 아니면 이론적으로만 그럴싸한가?
- MCP + 멀티 에이전트 조합이 이 문제에 적합한 복잡도인가?
- 더 간단하게 동일한 효과를 낼 수 있는 방법이 있는가?

---

*이 문서는 Claude (Anthropic)와의 대화를 통해 작성되었습니다.*  
*비판적 검토와 대안 제시를 환영합니다.*
