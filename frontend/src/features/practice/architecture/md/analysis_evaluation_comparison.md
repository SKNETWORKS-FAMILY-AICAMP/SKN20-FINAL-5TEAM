# 평가 방식 비교 분석: 기존 vs 수정

## 📊 요약
**결론: 사용자의 느낌이 맞습니다. 수정된 평가방식의 모범답안 품질이 더 낮을 가능성이 높습니다.**

---

## 🔍 주요 차이점

### 1️⃣ **평가 범위의 변화**
| 항목 | 기존 (MasterAgent) | 수정 (RubricEvaluator) |
|------|------------------|----------------------|
| 평가 대상 | **3개 기둥** (질문에서 파악된 부족 영역) | **6개 기둥** (모든 영역) |
| 평가 깊이 | 깊음 (3개 집중) | 얕음 (6개 분산) |
| 평가 관점 | 맞춤형 (문제별) | 포괄형 (균등) |

**👉 문제**: 6개 기둥을 모두 평가하려니, 같은 토큰 예산으로 각 기둥당 모범답안의 깊이가 떨어짐

---

### 2️⃣ **모범답안 길이 요구사항 (가장 중요)**

#### 기존 코드 (line 265)
```javascript
"modelAnswer": "이 시나리오/아키텍처에 맞는 모범답안 (5-7문장, 구체적 기술 포함)",
```
✅ **명시적**: "**5-7문장**, 구체적 기술 포함"

#### 수정 코드 (line 443-445)
```javascript
### 4. 모범답안
- 이 시나리오와 아키텍처에 맞는 구체적 답변
- 실제 기술/서비스 이름 포함
- 사용자가 배울 수 있도록 상세하게
```
❌ **모호함**: 길이 제약이 없음 → 모델의 판단에 맡겨짐

**👉 결과**: GPT-4o-mini는 "상세하게"를 자의적으로 해석 → 간결한 답변 생성 가능성 ↑

---

### 3️⃣ **프롬프트 복잡도 증가**

| 메트릭 | 기존 | 수정 | 증가량 |
|-------|------|------|--------|
| 루브릭 정의 | 1개 (RUBRIC_GRADES) | 2개 (공통 + 축별) | **2배** |
| 평가 기준 텍스트 | ~500줄 | ~850줄 | **70% 증가** |
| maxTokens | 2500 | 4500 | **1.8배** |
| 평가 대상 기둥 | 3개 | 6개 | **2배** |

**👉 분석**:
- 더 많은 토큰이 할당되었지만 (2500→4500)
- **처리할 작업이 2배 이상 증가** (3개→6개 기둥)
- 결과적으로 **각 기둥당 상세도 감소**
- 프롬프트 복잡도 증가로 인한 모델 집중도 분산

---

### 4️⃣ **루브릭 기준의 명확성 저하**

#### 기존: 단순함
```javascript
- 기본 점수 40점에서 시작
- 답변이 구체적이면 +점수
- 아키텍처에 반영되지 않으면 -점수
```
✅ 명확하고 일관성 있음

#### 수정: 복잡함
```javascript
1. RUBRIC_GRADES (공통 기준)
   - excellent (90-100) / good (75-89) / fair (60-74) / poor (40-59) / failing (0-39)

2. AXIS_SPECIFIC_RUBRICS (축별 기준)
   - 각 기둥마다 다른 기준 정의

3. 0점부터 시작 vs 40점부터 시작 (뉘앙스 차이)
```
❌ 모호함:
- "공통 기준"과 "축별 기준"을 어떻게 통합할지 불명확
- 모델이 두 가지를 균형있게 적용하기 어려움
- 특히 "5-7문장" 같은 구체적 지시가 없어서 모범답안이 짧아질 가능성 ↑

---

### 5️⃣ **questionEvaluations 매칭 로직 문제**

#### 수정 코드 (line 547-552)
```javascript
const questionEvaluations = (result.evaluations || []).slice(0, 3).map((ev, idx) => ({
  ...ev,
  question: qnaArray[idx]?.question || '',
  userAnswer: qnaArray[idx]?.answer || '',
  category: qnaArray[idx]?.category || ev.axisName || ''
}));
```

🔴 **문제점**:
- 프롬프트: "반드시 정확히 **6개 기둥** 평가하라" (line 432, 526)
- 실제 사용: `slice(0, 3)` → **3개만 사용**
- `evaluations[0]`과 `qnaArray[0]`이 항상 같은 의미의 기둥인지 보장 불가
- 예: evaluations 첫 번째가 "reliability"인데 qnaArray 첫 번째가 "security" 관련이면 매칭 오류

---

### 6️⃣ **토큰 효율성 분석**

#### 기존 방식
```
- 프롬프트 길이: ~3,000 토큰
- 3개 기둥 × (모범답안 5-7문장)
- 토큰 활용: 3개에 집중
```

#### 수정 방식
```
- 프롬프트 길이: ~5,000 토큰 (50% 증가)
- 6개 기둥 × (모범답안 길이 미정)
- 토큰 활용: 6개에 분산 → 각 항목의 품질 저하
```

**👉 효과**:
- 더 많은 프롬프트 설명으로 의도는 명확해졌지만
- 실제 모범답안 생성에 할당된 토큰은 상대적으로 **감소**

---

## 🎯 왜 모범답안 품질이 떨어졌을까?

### 직접적 원인
1. **명확한 길이 요구사항 제거** (5-7문장 → 없음)
   - 모델은 "적절한 길이"를 자체 판단
   - gpt-4o-mini는 보수적으로 해석 → 짧은 답변 생성

2. **평가 대상 확대** (3개 → 6개)
   - 6개를 모두 평가하느라 각 기둥당 상세도 저하
   - 특히 "improvements" 필드도 3-4개씩 생성해야 함

3. **프롬프트 복잡도 증가**
   - 루브릭 기준 2개 + 축별 지시사항
   - 모델의 주의력 분산 (attention fragmentation)

### 간접적 원인
4. **0점부터 시작** vs **40점부터 시작**
   - 기존: 기본 점수 40점 (긍정적 시작)
   - 수정: 기본 점수 0점 (부정적 시작)
   - 심리적으로 모범답안도 덜 긍정적으로 작성될 가능성

5. **가중치 개념 도입**
   - 각 기둥이 동등하지 않음을 암시
   - 낮은 가중치 기둥의 모범답안은 더 간략해질 가능성

---

## ✅ 개선 방안

### 즉시 개선 (프롬프트 수정)
```javascript
// line 442-446 수정
### 4. 모범답안
- 이 시나리오와 아키텍처에 맞는 구체적 답변
- 실제 기술/서비스 이름 포함
- **정확히 5-7문장**으로 상세하게 작성
- 트레이드오프와 선택 이유 명시
```

### 중기 개선
1. **3개 기둥 집중 평가로 복귀**
   ```javascript
   // 6개 모두 평가하되,
   // questionEvaluations는 사용자가 답변한 3개만 깊이있게 평가
   ```

2. **루브릭 통합**
   ```javascript
   // AXIS_SPECIFIC_RUBRICS를 RUBRIC_GRADES에 통합
   // 단일 기준으로 통일
   ```

3. **maxTokens 조정**
   ```javascript
   // 각 기둥당 상세도를 위해
   // 전체 토큰은 유지하되 프롬프트 간결화로 해결
   ```

---

## 📋 검증 방법

동일한 문제로 다음을 비교하세요:

```javascript
// 기존 방식
const oldResult = await evaluateWithMasterAgent(...);
console.log(oldResult.questionEvaluations[0].modelAnswer.length);

// 수정 방식
const newResult = await evaluateWithRubric(...);
console.log(newResult.questionEvaluations[0].modelAnswer.length);
```

예상 결과:
- 기존: 800-1000 글자 (5-7문장)
- 수정: 400-600 글자 (2-3문장)

---

## 🎓 결론

**사용자의 직관이 정확합니다.**

수정된 평가방식은:
- ✅ 더 공정함 (모든 기둥 평가, 0점부터)
- ✅ 더 명확함 (루브릭 기준 정의)
- ✅ 더 포괄적임 (6개 기둥 다양한 관점)

하지만:
- ❌ 모범답안 품질이 더 낮음 (깊이 부족)
- ❌ 특히 "5-7문장" 같은 구체적 지시 제거로 악화

**해결책**: 명시적인 길이 요구사항을 다시 추가하면 문제 해결 가능 ✨
