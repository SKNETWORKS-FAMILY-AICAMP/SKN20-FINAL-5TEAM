# BugHunt 인터뷰 LLM 신뢰성 검증 보고서

> **검증 대상**: `BugHuntInterviewView` (직접 평가자)
> **검증 일자**: 2026-02-25
> **총 LLM 호출 수**: 300회 (60샘플 × 5회 반복)

---

## 1. 개요

### 검증 목적

BugHuntInterviewView가 수험자의 답변을 루브릭 기반으로 채점할 때, **LLM이 신뢰할 수 있는 평가자로 동작하는지** 4가지 통계 지표로 검증한다.

### 검증 대상 시스템

| 항목 | 내용 |
|------|------|
| View | `BugHuntInterviewView` |
| 역할 | 2-3턴 인터뷰 진행 후 최종 채점 (직접 평가자) |
| 채점 방식 | 루브릭 기반 (core 40pt + mechanism 35pt + application 25pt = 100점 만점) |
| 채점 트리거 | `turn > MAX_TURNS(3)` 조건 충족 시 최종 평가 실행 |
| 채점 메커니즘 | RUBRIC_KEYWORDS 키워드 매칭, 임계값 기반 점수화 |

### 채점 임계값 (RUBRIC_KEYWORDS 기반)

각 카테고리(core/mechanism/application)별로 키워드 매칭 비율에 따라 점수를 부여:

| 키워드 매칭률 | 획득 점수 (각 카테고리 최대 대비) |
|------------|--------------------------------|
| ≥ 50% | 만점 (100%) |
| ≥ 30% | 65% |
| ≥ 15% | 35% |
| < 15% | 0점 |

---

## 2. 테스트 데이터 구성

### 샘플 설계

| 버그 유형 | 품질 레벨 | 샘플 수 | 반복 횟수 |
|-----------|----------|---------|---------|
| 12종 | 5레벨 | 60개 | 5회/샘플 |

**12종 버그 유형**: data_leakage, label_imbalance, overfitting, off_by_one, null_pointer, type_mismatch, metric_selection, feature_leakage, hyperparameter, memory_leak, race_condition, api_timeout

**5가지 품질 레벨 및 기대 점수 범위**:

| 품질 | 답변 특성 | 기대 점수 범위 |
|------|---------|-------------|
| excellent | 모든 카테고리 키워드 포함, 완전한 이해 | 80 ~ 100 |
| good | core+mechanism 키워드 포함, application 미언급 | 55 ~ 90 |
| average | core 키워드만 포함, 피상적 이해 | 25 ~ 65 |
| poor | core 키워드 1개만 언급, 최소한의 이해 | 5 ~ 24 |
| very_poor | 키워드 전혀 없음, 완전히 틀린 답변 | 0 ~ 4 |

### 샘플 생성 방식

- **excellent/good/average**: LLM을 통해 동적 생성 (케이스별 키워드 지시문 사용)
- **poor/very_poor**: 키워드 제어 불확실성 제거를 위해 **하드코딩 템플릿** 사용

---

## 3. 검증 결과 요약

| 지표 | 측정값 | 기준값 | 합격 여부 |
|------|-------|--------|---------|
| **일관성** (Consistency) | σ_avg = **1.78**, σ_max = 6.16 | σ_avg ≤ 5 | ✅ 합격 |
| **구분력** (Discrimination) | excellent-very_poor 차이 = **82.4점** | ≥ 30점 | ✅ 합격 |
| **순위 정확도** (Ranking) | Kendall's τ = **0.983** | ≥ 0.75 | ✅ 합격 |
| **수렴 타당도** (Convergent Validity) | Pearson r = **0.938** | ≥ 0.65 | ✅ 합격 |

**전체 합격: 4/4 지표 통과**

---

## 4. 지표별 상세 결과

### 4.1 일관성 (Consistency)

> **정의**: 동일 답변을 반복 평가할 때 점수가 얼마나 일치하는지

| 측정값 | 결과 |
|-------|------|
| 평균 표준편차 (σ_avg) | **1.78점** |
| 최대 표준편차 (σ_max) | **6.16점** (off_by_one_average) |
| 합격 기준 | σ_avg ≤ 5 |

**해석**: 동일 답변에 대해 5회 반복 평가 시, 평균적으로 ±1.78점 내외로 일관된 채점이 이루어진다. 최대 편차가 6.16점인 케이스(off_by_one_average)에서도 점수 범위가 58~72점으로, 품질 등급 판단에 영향을 줄 수준의 분산은 아니다.

**품질 레벨별 평균 σ:**

| 품질 | 평균 σ |
|------|-------|
| excellent | 1.07 |
| good | 1.19 |
| average | 2.28 |
| poor | 2.76 |
| very_poor | 1.47 |

> average/poor 구간에서 편차가 다소 크나, 이는 해당 품질 레벨의 경계 특성상 자연스러운 현상이다.

---

### 4.2 구분력 (Discrimination)

> **정의**: LLM이 최고 품질과 최저 품질 답변을 얼마나 명확히 구분하는지

| 측정값 | 결과 |
|-------|------|
| excellent 평균 점수 | **90.43점** |
| very_poor 평균 점수 | **8.03점** |
| 점수 차이 | **82.40점** |
| 합격 기준 | ≥ 30점 |

**케이스별 excellent vs very_poor 비교:**

| 버그 유형 | excellent | very_poor | 차이 |
|----------|-----------|-----------|------|
| data_leakage | 95.6 | 6.4 | 89.2 |
| label_imbalance | 92.0 | 6.4 | 85.6 |
| overfitting | 92.6 | 5.6 | 87.0 |
| off_by_one | 88.4 | 10.4 | 78.0 |
| null_pointer | 92.6 | 14.4 | 78.2 |
| type_mismatch | 88.2 | 8.0 | 80.2 |
| metric_selection | 87.6 | 7.2 | 80.4 |
| feature_leakage | 87.2 | 7.6 | 79.6 |
| hyperparameter | 89.2 | 6.8 | 82.4 |
| memory_leak | 89.8 | 8.0 | 81.8 |
| race_condition | 92.0 | 8.4 | 83.6 |
| api_timeout | 90.0 | 7.2 | 82.8 |

**해석**: 모든 12개 버그 유형에서 78점 이상의 차이가 발생한다. LLM이 완전히 틀린 답변과 완벽한 답변을 명확히 구분한다.

---

### 4.3 순위 정확도 (Ranking Accuracy)

> **정의**: LLM이 excellent > good > average > poor > very_poor 순서를 올바르게 매기는지 (Kendall's τ)

| 측정값 | 결과 |
|-------|------|
| 평균 Kendall's τ | **0.983** |
| 완전 정렬 케이스 | **11/12개** |
| 합격 기준 | τ ≥ 0.75 |

**비완전 정렬 케이스:**
- `type_mismatch`: good(88.8) > excellent(88.2) — good 샘플의 점수가 excellent를 미세하게 초과. 해당 버그 유형의 루브릭 키워드 특성상 good 지시문 답변이 예상보다 높게 채점됨.

**해석**: 12개 케이스 중 11개에서 완벽한 품질 순서 일치. 단 1건의 역전은 0.6점 차이(88.2 vs 88.8)로, 실용적으로 무시 가능한 수준이다.

---

### 4.4 수렴 타당도 (Convergent Validity)

> **정의**: LLM 점수와 규칙 기반 점수 간 선형 상관 관계 (Pearson r)

| 측정값 | 결과 |
|-------|------|
| Pearson r | **0.938** |
| p-value | **2.78 × 10⁻²⁸** (매우 유의함) |
| 합격 기준 | r ≥ 0.65 |

**해석**: LLM 채점 결과가 규칙 기반 키워드 점수와 0.938의 상관관계를 보인다. 이는 LLM이 수동으로 설계된 루브릭과 실질적으로 동일한 판단 기준을 적용하고 있음을 의미한다. p-value가 10⁻²⁸ 수준으로 매우 유의미하다.

---

## 5. 케이스별 전체 점수 분포

| 버그 유형 | excellent | good | average | poor | very_poor |
|----------|-----------|------|---------|------|-----------|
| data_leakage | 95.6 | 92.2 | 84.2 | 9.2 | 6.4 |
| label_imbalance | 92.0 | 79.6 | 65.4 | 19.6 | 6.4 |
| overfitting | 92.6 | 82.0 | 64.4 | 19.0 | 5.6 |
| off_by_one | 88.4 | 80.8 | 66.0 | 26.8 | 10.4 |
| null_pointer | 92.6 | 88.4 | 82.4 | 26.8 | 14.4 |
| type_mismatch | 88.2 | 88.8 | 76.0 | 27.2 | 8.0 |
| metric_selection | 87.6 | 83.0 | 68.6 | 20.0 | 7.2 |
| feature_leakage | 87.2 | 79.6 | 78.0 | 12.4 | 7.6 |
| hyperparameter | 89.2 | 78.0 | 61.0 | 18.8 | 6.8 |
| memory_leak | 89.8 | 79.6 | 57.6 | 17.2 | 8.0 |
| race_condition | 92.0 | 86.4 | 73.4 | 22.8 | 8.4 |
| api_timeout | 90.0 | 87.4 | 78.0 | 47.6 | 7.2 |
| **평균** | **90.4** | **83.8** | **71.3** | **22.3** | **8.0** |

### 주목할 점

1. **data_leakage 전반 고득점**: data_leakage 관련 키워드가 루브릭에 자주 등장하는 일반적인 ML 용어("분리", "검증", "누수")를 포함하고 있어, average 레벨에서도 84점이 나옴. 루브릭 키워드 재검토 고려 가능.

2. **api_timeout poor 이상 고득점 (47.6)**: api_timeout의 poor 샘플이 core 키워드 1개만 사용했음에도 47.6점으로 예상 범위(5~24)를 크게 초과. api_timeout의 RUBRIC_KEYWORDS core 항목이 답변 전체에서 자연스럽게 반복될 가능성 있음.

3. **null_pointer/type_mismatch poor 고득점 (26~27)**: 비슷한 이유로 해당 버그 유형의 핵심 키워드("null", "타입")가 문장 내 자연스럽게 포함되는 경향 있음.

---

## 6. 결론

BugHuntInterviewView의 LLM 채점 기능은 **4가지 신뢰성 지표 모두를 통과**하였으며, 특히 수렴 타당도(r=0.938)와 구분력(82.4점 차이)에서 우수한 성능을 보였다.

| 강점 | 개선 여지 |
|------|---------|
| 수렴 타당도 매우 높음 (r=0.938) | api_timeout/null_pointer 등 일부 케이스의 poor 점수가 예상 범위 초과 |
| 구분력 강함 (최고-최저 82.4점 차) | data_leakage average 점수가 예상 대비 높음 |
| 일관성 우수 (σ_avg=1.78) | type_mismatch의 excellent/good 순위 역전 (0.6점 차) |
| 거의 완벽한 순위 정확도 (11/12) | |

**종합 판정: LLM 채점 시스템은 신뢰할 수 있는 수준으로 동작하며, 실서비스 적용에 적합하다.**
