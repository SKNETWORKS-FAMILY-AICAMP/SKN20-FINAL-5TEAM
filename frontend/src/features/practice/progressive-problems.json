{
  "progressiveProblems": [
    {
      "id": "P1",
      "project_title": "ë”¥ëŸ¬ë‹ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…",
      "scenario": "í•™ìŠµëœ ëª¨ë¸ì„ ë°°í¬í–ˆëŠ”ë°, ì¶”ë¡  ê²°ê³¼ê°€ í•™ìŠµ ì‹œì™€ ë‹¤ë¥´ê²Œ ë‚˜ì˜µë‹ˆë‹¤. ë™ì¼í•œ ì…ë ¥ì¸ë°ë„ ë§¤ë²ˆ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ê±°ë‚˜, ë©”ëª¨ë¦¬ê°€ ê³„ì† ì¦ê°€í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.",
      "difficulty": 2,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "ì¶”ë¡ í•  ë•Œë§ˆë‹¤ ê²°ê³¼ê°€ ë‹¬ë¼ìš”",
          "bug_type": "A",
          "bug_type_name": "Inference Mode",
          "file_name": "inference.py",
          "buggy_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 64, 3)\n        self.bn = nn.BatchNorm2d(64)\n        self.fc = nn.Linear(64 * 6 * 6, 10)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = torch.relu(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nmodel = Model()\nmodel.load_state_dict(torch.load('model.pth'))\n\n# ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤: ì´ë¯¸ì§€ 1ì¥ì”© ì¶”ë¡ \nwith torch.no_grad():\n    for img in test_images:\n        output = model(img.unsqueeze(0))  # batch_size=1\n        pred = torch.softmax(output, dim=1)\n        print(pred)",
          "correct_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 64, 3)\n        self.bn = nn.BatchNorm2d(64)\n        self.fc = nn.Linear(64 * 6 * 6, 10)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = torch.relu(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nmodel = Model()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n\n# ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤: ì´ë¯¸ì§€ 1ì¥ì”© ì¶”ë¡ \nwith torch.no_grad():\n    for img in test_images:\n        output = model(img.unsqueeze(0))  # batch_size=1\n        pred = torch.softmax(output, dim=1)\n        print(pred)",
          "error_log": "=== ì‹¤ì‹œê°„ ì¶”ë¡  ê²°ê³¼ (ì‹œê°„ ìˆœì„œëŒ€ë¡œ) ===\nImage 1: class 2 (conf: 0.87)\nImage 2: class 2 (conf: 0.85)\nImage 3: class 2 (conf: 0.82)\n...\nImage 100: class 2 (conf: 0.71)\nImage 500: class 2 (conf: 0.58)\nImage 1000: class 3 (conf: 0.49)  # ì˜ˆì¸¡ í´ë˜ìŠ¤ ë³€ê²½!\n\n[ë¬¸ì œ] ì„œë¹„ìŠ¤ ìš´ì˜ ì¤‘ ëª¨ë¸ ì„±ëŠ¥ì´ ì ì  ì €í•˜ë¨\n[ë¶„ì„] running_mean/running_varê°€ ê³„ì† ì—…ë°ì´íŠ¸ë˜ë©´ì„œ drift ë°œìƒ",
          "success_log": "=== ì‹¤ì‹œê°„ ì¶”ë¡  ê²°ê³¼ (ì‹œê°„ ìˆœì„œëŒ€ë¡œ) ===\nImage 1: class 2 (conf: 0.87)\nImage 2: class 2 (conf: 0.87)\nImage 3: class 2 (conf: 0.87)\n...\nImage 100: class 2 (conf: 0.87)\nImage 500: class 2 (conf: 0.87)\nImage 1000: class 2 (conf: 0.87)\n\n[ì •ìƒ] ì‹œê°„ì´ ì§€ë‚˜ë„ ì¼ê´€ëœ ì˜ˆì¸¡",
          "hint": "ëª¨ë¸ì—ëŠ” í•™ìŠµ ëª¨ë“œì™€ ì¶”ë¡  ëª¨ë“œê°€ ìˆìŠµë‹ˆë‹¤. BatchNormì€ train ëª¨ë“œì—ì„œ running statisticsë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "model.eval()"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Inference Mode Error",
            "description": "PyTorch ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ í•™ìŠµ ëª¨ë“œ(train mode)ë¡œ ìƒì„±ë©ë‹ˆë‹¤.\n\ní•™ìŠµ ëª¨ë“œì˜ BatchNorm:\n- í˜„ì¬ ë°°ì¹˜ì˜ í‰ê· /ë¶„ì‚°ìœ¼ë¡œ ì •ê·œí™”\n- running_mean/running_varë¥¼ ì—…ë°ì´íŠ¸ (momentum ê¸°ë°˜ ì´ë™í‰ê· )\n- batch_size=1ì´ë©´ running statsê°€ í˜„ì¬ ìƒ˜í”Œì— ì¹˜ìš°ì³ drift ë°œìƒ\n\nì¶”ë¡  ëª¨ë“œ(eval mode)ì˜ BatchNorm:\n- í•™ìŠµ ì¤‘ ì €ì¥ëœ running_mean/running_var ì‚¬ìš© (ê³ ì •)\n- running stats ì—…ë°ì´íŠ¸ ì•ˆ í•¨\n\nì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œ batch_size=1ë¡œ ì¶”ë¡ í•˜ë©´ì„œ eval() ì—†ì´ ìš´ì˜í•˜ë©´,\nrunning statsê°€ ì ì  í˜„ì¬ ë°ì´í„° ë¶„í¬ë¡œ ì˜¤ì—¼ë˜ì–´ ëª¨ë¸ ì„±ëŠ¥ì´ ì„œì„œíˆ ì €í•˜ë©ë‹ˆë‹¤.",
            "suggestion": "ì¶”ë¡  ì „ì— ë°˜ë“œì‹œ model.eval()ì„ í˜¸ì¶œí•˜ì„¸ìš”. íŠ¹íˆ batch_size=1ì¸ ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ì´ ë²„ê·¸ëŠ” ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ 'ëª¨ë¸ì´ ì„œì„œíˆ ì´ìƒí•´ì§„ë‹¤'ëŠ” ë¦¬í¬íŠ¸ë¡œ ë°œê²¬ë©ë‹ˆë‹¤. Dropoutì´ ì—†ì–´ë„ BatchNormë§Œìœ¼ë¡œ ë°œìƒí•˜ë©°, ë°°í¬ í›„ ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ë‚˜íƒ€ë‚˜ ì›ì¸ íŒŒì•…ì´ ì–´ë µìŠµë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "ì¶”ë¡ í•˜ë©´ í• ìˆ˜ë¡ ë©”ëª¨ë¦¬ê°€ í„°ì ¸ìš”",
          "bug_type": "B",
          "bug_type_name": "Memory Leak",
          "file_name": "batch_inference.py",
          "buggy_code": "import torch\n\nmodel = Model()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\nmodel.cuda()\n\nresults = []\n\n# ëŒ€ëŸ‰ ë°°ì¹˜ ì¶”ë¡ \nfor batch in data_loader:\n    images = batch['image'].cuda()\n    \n    # ì¶”ë¡  ì‹¤í–‰\n    outputs = model(images)\n    predictions = torch.argmax(outputs, dim=1)\n    \n    results.extend(predictions.cpu().tolist())\n    \n    # ë§¤ 100ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ í™•ì¸\n    if len(results) % 100 == 0:\n        print(f\"Processed: {len(results)}, GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")",
          "correct_code": "import torch\n\nmodel = Model()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\nmodel.cuda()\n\nresults = []\n\n# ëŒ€ëŸ‰ ë°°ì¹˜ ì¶”ë¡ \nwith torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n    for batch in data_loader:\n        images = batch['image'].cuda()\n        \n        # ì¶”ë¡  ì‹¤í–‰\n        outputs = model(images)\n        predictions = torch.argmax(outputs, dim=1)\n        \n        results.extend(predictions.cpu().tolist())\n        \n        # ë§¤ 100ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ í™•ì¸\n        if len(results) % 100 == 0:\n            print(f\"Processed: {len(results)}, GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")",
          "error_log": "Processed: 100, GPU Memory: 2.15GB\nProcessed: 500, GPU Memory: 4.82GB\nProcessed: 1000, GPU Memory: 7.93GB\nProcessed: 1500, GPU Memory: 11.24GB\nRuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 12.00 GiB total capacity)",
          "success_log": "Processed: 100, GPU Memory: 1.23GB\nProcessed: 500, GPU Memory: 1.24GB\nProcessed: 1000, GPU Memory: 1.23GB\nProcessed: 5000, GPU Memory: 1.25GB\n[ì •ìƒ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¼ì •í•˜ê²Œ ìœ ì§€",
          "hint": "PyTorchëŠ” ì—­ì „íŒŒë¥¼ ìœ„í•´ forward ì—°ì‚°ì˜ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì¶”ë¡ ì—ì„œëŠ” ì´ê²Œ í•„ìš”í• ê¹Œìš”?",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "torch.no_grad()"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Memory Leak (Gradient Accumulation)",
            "description": "PyTorchëŠ” Autograd ì‹œìŠ¤í…œì„ í†µí•´ ìë™ ë¯¸ë¶„ì„ ì§€ì›í•©ë‹ˆë‹¤. ëª¨ë“  í…ì„œ ì—°ì‚°ì€ ì—°ì‚° ê·¸ë˜í”„(Computational Graph)ë¡œ ê¸°ë¡ë˜ë©°, backward() í˜¸ì¶œ ì‹œ ì´ ê·¸ë˜í”„ë¥¼ ì—­ìˆœíšŒí•˜ë©° ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n\në¬¸ì œëŠ” ì¶”ë¡  ì‹œì—ë„ ì´ ê·¸ë˜í”„ê°€ ê³„ì† ìƒì„±ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤:\n- ë§¤ forwardë§ˆë‹¤ ìƒˆë¡œìš´ ì—°ì‚° ê·¸ë˜í”„ ìƒì„±\n- ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ìš© ì¤‘ê°„ í…ì„œë“¤ì´ ë©”ëª¨ë¦¬ì— ëˆ„ì \n- ë°°ì¹˜ê°€ ìŒ“ì¼ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ â†’ OOM\n\ntorch.no_grad()ëŠ” ì—°ì‚° ê·¸ë˜í”„ ìƒì„± ìì²´ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•©ë‹ˆë‹¤.",
            "suggestion": "ì¶”ë¡  ì½”ë“œëŠ” ë°˜ë“œì‹œ with torch.no_grad(): ë¸”ë¡ ì•ˆì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”. ì¶”ê°€ë¡œ torch.inference_mode()ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ìµœì í™”ë©ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ëŒ€ê·œëª¨ ë°°ì¹˜ ì¶”ë¡ ì—ì„œ OOMì€ í”í•œ ë¬¸ì œì…ë‹ˆë‹¤. model.eval()ì€ ë ˆì´ì–´ ë™ì‘ì„ ë°”ê¾¸ê³ , torch.no_grad()ëŠ” ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•©ë‹ˆë‹¤. ë‘˜ì€ ë‹¤ë¥¸ ì—­í• ì´ë¯€ë¡œ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "CPUì—ì„œ ë§Œë“  í…ì„œê°€ GPU ëª¨ë¸ì„ ë§Œë‚˜ë©´?",
          "bug_type": "C",
          "bug_type_name": "Device Error",
          "file_name": "multi_device.py",
          "buggy_code": "import torch\nimport torch.nn as nn\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        # ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n    \n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        return x + self.pe[:, :x.size(1), :]\n\nmodel = PositionalEncoding(d_model=512)\nmodel.cuda()\n\nx = torch.randn(32, 100, 512).cuda()\noutput = model(x)",
          "correct_code": "import torch\nimport torch.nn as nn\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        # ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))  # bufferë¡œ ë“±ë¡\n    \n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        return x + self.pe[:, :x.size(1), :]\n\nmodel = PositionalEncoding(d_model=512)\nmodel.cuda()\n\nx = torch.randn(32, 100, 512).cuda()\noutput = model(x)  # ì •ìƒ ë™ì‘!",
          "error_log": "Traceback (most recent call last):\n  File \"inference.py\", line 25, in <module>\n    output = model(x)\n  File \"model.py\", line 18, in forward\n    return x + self.pe[:, :x.size(1), :]\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
          "success_log": "Model device: cuda:0\nInput device: cuda:0\nPositional encoding device: cuda:0\nOutput shape: torch.Size([32, 100, 512])\n[ì •ìƒ] ëª¨ë“  í…ì„œê°€ ê°™ì€ deviceì— ìˆìŒ",
          "hint": "model.cuda()ë¥¼ í˜¸ì¶œí•˜ë©´ nn.Parameterë§Œ GPUë¡œ ì´ë™í•©ë‹ˆë‹¤. ë‹¨ìˆœ í…ì„œ ì†ì„±(self.pe = ...)ì€ ì´ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "register_buffer"
            ],
            "forbidden": [
              "self.pe = pe"
            ]
          },
          "error_info": {
            "type": "Device Mismatch Error",
            "description": "PyTorchì—ì„œ model.cuda() ë˜ëŠ” model.to(device)ë¥¼ í˜¸ì¶œí•˜ë©´ ì´ë™í•˜ëŠ” ê²ƒë“¤:\n- nn.Parameterë¡œ ë“±ë¡ëœ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°\n- register_buffer()ë¡œ ë“±ë¡ëœ ë²„í¼\n\nì´ë™í•˜ì§€ ì•ŠëŠ” ê²ƒ:\n- ë‹¨ìˆœ í…ì„œ ì†ì„± (self.tensor = torch.zeros(...))\n- ì§€ì—­ ë³€ìˆ˜ë¡œ ìƒì„±ëœ í…ì„œ\n\nself.pe = pe.unsqueeze(0)ì€ ë‹¨ìˆœ ì†ì„± í• ë‹¹ì´ë¯€ë¡œ model.cuda() ì‹œ CPUì— ë‚¨ì•„ìˆê²Œ ë©ë‹ˆë‹¤. forwardì—ì„œ GPU í…ì„œ xì™€ ì—°ì‚°í•˜ë ¤ í•  ë•Œ device ë¶ˆì¼ì¹˜ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.",
            "suggestion": "í•™ìŠµë˜ì§€ ì•Šì§€ë§Œ ëª¨ë¸ê³¼ í•¨ê»˜ ì´ë™í•´ì•¼ í•˜ëŠ” í…ì„œëŠ” self.register_buffer('name', tensor)ë¡œ ë“±ë¡í•˜ì„¸ìš”. ì´ë ‡ê²Œ í•˜ë©´ device ì´ë™ê³¼ state_dict ì €ì¥ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: Positional Encoding, ë§ˆìŠ¤í¬ í…ì„œ ë“± í•™ìŠµë˜ì§€ ì•ŠëŠ” ìƒìˆ˜ í…ì„œë¥¼ ë‹¤ë£° ë•Œ register_bufferë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì•ˆ ê·¸ëŸ¬ë©´ GPU/CPU í˜¼ìš© ë²„ê·¸ë‚˜ ëª¨ë¸ ì €ì¥/ë¡œë“œ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤."
        }
      ]
    },
    {
      "id": "P2",
      "project_title": "í•™ìŠµ ë£¨í”„ ì¹˜ëª…ì  ë²„ê·¸",
      "scenario": "ëª¨ë¸ í•™ìŠµ ì½”ë“œë¥¼ ì‘ì„±í–ˆëŠ”ë°, lossê°€ ì´ìƒí•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤. ê°ì†Œí•´ì•¼ í•  lossê°€ ì¦ê°€í•˜ê±°ë‚˜, í•™ìŠµì´ ì „í˜€ ì•ˆ ë˜ê±°ë‚˜, ê°‘ìê¸° NaNì´ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤.",
      "difficulty": 2,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "Lossê°€ ë°°ì¹˜ë§ˆë‹¤ í­ë°œí•´ìš”",
          "bug_type": "A",
          "bug_type_name": "Gradient Bug",
          "file_name": "train_loop.py",
          "buggy_code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = SimpleNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(10):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        output = model(data)\n        loss = criterion(output, target)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        print(f\"Epoch {epoch}, Batch {batch_idx}: Loss = {loss.item():.4f}\")",
          "correct_code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = SimpleNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(10):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()  # â† ë°˜ë“œì‹œ backward ì „ì—!\n        \n        output = model(data)\n        loss = criterion(output, target)\n        \n        loss.backward()\n        optimizer.step()\n        \n        print(f\"Epoch {epoch}, Batch {batch_idx}: Loss = {loss.item():.4f}\")",
          "error_log": "Epoch 0, Batch 0: Loss = 2.3025\nEpoch 0, Batch 1: Loss = 4.7891\nEpoch 0, Batch 2: Loss = 8.2341\nEpoch 0, Batch 10: Loss = 156.2847\nEpoch 0, Batch 50: Loss = inf\nEpoch 1, Batch 0: Loss = nan",
          "success_log": "Epoch 0, Batch 0: Loss = 2.3025\nEpoch 0, Batch 1: Loss = 2.2891\nEpoch 0, Batch 10: Loss = 1.8542\nEpoch 0, Batch 50: Loss = 0.9823\nEpoch 1, Batch 0: Loss = 0.7654\n[ì •ìƒ] Lossê°€ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ",
          "hint": "zero_grad()ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. backward() ì „ì— í˜¸ì¶œí•´ì•¼ ì´ì „ ë°°ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ëˆ„ì ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "regex",
            "value": "zero_grad\\(\\)[\\s\\S]*?forward|zero_grad\\(\\)[\\s\\S]*?output\\s*=\\s*model",
            "flags": ""
          },
          "error_info": {
            "type": "Gradient Accumulation Bug",
            "description": "PyTorchëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ìœ ì—°ì„±ì„ ìœ„í•´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ëˆ„ì í•©ë‹ˆë‹¤. ì´ëŠ” ëŒ€í˜• ëª¨ë¸ì„ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ  í•™ìŠµí•  ë•Œ(Gradient Accumulation) ìœ ìš©í•˜ì§€ë§Œ, ì˜ë„ì¹˜ ì•Šì€ ëˆ„ì ì€ ë¬¸ì œë¥¼ ì¼ìœ¼í‚µë‹ˆë‹¤.\n\nì˜ëª»ëœ ìˆœì„œ (zero_gradê°€ step í›„):\n1. backward() â†’ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° + ì´ì „ ê·¸ë˜ë””ì–¸íŠ¸ì— ëˆ„ì \n2. step() â†’ ëˆ„ì ëœ (í­ë°œí•œ) ê·¸ë˜ë””ì–¸íŠ¸ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n3. zero_grad() â†’ ì´ˆê¸°í™” (ì´ë¯¸ ëŠ¦ìŒ)\n\në°°ì¹˜ê°€ ìŒ“ì¼ìˆ˜ë¡ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì ¸ loss í­ë°œ â†’ NaN ë°œìƒ",
            "suggestion": "í•™ìŠµ ë£¨í”„ì—ì„œ zero_grad()ëŠ” ë°˜ë“œì‹œ backward() ì „ì— í˜¸ì¶œí•˜ì„¸ìš”. í‘œì¤€ ìˆœì„œ: optimizer.zero_grad() â†’ output = model(x) â†’ loss.backward() â†’ optimizer.step()"
          },
          "coaching": "ğŸ¯ í˜„ì—…: í•™ìŠµ ì½”ë“œì˜ í‘œì¤€ ìˆœì„œë¥¼ ì™¸ì›Œë‘ì„¸ìš”. zero_grad â†’ forward â†’ loss â†’ backward â†’ step. ì´ ìˆœì„œê°€ í‹€ë¦¬ë©´ loss í­ë°œ, NaN, í•™ìŠµ ì•ˆ ë¨ ë“±ì˜ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "LossëŠ” ì¤„ì–´ë“œëŠ”ë° ëª¨ë¸ì´ ì•ˆ ë°”ë€Œì–´ìš”",
          "bug_type": "B",
          "bug_type_name": "Graph Break",
          "file_name": "custom_loss.py",
          "buggy_code": "import torch\nimport torch.nn as nn\n\ndef custom_loss(pred, target, model):\n    # L2 regularization ì¶”ê°€\n    mse_loss = nn.MSELoss()(pred, target)\n    \n    # ì •ê·œí™” í•­ ê³„ì‚°\n    l2_reg = 0\n    for param in model.parameters():\n        l2_reg += torch.norm(param)\n    \n    total_loss = mse_loss.detach() + 0.01 * l2_reg\n    return total_loss\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    pred = model(x)\n    loss = custom_loss(pred, y, model)\n    \n    # ë””ë²„ê¹…: ì—°ì‚° ê·¸ë˜í”„ í™•ì¸\n    print(f\"Loss grad_fn: {loss.grad_fn}\")\n    \n    loss.backward()\n    optimizer.step()\n    print(f\"Loss: {loss.item():.4f}\")",
          "correct_code": "import torch\nimport torch.nn as nn\n\ndef custom_loss(pred, target, model):\n    # L2 regularization ì¶”ê°€\n    mse_loss = nn.MSELoss()(pred, target)\n    \n    # ì •ê·œí™” í•­ ê³„ì‚°\n    l2_reg = 0\n    for param in model.parameters():\n        l2_reg += torch.norm(param)\n    \n    total_loss = mse_loss + 0.01 * l2_reg  # detach ì œê±°!\n    return total_loss\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    pred = model(x)\n    loss = custom_loss(pred, y, model)\n    \n    # ë””ë²„ê¹…: ì—°ì‚° ê·¸ë˜í”„ í™•ì¸\n    print(f\"Loss grad_fn: {loss.grad_fn}\")\n    \n    loss.backward()\n    optimizer.step()\n    print(f\"Loss: {loss.item():.4f}\")",
          "error_log": "# ë””ë²„ê¹… ì¶œë ¥\nLoss grad_fn: <AddBackward0 object>  # Addë§Œ ìˆìŒ, MSE ì—°ê²° ì—†ìŒ!\n\nEpoch 0: Loss = 2.3456\nEpoch 10: Loss = 0.0234  (ì¤„ì–´ë“œëŠ” ê²ƒì²˜ëŸ¼ ë³´ì„)\nEpoch 50: Loss = 0.0012\n\n[ê²€ì¦]\nTrain Accuracy: 12.3% (ë³€í™” ì—†ìŒ)\nTest Accuracy: 11.8% (ë³€í™” ì—†ìŒ)\n[ë¬¸ì œ] LossëŠ” ì¤„ì–´ë“¤ì§€ë§Œ ëª¨ë¸ ì„±ëŠ¥ ë¶ˆë³€",
          "success_log": "# ë””ë²„ê¹… ì¶œë ¥\nLoss grad_fn: <AddBackward0 object at 0x...>\n  â”œâ”€â”€ <MseLossBackward0>  # MSE ì—°ê²°ë¨!\n  â””â”€â”€ <MulBackward0>      # L2 reg ì—°ê²°ë¨\n\nEpoch 0: Loss = 2.3456\nEpoch 10: Loss = 0.4521\nEpoch 50: Loss = 0.0234\n\n[ê²€ì¦]\nTrain Accuracy: 98.2%\nTest Accuracy: 94.5%\n[ì •ìƒ] Loss ê°ì†Œì™€ í•¨ê»˜ ì„±ëŠ¥ë„ í–¥ìƒ",
          "hint": ".detach()ëŠ” í…ì„œë¥¼ ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ë¶„ë¦¬í•©ë‹ˆë‹¤. print(loss.grad_fn)ìœ¼ë¡œ ì—°ì‚° ê·¸ë˜í”„ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "mse_loss + 0.01"
            ],
            "forbidden": [
              "mse_loss.detach()"
            ]
          },
          "error_info": {
            "type": "Computational Graph Disconnection",
            "description": "PyTorchì˜ ìë™ ë¯¸ë¶„ì€ ì—°ì‚° ê·¸ë˜í”„ë¥¼ í†µí•´ ì‘ë™í•©ë‹ˆë‹¤. .detach()ë¥¼ í˜¸ì¶œí•˜ë©´:\n\n1. í•´ë‹¹ í…ì„œê°€ ê·¸ë˜í”„ì—ì„œ ë¶„ë¦¬ë¨\n2. ë¶„ë¦¬ëœ í…ì„œë¥¼ ì‚¬ìš©í•œ ì´í›„ ì—°ì‚°ì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ê°€ íë¥´ì§€ ì•ŠìŒ\n\nì´ ì½”ë“œì—ì„œ:\n- mse_loss.detach() â†’ MSE ì†ì‹¤ê³¼ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê°„ì˜ ì—°ê²° ëŠê¹€\n- l2_regë§Œ ê·¸ë˜í”„ì— ì—°ê²°ë¨\n- backward() ì‹œ MSEì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ = 0\n- ê²°ê³¼: ì •ê·œí™” í•­ë§Œ ìµœì†Œí™”ë˜ê³ , ì‹¤ì œ ì˜ˆì¸¡ ì„±ëŠ¥ì€ ê°œì„  ì•ˆ ë¨\n\n**ë””ë²„ê¹… ë°©ë²•:**\nprint(loss.grad_fn)ìœ¼ë¡œ ì—°ì‚° ê·¸ë˜í”„ì˜ ì—°ê²° ìƒíƒœë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì˜ëª»ëœ ê²½ìš°: AddBackwardë§Œ ë³´ì´ê³  MseLossBackwardê°€ ì—†ìŒ",
            "suggestion": "detach()ëŠ” íŠ¹ì • í…ì„œì˜ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ì„ ë§‰ê³  ì‹¶ì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”. loss ê³„ì‚°ì—ì„œëŠ” ëŒ€ë¶€ë¶„ detachê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: 'Lossê°€ ì¤„ì–´ë“œëŠ”ë° ì„±ëŠ¥ì´ ì•ˆ ì˜¤ë¥¸ë‹¤'ë©´ print(loss.grad_fn)ìœ¼ë¡œ ì—°ì‚° ê·¸ë˜í”„ë¥¼ í™•ì¸í•˜ì„¸ìš”. grad_fnì´ Noneì´ê±°ë‚˜ ì˜ˆìƒë³´ë‹¤ ì§§ìœ¼ë©´ ì¤‘ê°„ì— ê·¸ë˜í”„ê°€ ëŠê¸´ ê²ƒì…ë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¼ë”ë‹ˆ ì˜¤íˆë ¤ ë§í–ˆì–´ìš”",
          "bug_type": "C",
          "bug_type_name": "Scheduler Bug",
          "file_name": "scheduler.py",
          "buggy_code": "import torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\nmodel = MyModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nfor epoch in range(100):\n    for batch in train_loader:\n        scheduler.step()\n        \n        optimizer.zero_grad()\n        output = model(batch['x'])\n        loss = criterion(output, batch['y'])\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch {epoch}: LR = {scheduler.get_last_lr()[0]:.6f}\")",
          "correct_code": "import torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\nmodel = MyModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nfor epoch in range(100):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        output = model(batch['x'])\n        loss = criterion(output, batch['y'])\n        loss.backward()\n        optimizer.step()\n    \n    scheduler.step()  # â† ì—í¬í¬ ëì—ì„œ í•œ ë²ˆë§Œ í˜¸ì¶œ\n    print(f\"Epoch {epoch}: LR = {scheduler.get_last_lr()[0]:.6f}\")",
          "error_log": "Epoch 0: LR = 0.000100 (ì²« ë°°ì¹˜ í›„ ì´ë¯¸ ê¸‰ê°)\nEpoch 1: LR = 0.000001\nEpoch 2: LR = 0.000000\nLossê°€ ìˆ˜ë ´í•˜ì§€ ì•Šê³  ì§„ë™",
          "success_log": "Epoch 0: LR = 0.001000\nEpoch 9: LR = 0.001000\nEpoch 10: LR = 0.000100\nEpoch 19: LR = 0.000100\nEpoch 20: LR = 0.000010\n[ì •ìƒ] 10ì—í¬í¬ë§ˆë‹¤ LRì´ 1/10ë¡œ ê°ì†Œ",
          "hint": "StepLR(step_size=10)ì€ '10ë²ˆì˜ step() í˜¸ì¶œ í›„' LRì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œí•˜ë©´ step_sizeì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.",
          "solution_check": {
            "type": "regex",
            "value": "for batch[\\s\\S]*?optimizer\\.step\\(\\)[\\s\\S]*?\\n\\s*scheduler\\.step\\(\\)",
            "flags": ""
          },
          "error_info": {
            "type": "LR Scheduler Misuse",
            "description": "Learning Rate SchedulerëŠ” í•™ìŠµë¥ ì„ ë™ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í˜¸ì¶œ ìœ„ì¹˜ê°€ ì˜ëª»ë˜ë©´:\n\n**ì˜ëª»ëœ ì‚¬ìš© (ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œ):**\n- StepLR(step_size=10)ì´ 10ë°°ì¹˜ í›„ LR ê°ì†Œ\n- í•œ ì—í¬í¬ì— 100ë°°ì¹˜ë¼ë©´, 10ë°°ì¹˜ë§Œì— LR = 0.0001\n- 20ë°°ì¹˜ í›„ LR = 0.00001... ê¸‰ê²©íˆ 0ì— ìˆ˜ë ´\n\n**ì˜¬ë°”ë¥¸ ì‚¬ìš© (ì—í¬í¬ë§ˆë‹¤ í˜¸ì¶œ):**\n- 10ì—í¬í¬ í›„ LR ê°ì†Œ\n- ì˜ë„í•œ ëŒ€ë¡œ ì ì§„ì  í•™ìŠµë¥  ê°ì†Œ\n\në˜í•œ PyTorch 1.1 ì´í›„ë¶€í„°ëŠ” scheduler.step()ì„ optimizer.step() ì´í›„ì— í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.",
            "suggestion": "ëŒ€ë¶€ë¶„ì˜ ìŠ¤ì¼€ì¤„ëŸ¬(StepLR, ExponentialLR ë“±)ëŠ” ì—í¬í¬ ëì—ì„œ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ì„¸ìš”. ë°°ì¹˜ ë‹¨ìœ„ ìŠ¤ì¼€ì¤„ëŸ¬(OneCycleLR ë“±)ëŠ” ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œí•©ë‹ˆë‹¤. ìŠ¤ì¼€ì¤„ëŸ¬ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ìŠ¤ì¼€ì¤„ëŸ¬ ë²„ê·¸ëŠ” ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. get_last_lr()ë¡œ í˜„ì¬ í•™ìŠµë¥ ì„ ë¡œê¹…í•˜ê³ , ì˜ë„í•œ ìŠ¤ì¼€ì¤„ê³¼ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. íŠ¹íˆ ì¬í•™ìŠµ ì‹œ scheduler.last_epoch ì´ˆê¸°í™”ì— ì£¼ì˜í•˜ì„¸ìš”."
        }
      ]
    },
    {
      "id": "P3",
      "project_title": "ë°ì´í„° ë¡œë” & ì „ì²˜ë¦¬ í•¨ì •",
      "scenario": "ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í–ˆëŠ”ë°, í•™ìŠµ ì†ë„ê°€ ë„ˆë¬´ ëŠë¦¬ê±°ë‚˜, ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜, ì¬í˜„ì´ ì•ˆ ë˜ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.",
      "difficulty": 2,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "GPUëŠ” ë†€ê³  ìˆê³  CPUë§Œ ì¼í•´ìš”",
          "bug_type": "A",
          "bug_type_name": "I/O Bottleneck",
          "file_name": "dataloader.py",
          "buggy_code": "from torch.utils.data import DataLoader\n\n# ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (50,000ì¥, ê° 224x224)\ntrain_dataset = ImageDataset(root='./data', transform=transform)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True\n)\n\n# Training loop\nfor epoch in range(10):\n    for batch in train_loader:  # ì—¬ê¸°ì„œ ëŒ€ë¶€ë¶„ì˜ ì‹œê°„ ì†Œìš”\n        images = batch['image'].cuda()\n        labels = batch['label'].cuda()\n        # ... í•™ìŠµ ì½”ë“œ",
          "correct_code": "from torch.utils.data import DataLoader\n\n# ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (50,000ì¥, ê° 224x224)\ntrain_dataset = ImageDataset(root='./data', transform=transform)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    num_workers=4,      # ë³‘ë ¬ ë°ì´í„° ë¡œë”©\n    pin_memory=True     # GPU ì „ì†¡ ìµœì í™”\n)\n\n# Training loop\nfor epoch in range(10):\n    for batch in train_loader:\n        images = batch['image'].cuda(non_blocking=True)\n        labels = batch['label'].cuda(non_blocking=True)\n        # ... í•™ìŠµ ì½”ë“œ",
          "error_log": "=== ë°°ì¹˜ ì‹œê°„ ë¶„ì„ ===\nData Loading: 4.52s (90.4%)\nForward Pass: 0.31s (6.2%)\nBackward Pass: 0.17s (3.4%)\nTotal: 5.00s\n\nGPU Utilization: 8%\nCPU Utilization: 12%\n[ë³‘ëª©] ë°ì´í„° ë¡œë”©ì´ ì „ì²´ ì‹œê°„ì˜ 90% ì°¨ì§€",
          "success_log": "=== ë°°ì¹˜ ì‹œê°„ ë¶„ì„ ===\nData Loading: 0.08s (14.3%)\nForward Pass: 0.31s (55.4%)\nBackward Pass: 0.17s (30.3%)\nTotal: 0.56s\n\nGPU Utilization: 92%\nCPU Utilization: 45%\n[ì •ìƒ] GPUê°€ ì¶©ë¶„íˆ í™œìš©ë¨, 8.9ë°° ì†ë„ í–¥ìƒ",
          "hint": "DataLoaderì˜ num_workersì™€ pin_memory ì˜µì…˜ì„ í™•ì¸í•˜ì„¸ìš”. ê¸°ë³¸ê°’ì€ ë³‘ë ¬ ë¡œë”©ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "num_workers=",
              "pin_memory=True"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Data Loading Bottleneck",
            "description": "DataLoaderì˜ ê¸°ë³¸ ì„¤ì •(num_workers=0)ì€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤:\n\n1. GPUê°€ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ\n2. ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ë‹¤ìŒ ë°°ì¹˜ ë¡œë”© ì‹œì‘\n3. GPU ëŒ€ê¸°... (ì´ ì‹œê°„ì´ ë‚­ë¹„)\n4. ë¡œë”© ì™„ë£Œ í›„ GPUì— ì „ì†¡\n5. GPU ì²˜ë¦¬ ì‹œì‘\n\nnum_workers > 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´:\n1. ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ë“¤ì´ ë¯¸ë¦¬ ë‹¤ìŒ ë°°ì¹˜ë“¤ì„ ì¤€ë¹„\n2. GPU ì²˜ë¦¬ ì¤‘ì— ë‹¤ìŒ ë°°ì¹˜ê°€ ì´ë¯¸ ì¤€ë¹„ë¨\n3. GPU ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™”\n\npin_memory=TrueëŠ” CPUâ†’GPU ì „ì†¡ì„ ë¹„ë™ê¸°ë¡œ ë§Œë“¤ì–´ ì¶”ê°€ ìµœì í™”í•©ë‹ˆë‹¤.",
            "suggestion": "num_workersëŠ” CPU ì½”ì–´ ìˆ˜ì˜ 2~4ë°°ë¡œ ì‹œì‘í•˜ì„¸ìš” (ë„ˆë¬´ ë†’ìœ¼ë©´ ì˜¤ë²„í—¤ë“œ). GPU ì‚¬ìš© ì‹œ pin_memory=True, .cuda(non_blocking=True)ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: nvidia-smië¡œ GPU ì‚¬ìš©ë¥ ì„ í™•ì¸í•˜ì„¸ìš”. 30% ë¯¸ë§Œì´ë©´ ë°ì´í„° ë¡œë”© ë³‘ëª©ì„ ì˜ì‹¬í•˜ì„¸ìš”. num_workers ìµœì ê°’ì€ ì‹¤í—˜ìœ¼ë¡œ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "ë˜‘ê°™ì´ í•™ìŠµí–ˆëŠ”ë° ê²°ê³¼ê°€ ë§¤ë²ˆ ë‹¬ë¼ìš”",
          "bug_type": "B",
          "bug_type_name": "Reproducibility",
          "file_name": "seed_fix.py",
          "buggy_code": "import random\nimport numpy as np\nimport torch\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n\nset_seed(42)\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters())\n\n# DataLoader\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Training\nfor epoch in range(10):\n    for batch in train_loader:\n        # ... í•™ìŠµ ì½”ë“œ",
          "correct_code": "import random\nimport numpy as np\nimport torch\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # ë©€í‹° GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters())\n\n# DataLoaderì—ë„ ì‹œë“œ ì ìš©\ng = torch.Generator()\ng.manual_seed(42)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True, generator=g)\n\n# Training\nfor epoch in range(10):\n    for batch in train_loader:\n        # ... í•™ìŠµ ì½”ë“œ",
          "error_log": "=== Run 1 ===\nFinal Test Accuracy: 94.23%\n\n=== Run 2 (ë™ì¼ ì½”ë“œ) ===\nFinal Test Accuracy: 91.87%\n\n=== Run 3 (ë™ì¼ ì½”ë“œ) ===\nFinal Test Accuracy: 93.51%\n\n[ë¬¸ì œ] ì‹¤í—˜ ì¬í˜„ ë¶ˆê°€ëŠ¥",
          "success_log": "=== Run 1 ===\nFinal Test Accuracy: 93.42%\n\n=== Run 2 (ë™ì¼ ì½”ë“œ) ===\nFinal Test Accuracy: 93.42%\n\n=== Run 3 (ë™ì¼ ì½”ë“œ) ===\nFinal Test Accuracy: 93.42%\n\n[ì •ìƒ] ì™„ë²½í•œ ì¬í˜„ì„±",
          "hint": "randomê³¼ numpy ì‹œë“œë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. PyTorch, CUDA, cuDNN, DataLoader ëª¨ë‘ ì‹œë“œ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "torch.manual_seed(",
              "torch.cuda.manual_seed("
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Incomplete Seed Setting",
            "description": "ë”¥ëŸ¬ë‹ í•™ìŠµì—ì„œ ë‚œìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê³³:\n\n1. **Python random**: shuffle, random.choice ë“±\n2. **NumPy**: ë°ì´í„° ì „ì²˜ë¦¬, augmentation\n3. **PyTorch CPU**: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”, Dropout\n4. **PyTorch CUDA**: GPU ì—°ì‚°\n5. **cuDNN**: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ\n6. **DataLoader**: ë°ì´í„° ì…”í”Œë§\n\nëª¨ë“  ì†ŒìŠ¤ì— ì‹œë“œë¥¼ ì„¤ì •í•´ì•¼ ì™„ë²½í•œ ì¬í˜„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nì£¼ì˜: cudnn.deterministic=TrueëŠ” ì„±ëŠ¥ì„ ì•½ê°„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "suggestion": "ì¬í˜„ì„±ì´ ì¤‘ìš”í•œ ì‹¤í—˜ì—ì„œëŠ” ëª¨ë“  ì‹œë“œë¥¼ ê³ ì •í•˜ì„¸ìš”. ë‹¨, cudnn.deterministicì€ ì„±ëŠ¥ ì˜í–¥ì´ ìˆìœ¼ë¯€ë¡œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë„ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ì‹¤í—˜ ì¬í˜„ì„±ì€ ì—°êµ¬ì™€ ë””ë²„ê¹…ì˜ ê¸°ë³¸ì…ë‹ˆë‹¤. ì‹œë“œ ê³ ì • í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ ì¬ì‚¬ìš©í•˜ì„¸ìš”. ë˜í•œ í™˜ê²½ ì •ë³´(PyTorch ë²„ì „, CUDA ë²„ì „)ë„ í•¨ê»˜ ê¸°ë¡í•˜ì„¸ìš”."
        },
        {
          "step": 3,
          "title": "Validationì—ì„œ ë³¸ ë°ì´í„°ê°€ Trainì—ë„ ìˆì–´ìš”",
          "bug_type": "C",
          "bug_type_name": "Data Leakage",
          "bug_count": 2,
          "file_name": "data_split.py",
          "buggy_code": "from sklearn.model_selection import train_test_split\nimport albumentations as A\nimport numpy as np\n\n# ì›ë³¸ ë°ì´í„°: 1000ì¥\noriginal_images = load_images('./data')  # [0-255] ê°’\n\n# 1. ì „ì²´ ë°ì´í„°ë¡œ ì •ê·œí™” í†µê³„ ê³„ì‚°\nall_mean = np.mean(original_images)\nall_std = np.std(original_images)\n\n# 2. Augmentation ì ìš© (5ë°° ì¦ê°•)\naugmented_images = []\naugment = A.Compose([A.HorizontalFlip(), A.RandomBrightness()])\n\nfor img in original_images:\n    augmented_images.append(img)\n    for _ in range(4):\n        augmented_images.append(augment(image=img)['image'])\n\n# 3. ì¦ê°•ëœ ë°ì´í„°ì—ì„œ ë¶„í•  í›„ ì •ê·œí™”\nX_train, X_val = train_test_split(augmented_images, test_size=0.2)\nX_train = [(img - all_mean) / all_std for img in X_train]\nX_val = [(img - all_mean) / all_std for img in X_val]",
          "correct_code": "from sklearn.model_selection import train_test_split\nimport albumentations as A\nimport numpy as np\n\n# ì›ë³¸ ë°ì´í„°: 1000ì¥\noriginal_images = load_images('./data')  # [0-255] ê°’\n\n# 1. ë¨¼ì € ì›ë³¸ ë°ì´í„°ë¥¼ ë¶„í•  (ë²„ê·¸1 ìˆ˜ì •)\nX_train_orig, X_val_orig = train_test_split(original_images, test_size=0.2)\n\n# 2. Train ë°ì´í„°ë§Œìœ¼ë¡œ ì •ê·œí™” í†µê³„ ê³„ì‚° (ë²„ê·¸2 ìˆ˜ì •)\ntrain_mean = np.mean(X_train_orig)\ntrain_std = np.std(X_train_orig)\n\n# 3. Train ë°ì´í„°ì—ë§Œ Augmentation ì ìš©\naugment = A.Compose([A.HorizontalFlip(), A.RandomBrightness()])\n\nX_train = []\nfor img in X_train_orig:\n    X_train.append(img)\n    for _ in range(4):\n        X_train.append(augment(image=img)['image'])\n\nX_val = X_val_orig  # ì¦ê°• ì—†ìŒ\n\n# 4. Train í†µê³„ë¡œ ì •ê·œí™”\nX_train = [(img - train_mean) / train_std for img in X_train]\nX_val = [(img - train_mean) / train_std for img in X_val]",
          "error_log": "=== í•™ìŠµ ê²°ê³¼ ===\nTrain Accuracy: 98.5%\nValidation Accuracy: 97.8%\n\n=== ì‹¤ì œ ì„œë¹„ìŠ¤ ===\nTest Accuracy: 58.3%\n\n[ë¬¸ì œ] Valê³¼ Test ì„±ëŠ¥ ì°¨ì´ê°€ 39%p!\n[ë²„ê·¸1] ì¦ê°• í›„ ë¶„í•  â†’ ê°™ì€ ì›ë³¸ì´ Train/Valì— ë¶„ì‚°\n[ë²„ê·¸2] ì „ì²´ ë°ì´í„°ë¡œ mean/std ê³„ì‚° â†’ Val ì •ë³´ ëˆ„ìˆ˜",
          "success_log": "=== í•™ìŠµ ê²°ê³¼ ===\nTrain Accuracy: 94.2%\nValidation Accuracy: 86.4%\n\n=== ì‹¤ì œ ì„œë¹„ìŠ¤ ===\nTest Accuracy: 85.7%\n\n[ì •ìƒ] Valê³¼ Test ì„±ëŠ¥ì´ ìœ ì‚¬\n[ë¶„ì„] ë°ì´í„° ë¶„í•  â†’ Trainë§Œìœ¼ë¡œ í†µê³„ ê³„ì‚° â†’ ì¦ê°• ìˆœì„œ ì¤€ìˆ˜",
          "hint": "Data LeakageëŠ” ì—¬ëŸ¬ ê³³ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (1) ì¦ê°• ì‹œì , (2) ì •ê·œí™” í†µê³„ ê³„ì‚° ì‹œì ì„ ëª¨ë‘ ì ê²€í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "train_test_split(original_images",
              "np.mean(X_train",
              "np.std(X_train"
            ],
            "forbidden": [
              "train_test_split(augmented",
              "np.mean(original_images)",
              "np.std(original_images)"
            ]
          },
          "error_info": {
            "type": "Multiple Data Leakage",
            "description": "ì´ ì½”ë“œì—ëŠ” 2ê°€ì§€ Data Leakageê°€ ìˆìŠµë‹ˆë‹¤:\n\n**ë²„ê·¸1: Augmentation í›„ ë¶„í• **\n- ê°™ì€ ì›ë³¸ì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ Train/Valì— ì„ì„\n- Valì—ì„œ 'ê±°ì˜ ê°™ì€' ì´ë¯¸ì§€ë¥¼ ë´„ â†’ ê³¼ëŒ€ í‰ê°€\n\n**ë²„ê·¸2: ì „ì²´ ë°ì´í„°ë¡œ ì •ê·œí™” í†µê³„ ê³„ì‚°**\n- val/test ë°ì´í„°ì˜ mean/stdê°€ normalizeì— ë°˜ì˜ë¨\n- ì‹¤ì„œë¹„ìŠ¤ì—ì„œëŠ” train í†µê³„ë§Œ ì‚¬ìš© ê°€ëŠ¥ â†’ ë¶„í¬ ë¶ˆì¼ì¹˜\n\në‘ ëˆ„ìˆ˜ê°€ í•©ì³ì§€ë©´ Validationì´ ì‹¤ì œ ì„±ëŠ¥ì„ ì „í˜€ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤.",
            "suggestion": "ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìˆœì„œ: (1) ì›ë³¸ ë°ì´í„° ë¶„í•  â†’ (2) Trainë§Œìœ¼ë¡œ í†µê³„ ê³„ì‚° â†’ (3) Trainì—ë§Œ augmentation â†’ (4) Train í†µê³„ë¡œ ì •ê·œí™”. ì´ ìˆœì„œë¥¼ ì§€í‚¤ë©´ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: Data LeakageëŠ” í•œ ê³³ì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ê³³ì—ì„œ ë™ì‹œì— ë°œìƒí•©ë‹ˆë‹¤. Val ì„±ëŠ¥ì´ ìœ ë‚œíˆ ì¢‹ìœ¼ë©´ íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì ê²€í•˜ì„¸ìš”. íŠ¹íˆ ì „ì²˜ë¦¬ í†µê³„(mean, std, vocab ë“±)ê°€ test ë°ì´í„°ë¥¼ ë³´ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        }
      ]
    },
    {
      "id": "P4",
      "project_title": "ëª¨ë¸ ì €ì¥ & ë°°í¬ ì‹¤ìˆ˜",
      "scenario": "í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë‚˜ì¤‘ì— ë‹¤ì‹œ ë¶ˆëŸ¬ì™”ëŠ”ë°, ì„±ëŠ¥ì´ ë‹¤ë¥´ê±°ë‚˜ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. ë˜ëŠ” ë‹¤ë¥¸ í™˜ê²½ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
      "difficulty": 3,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "ì €ì¥í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™”ë”ë‹ˆ ì„±ëŠ¥ì´ ë§ê°€ì¡Œì–´ìš”",
          "bug_type": "A",
          "bug_type_name": "Save/Load Error",
          "file_name": "save_model.py",
          "buggy_code": "import torch\n\n# í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥\nmodel = MyModel()\n# ... í•™ìŠµ ...\nprint(f\"Before save: Accuracy = {evaluate(model)}\")\n\n# ëª¨ë¸ ì €ì¥\ntorch.save(model.state_dict(), 'model.pth')\n\n# ë‚˜ì¤‘ì— ë¡œë“œ\nloaded_model = torch.load('model.pth')  # ëª¨ë¸ ë¡œë“œ\nloaded_model.eval()\nprint(f\"After load: Accuracy = {evaluate(loaded_model)}\")",
          "correct_code": "import torch\n\n# í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥\nmodel = MyModel()\n# ... í•™ìŠµ ...\nprint(f\"Before save: Accuracy = {evaluate(model)}\")\n\n# ëª¨ë¸ ì €ì¥\ntorch.save(model.state_dict(), 'model.pth')\n\n# ì˜¬ë°”ë¥¸ ë¡œë“œ ë°©ë²•\nloaded_model = MyModel()  # ë¨¼ì € ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\nloaded_model.load_state_dict(torch.load('model.pth'))  # state_dict ë¡œë“œ\nloaded_model.eval()\nprint(f\"After load: Accuracy = {evaluate(loaded_model)}\")",
          "error_log": "Before save: Accuracy = 94.52%\n\n# ë¡œë“œ ì‹œë„\n>>> loaded_model = torch.load('model.pth')\n>>> type(loaded_model)\n<class 'collections.OrderedDict'>  # ëª¨ë¸ì´ ì•„ë‹ˆë¼ ë”•ì…”ë„ˆë¦¬!\n\n>>> loaded_model.eval()\nAttributeError: 'OrderedDict' object has no attribute 'eval'",
          "success_log": "Before save: Accuracy = 94.52%\n\n# ì˜¬ë°”ë¥¸ ë¡œë“œ\n>>> loaded_model = MyModel()\n>>> loaded_model.load_state_dict(torch.load('model.pth'))\n<All keys matched successfully>\n>>> loaded_model.eval()\n>>> evaluate(loaded_model)\nAfter load: Accuracy = 94.52%",
          "hint": "torch.save(model.state_dict(), ...)ë¡œ ì €ì¥í–ˆë‹¤ë©´, ë¡œë“œí•  ë•Œë„ state_dict ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "load_state_dict(",
              "MyModel()"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "State Dict Confusion",
            "description": "PyTorch ëª¨ë¸ ì €ì¥ ë°©ì‹:\n\n**1. state_dict ì €ì¥ (ê¶Œì¥):**\n- torch.save(model.state_dict(), 'model.pth')\n- ì €ì¥: íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜, ë°”ì´ì–´ìŠ¤)ë§Œ\n- ë¡œë“œ: model = MyModel(); model.load_state_dict(torch.load(...))\n\n**2. ì „ì²´ ëª¨ë¸ ì €ì¥:**\n- torch.save(model, 'model.pth')\n- ì €ì¥: ëª¨ë¸ êµ¬ì¡° + íŒŒë¼ë¯¸í„°\n- ë¡œë“œ: model = torch.load('model.pth')\n- ë‹¨ì : ì €ì¥ ì‹œì ì˜ ì½”ë“œ ê²½ë¡œì— ì˜ì¡´\n\në¬¸ì œ ì½”ë“œëŠ” state_dictë¥¼ ì €ì¥í•˜ê³  ì „ì²´ ëª¨ë¸ ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì„œ ë”•ì…”ë„ˆë¦¬ ê°ì²´ê°€ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "suggestion": "state_dict ë°©ì‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì½”ë“œ ê²½ë¡œ ì˜ì¡´ì„±ì´ ì—†ê³ , ë‹¤ë¥¸ í™˜ê²½ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤. ì €ì¥/ë¡œë“œ ë°©ì‹ì„ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: state_dict ë°©ì‹ì´ í‘œì¤€ì…ë‹ˆë‹¤. ì¶”ê°€ë¡œ optimizer.state_dict()ë„ í•¨ê»˜ ì €ì¥í•˜ë©´ í•™ìŠµì„ ì´ì–´ì„œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œí–ˆë”ë‹ˆ ì„±ëŠ¥ì´ ë–¨ì–´ì¡Œì–´ìš”",
          "bug_type": "B",
          "bug_type_name": "Checkpoint Error",
          "bug_count": 3,
          "file_name": "checkpoint.py",
          "buggy_code": "import torch\nfrom torch.cuda.amp import GradScaler\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\nscaler = GradScaler()  # Mixed Precision í•™ìŠµ\n\n# ì²´í¬í¬ì¸íŠ¸ ì €ì¥\ndef save_checkpoint(model, epoch, loss):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'loss': loss\n    }, 'checkpoint.pth')\n\n# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\ndef load_checkpoint(model):\n    checkpoint = torch.load('checkpoint.pth')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    return checkpoint['epoch']\n\n# Epoch 50ì—ì„œ ì¤‘ë‹¨ í›„ ì¬ê°œ\nstart_epoch = load_checkpoint(model)\nfor epoch in range(start_epoch, 100):\n    # AMP í•™ìŠµ ë£¨í”„...",
          "correct_code": "import torch\nfrom torch.cuda.amp import GradScaler\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\nscaler = GradScaler()  # Mixed Precision í•™ìŠµ\n\n# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ëª¨ë“  ìƒíƒœ í¬í•¨)\ndef save_checkpoint(model, optimizer, scheduler, scaler, epoch, loss):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'scaler_state_dict': scaler.state_dict(),\n        'loss': loss\n    }, 'checkpoint.pth')\n\n# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\ndef load_checkpoint(model, optimizer, scheduler, scaler):\n    checkpoint = torch.load('checkpoint.pth')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n    return checkpoint['epoch']\n\n# Epoch 50ì—ì„œ ì¤‘ë‹¨ í›„ ì¬ê°œ\nstart_epoch = load_checkpoint(model, optimizer, scheduler, scaler)\nfor epoch in range(start_epoch, 100):\n    # AMP í•™ìŠµ ë£¨í”„...",
          "error_log": "=== ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Epoch 50) ===\nLoss: 0.1523, LR: 0.00025, Accuracy: 94.2%\n\n=== í•™ìŠµ ì¬ê°œ ===\nEpoch 51: Loss = 0.8934, LR = 0.001 (ì´ˆê¸°í™”ë¨!)\n[ë²„ê·¸1] optimizer momentum ë¦¬ì…‹\n[ë²„ê·¸2] schedulerê°€ epoch 0ë¶€í„° ë‹¤ì‹œ ì‹œì‘\n[ë²„ê·¸3] AMP scaler scale ë¦¬ì…‹ â†’ gradient underflow\n\nEpoch 60: Loss = 0.3421 (10 epoch ë‚­ë¹„)\nEpoch 80: Loss = nan (AMP scale ë¶ˆì•ˆì •)",
          "success_log": "=== ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Epoch 50) ===\nLoss: 0.1523, LR: 0.00025, Accuracy: 94.2%\n\n=== í•™ìŠµ ì¬ê°œ ===\nEpoch 51: Loss = 0.1498, LR = 0.00024 (ì—°ì†!)\nEpoch 52: Loss = 0.1456, LR = 0.00023\n\n[ì •ìƒ] optimizer, scheduler, scaler ëª¨ë‘ ë³µì›ë¨",
          "hint": "ì²´í¬í¬ì¸íŠ¸ì—ëŠ” ëª¨ë¸ë¿ ì•„ë‹ˆë¼ í•™ìŠµ ìƒíƒœ ì „ì²´ê°€ í•„ìš”í•©ë‹ˆë‹¤. optimizer, scheduler, ê·¸ë¦¬ê³  AMP ì‚¬ìš© ì‹œ scaler ìƒíƒœë„ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "optimizer.state_dict()",
              "scheduler.state_dict()",
              "scaler.state_dict()",
              "optimizer.load_state_dict(",
              "scheduler.load_state_dict(",
              "scaler.load_state_dict("
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Incomplete Checkpoint (Multiple States)",
            "description": "ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëˆ„ë½ë˜ê¸° ì‰¬ìš´ ìƒíƒœë“¤:\n\n**1. Optimizer ìƒíƒœ:**\n- Adamì˜ momentum (exp_avg, exp_avg_sq)\n- ëˆ„ë½ ì‹œ: adaptive LR ë¦¬ì…‹ â†’ Loss ê¸‰ì¦\n\n**2. Scheduler ìƒíƒœ:**\n- last_epoch, ë‚´ë¶€ ì¹´ìš´í„°\n- ëˆ„ë½ ì‹œ: LRì´ ì´ˆê¸°ê°’ìœ¼ë¡œ ë¦¬ì…‹ â†’ í•™ìŠµ ë¶ˆì•ˆì •\n\n**3. AMP GradScaler ìƒíƒœ:**\n- scale factor, growth tracker\n- ëˆ„ë½ ì‹œ: scaleì´ ì´ˆê¸°í™”ë˜ì–´ gradient underflow/overflow\n- ì‹¬í•˜ë©´ NaN ë°œìƒ\n\nì„¸ ê°€ì§€ ëª¨ë‘ ì €ì¥í•´ì•¼ í•™ìŠµì„ ì™„ë²½í•˜ê²Œ ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "suggestion": "ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸: model, optimizer, scheduler, scaler(AMPì‹œ), epoch, loss, rng_state(ì¬í˜„ì„±). íŠ¹íˆ AMP í•™ìŠµì—ì„œ scaler ëˆ„ë½ì€ NaNì˜ ì›ì¸ì´ ë©ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ëŒ€ê·œëª¨ í•™ìŠµ(LLM fine-tuning, ë¶„ì‚° í•™ìŠµ)ì—ì„œ ì²´í¬í¬ì¸íŠ¸ ëˆ„ë½ì€ ìˆ˜ ì¼ê°„ì˜ GPU ë¹„ìš© ë‚­ë¹„ì…ë‹ˆë‹¤. save/load í•¨ìˆ˜ë¥¼ í…œí”Œë¦¿í™”í•˜ê³ , ë¡œë“œ í›„ LRê³¼ scaleì„ ë¡œê¹…í•´ì„œ ê²€ì¦í•˜ì„¸ìš”."
        },
        {
          "step": 3,
          "title": "ë‹¤ë¥¸ ì„œë²„ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ìš”",
          "bug_type": "C",
          "bug_type_name": "Device Compat",
          "file_name": "cross_device.py",
          "buggy_code": "import torch\n\n# === GPU ì„œë²„ì—ì„œ í•™ìŠµ ë° ì €ì¥ ===\nmodel = MyModel().cuda()\n# ... í•™ìŠµ ...\ntorch.save(model.state_dict(), 'model.pth')\n\n# === CPU ì„œë²„ì—ì„œ ë¡œë“œ (ì—ëŸ¬!) ===\nmodel = MyModel()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()",
          "correct_code": "import torch\n\n# === GPU ì„œë²„ì—ì„œ í•™ìŠµ ë° ì €ì¥ ===\nmodel = MyModel().cuda()\n# ... í•™ìŠµ ...\ntorch.save(model.state_dict(), 'model.pth')\n\n# === CPU ì„œë²„ì—ì„œ ë¡œë“œ ===\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MyModel()\nmodel.load_state_dict(torch.load('model.pth', map_location=device))\nmodel.to(device)\nmodel.eval()",
          "error_log": "# CPU ì„œë²„ì—ì„œ ì‹¤í–‰\n>>> torch.cuda.is_available()\nFalse\n\n>>> model.load_state_dict(torch.load('model.pth'))\nRuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu').",
          "success_log": "# CPU ì„œë²„ì—ì„œ ì‹¤í–‰\n>>> torch.cuda.is_available()\nFalse\n>>> device = torch.device('cpu')\n>>> model.load_state_dict(torch.load('model.pth', map_location=device))\n<All keys matched successfully>\n>>> model.to(device)\n>>> model.eval()\n\n# ì¶”ë¡  ì •ìƒ ë™ì‘",
          "hint": "torch.load()ì˜ map_location íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ì €ì¥ëœ í…ì„œë¥¼ ì›í•˜ëŠ” deviceë¡œ ë§¤í•‘í•˜ì—¬ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "map_location"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Cross-Device Compatibility Error",
            "description": "torch.save()ëŠ” í…ì„œì˜ device ì •ë³´ë„ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.\n\n**GPUì—ì„œ ì €ì¥ ì‹œ:**\n- í…ì„œ ë°ì´í„°: [0.123, 0.456, ...]\n- Device ì •ë³´: 'cuda:0'\n\n**CPUì—ì„œ ë¡œë“œ ì‹œ:**\n- torch.load()ê°€ 'cuda:0'ìœ¼ë¡œ í…ì„œë¥¼ ë³µì›í•˜ë ¤ ì‹œë„\n- CUDA ì—†ìŒ â†’ ì—ëŸ¬\n\n**í•´ê²°ì±…:**\n- map_location='cpu': ëª¨ë“  í…ì„œë¥¼ CPUë¡œ\n- map_location=device: ë™ì ìœ¼ë¡œ device ê²°ì •\n- map_location={'cuda:0': 'cuda:1'}: íŠ¹ì • GPUâ†’ë‹¤ë¥¸ GPU ë§¤í•‘",
            "suggestion": "ë°°í¬ í™˜ê²½ì´ í•™ìŠµ í™˜ê²½ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¡œë“œ ì½”ë“œì—ì„œ í•­ìƒ map_locationì„ ì‚¬ìš©í•˜ì„¸ìš”. íŠ¹íˆ torch.device('cuda' if torch.cuda.is_available() else 'cpu') íŒ¨í„´ì„ í™œìš©í•˜ë©´ ì–´ëŠ í™˜ê²½ì—ì„œë„ ë™ì‘í•©ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: GPUì—ì„œ í•™ìŠµí•˜ê³  CPUì—ì„œ ì„œë¹™í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë˜í•œ ë©€í‹° GPUë¡œ í•™ìŠµí•˜ê³  ë‹¨ì¼ GPUë¡œ ë°°í¬í•˜ëŠ” ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤. map_location ì‚¬ìš©ì„ ìŠµê´€í™”í•˜ì„¸ìš”."
        }
      ]
    },
    {
      "id": "P5",
      "project_title": "LLM & Transformer ë””ë²„ê¹…",
      "scenario": "Transformer ê¸°ë°˜ ëª¨ë¸(BERT, GPT ë“±)ì„ Fine-tuningí•˜ê±°ë‚˜ ì¶”ë¡ í•˜ëŠ”ë°, ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¤ê±°ë‚˜ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.",
      "difficulty": 3,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "íŒ¨ë”© í† í°ì´ ë¬¸ì¥ ì˜ë¯¸ë¥¼ ë§ì³ìš”",
          "bug_type": "A",
          "bug_type_name": "Attention Mask",
          "file_name": "bert_inference.py",
          "buggy_code": "from transformers import BertModel, BertTokenizer\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()\n\ndef get_embedding(text, max_length=128):\n    # í† í¬ë‚˜ì´ì§•\n    tokens = tokenizer(\n        text,\n        padding='max_length',\n        max_length=max_length,\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    with torch.no_grad():\n        # ëª¨ë¸ ì¶”ë¡ \n        outputs = model(input_ids=tokens['input_ids'])\n        \n    # [CLS] í† í°ì˜ ì„ë² ë”© ë°˜í™˜\n    return outputs.last_hidden_state[:, 0, :]\n\n# ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°\nemb_a = get_embedding(\"I love you\")\nemb_b = get_embedding(\"I love you\")\nemb_c = get_embedding(\"I hate everything in this world\")\n\nprint(f\"A-B similarity: {cosine_similarity(emb_a, emb_b)}\")\nprint(f\"A-C similarity: {cosine_similarity(emb_a, emb_c)}\")",
          "correct_code": "from transformers import BertModel, BertTokenizer\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()\n\ndef get_embedding(text, max_length=128):\n    # í† í¬ë‚˜ì´ì§•\n    tokens = tokenizer(\n        text,\n        padding='max_length',\n        max_length=max_length,\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    with torch.no_grad():\n        # attention_mask ì „ë‹¬!\n        outputs = model(\n            input_ids=tokens['input_ids'],\n            attention_mask=tokens['attention_mask']  # íŒ¨ë”© ë§ˆìŠ¤í‚¹\n        )\n        \n    # [CLS] í† í°ì˜ ì„ë² ë”© ë°˜í™˜\n    return outputs.last_hidden_state[:, 0, :]\n\n# ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°\nemb_a = get_embedding(\"I love you\")\nemb_b = get_embedding(\"I love you\")\nemb_c = get_embedding(\"I hate everything in this world\")\n\nprint(f\"A-B similarity: {cosine_similarity(emb_a, emb_b)}\")\nprint(f\"A-C similarity: {cosine_similarity(emb_a, emb_c)}\")",
          "error_log": "A: \"I love you\" (125 padding tokens)\nB: \"I love you\" (125 padding tokens)\nC: \"I hate everything in this world\" (120 padding tokens)\n\nA-B similarity: 0.9234 (OK)\nA-C similarity: 0.9456 (ì´ìƒ! A-Bë³´ë‹¤ ë†’ìŒ)\n\n[ì›ì¸] íŒ¨ë”© í† í°ì´ attentionì— í¬í•¨ë˜ì–´ ì˜ë¯¸ ì™œê³¡",
          "success_log": "A: \"I love you\" (masked: 125 padding tokens)\nB: \"I love you\" (masked: 125 padding tokens)\nC: \"I hate everything in this world\" (masked: 120 padding tokens)\n\nA-B similarity: 1.0000 (ì™„ë²½íˆ ë™ì¼)\nA-C similarity: 0.2341 (ë‚®ìŒ - ì˜ë¯¸ ì°¨ì´ ë°˜ì˜)\n\n[ì •ìƒ] ì‹¤ì œ í† í°ë§Œ attentionì— ë°˜ì˜",
          "hint": "Tokenizerê°€ ë°˜í™˜í•˜ëŠ” attention_maskëŠ” ì‹¤ì œ í† í°=1, íŒ¨ë”©=0ì…ë‹ˆë‹¤. ì´ ë§ˆìŠ¤í¬ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•´ì•¼ íŒ¨ë”©ì´ ë¬´ì‹œë©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "attention_mask"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Missing Attention Mask",
            "description": "Transformerì˜ Self-Attentionì€ ëª¨ë“  í† í° ê°„ì˜ ê´€ê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:\n\n**Attention mask ì—†ì´:**\n- [CLS] I love you [PAD] [PAD] [PAD] ... (125ê°œ)\n- [CLS]ê°€ ëª¨ë“  [PAD] í† í°ì—ë„ attention\n- íŒ¨ë”©ì´ ë§ì„ìˆ˜ë¡ ì„ë² ë”©ì´ íŒ¨ë”©ì— ì˜í•´ ì™œê³¡\n\n**Attention mask ìˆìœ¼ë©´:**\n- attention_mask = [1, 1, 1, 1, 0, 0, 0, ...]\n- [PAD] ìœ„ì¹˜ì˜ attention score = -âˆ â†’ softmax í›„ 0\n- ì‹¤ì œ í† í°ë§Œ attention ê³„ì‚°ì— ì°¸ì—¬\n\nì§§ì€ ë¬¸ì¥ë¼ë¦¬ íŒ¨ë”©ì´ ë¹„ìŠ·í•˜ë©´ 'íŒ¨ë”© ìœ ì‚¬ë„'ê°€ ë†’ì•„ì ¸ ì˜ë¯¸ ë¹„êµê°€ ì™œê³¡ë©ë‹ˆë‹¤.",
            "suggestion": "HuggingFace ëª¨ë¸ ì‚¬ìš© ì‹œ tokenizerì˜ attention_maskë¥¼ í•­ìƒ ëª¨ë¸ì— ì „ë‹¬í•˜ì„¸ìš”. **tokensë¥¼ ì‚¬ìš©í•˜ë©´ í•œ ë²ˆì— ì „ë‹¬ë©ë‹ˆë‹¤: model(**tokens)"
          },
          "coaching": "ğŸ¯ í˜„ì—…: BERT, GPT ë“± Transformer ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ attention_maskëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. íŠ¹íˆ ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë¬¸ì¥ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ íŒ¨ë”©ì´ ë¶ˆê· í˜•í•´ì§€ë¯€ë¡œ ë°˜ë“œì‹œ ë§ˆìŠ¤í‚¹í•´ì•¼ í•©ë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "Tokenizerì™€ ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ ë§ì„ í•´ìš”",
          "bug_type": "B",
          "bug_type_name": "Tokenizer Mismatch",
          "file_name": "mismatched_tokenizer.py",
          "buggy_code": "from transformers import BertForSequenceClassification, BertTokenizer\nimport torch\n\n# Fine-tuningëœ ëª¨ë¸ ë¡œë“œ\nmodel = BertForSequenceClassification.from_pretrained('./my_finetuned_model')\nmodel.eval()\n\n# Tokenizer ë¡œë“œ\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\ndef predict(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    probs = torch.softmax(outputs.logits, dim=1)\n    return 'Positive' if probs[0, 1] > 0.5 else 'Negative', probs[0].tolist()\n\nresult, probs = predict(\"The movie was great!\")\nprint(f\"Prediction: {result}, Probabilities: {probs}\")",
          "correct_code": "from transformers import BertForSequenceClassification, BertTokenizer\nimport torch\n\n# Fine-tuningëœ ëª¨ë¸ ë¡œë“œ\nmodel = BertForSequenceClassification.from_pretrained('./my_finetuned_model')\nmodel.eval()\n\n# ê°™ì€ ëª¨ë¸ì˜ tokenizer ì‚¬ìš©!\ntokenizer = BertTokenizer.from_pretrained('./my_finetuned_model')  # ëª¨ë¸ê³¼ ë™ì¼ ê²½ë¡œ\n# ë˜ëŠ”: BertTokenizer.from_pretrained('bert-base-uncased')  # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼\n\ndef predict(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    probs = torch.softmax(outputs.logits, dim=1)\n    return 'Positive' if probs[0, 1] > 0.5 else 'Negative', probs[0].tolist()\n\nresult, probs = predict(\"The movie was great!\")\nprint(f\"Prediction: {result}, Probabilities: {probs}\")",
          "error_log": "Input: \"The movie was great!\"\n\n[ì˜ëª»ëœ Tokenizer: bert-large-uncased]\nTokens: ['the', 'movie', 'was', 'great', '!']\nToken IDs: [1996, 3185, 2001, 2307, 999]\n\nPrediction: Negative (0.01, 0.99)\n[í‹€ë¦¼] ëª¨ë¸ì´ ë³¸ ì  ì—†ëŠ” í† í° ID ë§¤í•‘",
          "success_log": "Input: \"The movie was great!\"\n\n[ì˜¬ë°”ë¥¸ Tokenizer: bert-base-uncased]\nTokens: ['the', 'movie', 'was', 'great', '!']\nToken IDs: [1996, 3185, 2001, 2307, 999]\n\nPrediction: Positive (0.97, 0.03)\n[ì •ë‹µ] í•™ìŠµ ì‹œì™€ ë™ì¼í•œ í† í°í™”",
          "hint": "ëª¨ë¸ê³¼ TokenizerëŠ” í•­ìƒ ì§ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤. Fine-tuningí•œ ëª¨ë¸ì„ ì €ì¥í•  ë•Œ tokenizerë„ ê°™ì€ ê²½ë¡œì— ì €ì¥í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_any": [
              "'./my_finetuned_model'",
              "'bert-base-uncased'"
            ],
            "forbidden": [
              "'bert-large-uncased'"
            ]
          },
          "error_info": {
            "type": "Tokenizer-Model Mismatch",
            "description": "Tokenizerì™€ ëª¨ë¸ì˜ ë¶ˆì¼ì¹˜ ë¬¸ì œ:\n\n**ê°™ì€ ë‹¨ì–´ â†’ ë‹¤ë¥¸ í† í° ID:**\n- bert-base: \"great\" â†’ ID 2307\n- bert-large: \"great\" â†’ ID 2307 (ê°™ì„ ìˆ˜ ìˆì§€ë§Œ...)\n- roberta: \"great\" â†’ ID 372 (ì™„ì „íˆ ë‹¤ë¦„!)\n\n**ë¬¸ì œ:**\n- ëª¨ë¸ì€ í•™ìŠµ ì‹œ íŠ¹ì • vocabularyë¡œ í•™ìŠµ\n- ë‹¤ë¥¸ tokenizerëŠ” ë‹¤ë¥¸ ID ë§¤í•‘\n- ID 2307ì´ í•™ìŠµ ë•Œ \"great\"ì˜€ì§€ë§Œ, ë‹¤ë¥¸ tokenizerì—ì„œëŠ” \"terrible\"ì¼ ìˆ˜ ìˆìŒ\n- ê²°ê³¼: ì™„ì „íˆ ì—‰ëš±í•œ ì˜ˆì¸¡\n\n**íŠ¹íˆ ìœ„í—˜í•œ ê²½ìš°:**\n- ëª¨ë¸ ê³„ì—´ì´ ë‹¤ë¥¸ ê²½ìš° (BERT vs RoBERTa)\n- ê°™ì€ ê³„ì—´ì´ì§€ë§Œ í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš° (base vs large)",
            "suggestion": "ëª¨ë¸ ì €ì¥ ì‹œ tokenizer.save_pretrained(path)ë¡œ tokenizerë„ í•¨ê»˜ ì €ì¥í•˜ì„¸ìš”. ë¡œë“œí•  ë•ŒëŠ” ëª¨ë¸ê³¼ ê°™ì€ ê²½ë¡œì—ì„œ tokenizerë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: HuggingFace ëª¨ë¸ ë°°í¬ ì‹œ ëª¨ë¸+tokenizer+configë¥¼ ì„¸íŠ¸ë¡œ ê´€ë¦¬í•˜ì„¸ìš”. ë²„ì „ ë¶ˆì¼ì¹˜ëŠ” ì°¾ê¸° ì–´ë ¤ìš´ ë²„ê·¸ì˜ ì›ì¸ì´ ë©ë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "GPTê°€ ì´ìƒí•œ ë§ë§Œ ë°˜ë³µí•˜ê³  ë„ˆë¬´ ëŠë ¤ìš”",
          "bug_type": "C",
          "bug_type_name": "Generation Bug",
          "bug_count": 3,
          "file_name": "gpt_generate.py",
          "buggy_code": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.eval()\nmodel.cuda()\n\ndef generate_text(prompt, max_new_tokens=100):\n    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs['input_ids'],\n            max_new_tokens=max_new_tokens,\n            use_cache=False,\n        )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# ë²¤ì¹˜ë§ˆí¬\nimport time\nstart = time.time()\nresult = generate_text(\"Once upon a time\")\nprint(f\"Time: {time.time() - start:.2f}s\")\nprint(result)",
          "correct_code": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token  # pad_token ì„¤ì •\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.eval()\nmodel.cuda()\n\ndef generate_text(prompt, max_new_tokens=100):\n    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs['input_ids'],\n            max_new_tokens=max_new_tokens,\n            use_cache=True,            # KV cache í™œì„±í™” (ì†ë„)\n            do_sample=True,            # ìƒ˜í”Œë§ í™œì„±í™” (í’ˆì§ˆ)\n            temperature=0.8,\n            top_p=0.95,\n            repetition_penalty=1.2,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# ë²¤ì¹˜ë§ˆí¬\nimport time\nstart = time.time()\nresult = generate_text(\"Once upon a time\")\nprint(f\"Time: {time.time() - start:.2f}s\")\nprint(result)",
          "error_log": "Time: 8.42s  (100 í† í° ìƒì„±ì— 8ì´ˆ!)\n\nOutput: \"Once upon a time, the the the the the the the the...\"\n\n[ë²„ê·¸1] use_cache=False â†’ ë§¤ í† í°ë§ˆë‹¤ ì „ì²´ ì‹œí€€ìŠ¤ ì¬ê³„ì‚°\n[ë²„ê·¸2] Greedy decoding â†’ ë°˜ë³µ ë£¨í”„\n[ë²„ê·¸3] pad_token ë¯¸ì„¤ì • â†’ ë°°ì¹˜ ì¶”ë¡  ì‹œ ì—ëŸ¬ ë°œìƒ ì˜ˆì •\n\nUserWarning: Setting `pad_token_id` to `eos_token_id` for open-end generation.",
          "success_log": "Time: 0.87s  (9.7ë°° ë¹ ë¦„!)\n\nOutput: \"Once upon a time, there was a young princess who lived in a magnificent castle at the edge of an enchanted forest. Every morning, she would wake up to the sound of birds singing...\"\n\n[ì •ìƒ] KV cacheë¡œ ë¹ ë¥¸ ì¶”ë¡  + ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìƒì„±",
          "hint": "LLM ì¶”ë¡ ì—ì„œ (1) KV cacheëŠ” ì†ë„, (2) sampling íŒŒë¼ë¯¸í„°ëŠ” í’ˆì§ˆ, (3) pad_tokenì€ ë°°ì¹˜ ì²˜ë¦¬ì— í•„ìˆ˜ì…ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "use_cache=True",
              "do_sample=True",
              "pad_token"
            ],
            "required_any": [
              "temperature=",
              "top_p=",
              "repetition_penalty="
            ],
            "forbidden": [
              "use_cache=False"
            ]
          },
          "error_info": {
            "type": "LLM Inference Issues (Multiple)",
            "description": "LLM ì¶”ë¡ ì˜ 3ê°€ì§€ í•µì‹¬ ì´ìŠˆ:\n\n**1. KV Cache (ì†ë„)**\n- TransformerëŠ” ì´ì „ í† í°ì˜ Key-Valueë¥¼ ì¬ì‚¬ìš© ê°€ëŠ¥\n- use_cache=False: ë§¤ í† í°ë§ˆë‹¤ O(nÂ²) ì—°ì‚° ë°˜ë³µ\n- use_cache=True: ìƒˆ í† í°ì— ëŒ€í•´ì„œë§Œ ê³„ì‚° â†’ O(n) per token\n- íš¨ê³¼: ê¸´ ì‹œí€€ìŠ¤ì—ì„œ 10ë°° ì´ìƒ ì†ë„ í–¥ìƒ\n\n**2. Sampling íŒŒë¼ë¯¸í„° (í’ˆì§ˆ)**\n- Greedy: ê°€ì¥ ë†’ì€ í™•ë¥ ë§Œ ì„ íƒ â†’ ë°˜ë³µ\n- do_sample + temperature/top_p: ë‹¤ì–‘ì„± í™•ë³´\n\n**3. Pad Token (ì•ˆì •ì„±)**\n- GPT2ëŠ” pad_tokenì´ ê¸°ë³¸ ë¯¸ì„¤ì •\n- ë°°ì¹˜ ì¶”ë¡  ì‹œ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ íŒ¨ë”© í•„ìš”\n- ë¯¸ì„¤ì • ì‹œ ê²½ê³  ë˜ëŠ” ì—ëŸ¬",
            "suggestion": "LLM ì¶”ë¡  ì²´í¬ë¦¬ìŠ¤íŠ¸: (1) use_cache=True, (2) do_sample + temperature, (3) pad_token ì„¤ì •, (4) í•„ìš”ì‹œ attention_mask. í”„ë¡œë•ì…˜ì—ì„œëŠ” vLLM, TGI ê°™ì€ ìµœì í™” ì„œë¹™ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: LLM ì„œë¹™ì—ì„œ KV cache ë¯¸ì‚¬ìš©ì€ GPU ë¹„ìš© ë‚­ë¹„ì…ë‹ˆë‹¤. ë˜í•œ ê¸´ ì»¨í…ìŠ¤íŠ¸(8K+ í† í°)ì—ì„œëŠ” KV cache ë©”ëª¨ë¦¬ë„ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. vLLMì˜ PagedAttention ê°™ì€ ê¸°ìˆ ì„ ê³µë¶€í•˜ì„¸ìš”."
        }
      ]
    }
  ]
}
