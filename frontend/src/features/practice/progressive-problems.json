{
  "progressiveProblems": [
    {
      "id": "P1",
      "project_title": "ë”¥ëŸ¬ë‹ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…",
      "scenario": "í•™ìŠµëœ ëª¨ë¸ì„ ë°°í¬í–ˆëŠ”ë°, ì¶”ë¡  ê²°ê³¼ê°€ í•™ìŠµ ì‹œì™€ ë‹¤ë¥´ê²Œ ë‚˜ì˜µë‹ˆë‹¤. ë™ì¼í•œ ì…ë ¥ì¸ë°ë„ ë§¤ë²ˆ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ê±°ë‚˜, ë©”ëª¨ë¦¬ê°€ ê³„ì† ì¦ê°€í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.",
      "difficulty": 2,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "ì¶”ë¡ í•  ë•Œë§ˆë‹¤ ê²°ê³¼ê°€ ë‹¬ë¼ìš” (model.eval ëˆ„ë½)",
          "bug_type": "A",
          "bug_type_name": "Inference Mode",
          "file_name": "inference.py",
          "buggy_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 64, 3)\n        self.bn = nn.BatchNorm2d(64)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(64 * 6 * 6, 10)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = torch.relu(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n# í•™ìŠµ ì™„ë£Œ í›„ ì¶”ë¡ \nmodel = Model()\nmodel.load_state_dict(torch.load('model.pth'))\n\n# ì¶”ë¡  ì‹¤í–‰\nwith torch.no_grad():\n    for img in test_images:\n        output = model(img)\n        pred = torch.softmax(output, dim=1)\n        print(pred)",
          "correct_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 64, 3)\n        self.bn = nn.BatchNorm2d(64)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(64 * 6 * 6, 10)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = torch.relu(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n# í•™ìŠµ ì™„ë£Œ í›„ ì¶”ë¡ \nmodel = Model()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜\n\n# ì¶”ë¡  ì‹¤í–‰\nwith torch.no_grad():\n    for img in test_images:\n        output = model(img)\n        pred = torch.softmax(output, dim=1)\n        print(pred)",
          "error_log": "=== ì¶”ë¡  ê²°ê³¼ (5íšŒ ì‹¤í–‰) ===\nRun 1: [0.12, 0.03, 0.82, ...] â†’ class 2 (conf: 0.82)\nRun 2: [0.08, 0.05, 0.78, ...] â†’ class 2 (conf: 0.78)\nRun 3: [0.15, 0.02, 0.85, ...] â†’ class 2 (conf: 0.85)\nRun 4: [0.22, 0.04, 0.71, ...] â†’ class 2 (conf: 0.71)\nRun 5: [0.10, 0.03, 0.83, ...] â†’ class 2 (conf: 0.83)\n[ë¬¸ì œ] ê°™ì€ ì…ë ¥ì¸ë° í™•ë¥ ê°’ì´ ë§¤ë²ˆ ë‹¤ë¦„",
          "success_log": "=== ì¶”ë¡  ê²°ê³¼ (5íšŒ ì‹¤í–‰) ===\nRun 1: [0.08, 0.02, 0.87, ...] â†’ class 2 (conf: 0.87)\nRun 2: [0.08, 0.02, 0.87, ...] â†’ class 2 (conf: 0.87)\nRun 3: [0.08, 0.02, 0.87, ...] â†’ class 2 (conf: 0.87)\nRun 4: [0.08, 0.02, 0.87, ...] â†’ class 2 (conf: 0.87)\nRun 5: [0.08, 0.02, 0.87, ...] â†’ class 2 (conf: 0.87)\n[ì •ìƒ] ë™ì¼ ì…ë ¥ â†’ ë™ì¼ ì¶œë ¥",
          "hint": "ëª¨ë¸ì—ëŠ” í•™ìŠµ ëª¨ë“œì™€ ì¶”ë¡  ëª¨ë“œê°€ ìˆìŠµë‹ˆë‹¤. Dropoutê³¼ BatchNormì€ ì´ ë‘ ëª¨ë“œì—ì„œ ë‹¤ë¥´ê²Œ ë™ì‘í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "model.eval()"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Inference Mode Error",
            "description": "PyTorch ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ í•™ìŠµ ëª¨ë“œ(train mode)ë¡œ ìƒì„±ë©ë‹ˆë‹¤.\n\ní•™ìŠµ ëª¨ë“œì—ì„œ:\n- Dropout: ë‰´ëŸ°ì„ ëœë¤í•˜ê²Œ ë¹„í™œì„±í™” (ë§¤ forwardë§ˆë‹¤ ë‹¤ë¦„)\n- BatchNorm: í˜„ì¬ ë°°ì¹˜ì˜ í‰ê· /ë¶„ì‚° ì‚¬ìš©\n\nì¶”ë¡  ëª¨ë“œ(eval mode)ì—ì„œ:\n- Dropout: ëª¨ë“  ë‰´ëŸ° í™œì„±í™” (dropout ë¹„ìœ¨ë§Œí¼ ìŠ¤ì¼€ì¼ë§)\n- BatchNorm: í•™ìŠµ ì¤‘ ì €ì¥ëœ running_mean/running_var ì‚¬ìš©\n\nmodel.eval()ì„ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë©´ ì¶”ë¡  ì‹œì—ë„ Dropoutì´ ëœë¤í•˜ê²Œ ë™ì‘í•˜ì—¬ ê°™ì€ ì…ë ¥ì— ëŒ€í•´ ë‹¤ë¥¸ ì¶œë ¥ì´ ë‚˜ì˜µë‹ˆë‹¤.",
            "suggestion": "ì¶”ë¡  ì „ì— ë°˜ë“œì‹œ model.eval()ì„ í˜¸ì¶œí•˜ì„¸ìš”. í•™ìŠµì„ ë‹¤ì‹œ ì‹œì‘í•  ë•ŒëŠ” model.train()ì„ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: model.eval() ëˆ„ë½ì€ ì‹ ì… ê°œë°œìê°€ ê°€ì¥ ë§ì´ í•˜ëŠ” ì‹¤ìˆ˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì¶”ë¡  ì½”ë“œë¥¼ ì‘ì„±í•  ë•ŒëŠ” model.eval()ê³¼ torch.no_grad()ë¥¼ ì„¸íŠ¸ë¡œ ê¸°ì–µí•˜ì„¸ìš”."
        },
        {
          "step": 2,
          "title": "ì¶”ë¡ í•˜ë©´ í• ìˆ˜ë¡ ë©”ëª¨ë¦¬ê°€ í„°ì ¸ìš” (no_grad ëˆ„ë½)",
          "bug_type": "B",
          "bug_type_name": "Memory Leak",
          "file_name": "batch_inference.py",
          "buggy_code": "import torch\n\nmodel = Model()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\nmodel.cuda()\n\nresults = []\n\n# ëŒ€ëŸ‰ ë°°ì¹˜ ì¶”ë¡ \nfor batch in data_loader:\n    images = batch['image'].cuda()\n    \n    # ì¶”ë¡  ì‹¤í–‰\n    outputs = model(images)\n    predictions = torch.argmax(outputs, dim=1)\n    \n    results.extend(predictions.cpu().tolist())\n    \n    # ë§¤ 100ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ í™•ì¸\n    if len(results) % 100 == 0:\n        print(f\"Processed: {len(results)}, GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")",
          "correct_code": "import torch\n\nmodel = Model()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\nmodel.cuda()\n\nresults = []\n\n# ëŒ€ëŸ‰ ë°°ì¹˜ ì¶”ë¡ \nwith torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n    for batch in data_loader:\n        images = batch['image'].cuda()\n        \n        # ì¶”ë¡  ì‹¤í–‰\n        outputs = model(images)\n        predictions = torch.argmax(outputs, dim=1)\n        \n        results.extend(predictions.cpu().tolist())\n        \n        # ë§¤ 100ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ í™•ì¸\n        if len(results) % 100 == 0:\n            print(f\"Processed: {len(results)}, GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")",
          "error_log": "Processed: 100, GPU Memory: 2.15GB\nProcessed: 500, GPU Memory: 4.82GB\nProcessed: 1000, GPU Memory: 7.93GB\nProcessed: 1500, GPU Memory: 11.24GB\nRuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 12.00 GiB total capacity)",
          "success_log": "Processed: 100, GPU Memory: 1.23GB\nProcessed: 500, GPU Memory: 1.24GB\nProcessed: 1000, GPU Memory: 1.23GB\nProcessed: 5000, GPU Memory: 1.25GB\n[ì •ìƒ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¼ì •í•˜ê²Œ ìœ ì§€",
          "hint": "PyTorchëŠ” ì—­ì „íŒŒë¥¼ ìœ„í•´ forward ì—°ì‚°ì˜ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì¶”ë¡ ì—ì„œëŠ” ì´ê²Œ í•„ìš”í• ê¹Œìš”?",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "torch.no_grad()"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Memory Leak (Gradient Accumulation)",
            "description": "PyTorchëŠ” Autograd ì‹œìŠ¤í…œì„ í†µí•´ ìë™ ë¯¸ë¶„ì„ ì§€ì›í•©ë‹ˆë‹¤. ëª¨ë“  í…ì„œ ì—°ì‚°ì€ ì—°ì‚° ê·¸ë˜í”„(Computational Graph)ë¡œ ê¸°ë¡ë˜ë©°, backward() í˜¸ì¶œ ì‹œ ì´ ê·¸ë˜í”„ë¥¼ ì—­ìˆœíšŒí•˜ë©° ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n\në¬¸ì œëŠ” ì¶”ë¡  ì‹œì—ë„ ì´ ê·¸ë˜í”„ê°€ ê³„ì† ìƒì„±ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤:\n- ë§¤ forwardë§ˆë‹¤ ìƒˆë¡œìš´ ì—°ì‚° ê·¸ë˜í”„ ìƒì„±\n- ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ìš© ì¤‘ê°„ í…ì„œë“¤ì´ ë©”ëª¨ë¦¬ì— ëˆ„ì \n- ë°°ì¹˜ê°€ ìŒ“ì¼ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ â†’ OOM\n\ntorch.no_grad()ëŠ” ì—°ì‚° ê·¸ë˜í”„ ìƒì„± ìì²´ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•©ë‹ˆë‹¤.",
            "suggestion": "ì¶”ë¡  ì½”ë“œëŠ” ë°˜ë“œì‹œ with torch.no_grad(): ë¸”ë¡ ì•ˆì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”. ì¶”ê°€ë¡œ torch.inference_mode()ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ìµœì í™”ë©ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ëŒ€ê·œëª¨ ë°°ì¹˜ ì¶”ë¡ ì—ì„œ OOMì€ í”í•œ ë¬¸ì œì…ë‹ˆë‹¤. model.eval()ì€ ë ˆì´ì–´ ë™ì‘ì„ ë°”ê¾¸ê³ , torch.no_grad()ëŠ” ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•©ë‹ˆë‹¤. ë‘˜ì€ ë‹¤ë¥¸ ì—­í• ì´ë¯€ë¡œ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "CPUì—ì„œ ë§Œë“  í…ì„œê°€ GPU ëª¨ë¸ì„ ë§Œë‚˜ë©´? (Device Mismatch)",
          "bug_type": "C",
          "bug_type_name": "Device Error",
          "file_name": "multi_device.py",
          "buggy_code": "import torch\nimport torch.nn as nn\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        # ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n    \n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        return x + self.pe[:, :x.size(1), :]\n\nmodel = PositionalEncoding(d_model=512)\nmodel.cuda()\n\nx = torch.randn(32, 100, 512).cuda()\noutput = model(x)  # ì—ëŸ¬ ë°œìƒ!",
          "correct_code": "import torch\nimport torch.nn as nn\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        # ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))  # bufferë¡œ ë“±ë¡\n    \n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        return x + self.pe[:, :x.size(1), :]\n\nmodel = PositionalEncoding(d_model=512)\nmodel.cuda()\n\nx = torch.randn(32, 100, 512).cuda()\noutput = model(x)  # ì •ìƒ ë™ì‘!",
          "error_log": "Traceback (most recent call last):\n  File \"inference.py\", line 25, in <module>\n    output = model(x)\n  File \"model.py\", line 18, in forward\n    return x + self.pe[:, :x.size(1), :]\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
          "success_log": "Model device: cuda:0\nInput device: cuda:0\nPositional encoding device: cuda:0\nOutput shape: torch.Size([32, 100, 512])\n[ì •ìƒ] ëª¨ë“  í…ì„œê°€ ê°™ì€ deviceì— ìˆìŒ",
          "hint": "model.cuda()ë¥¼ í˜¸ì¶œí•˜ë©´ nn.Parameterë§Œ GPUë¡œ ì´ë™í•©ë‹ˆë‹¤. ë‹¨ìˆœ í…ì„œ ì†ì„±(self.pe = ...)ì€ ì´ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "register_buffer"
            ],
            "forbidden": [
              "self.pe = pe"
            ]
          },
          "error_info": {
            "type": "Device Mismatch Error",
            "description": "PyTorchì—ì„œ model.cuda() ë˜ëŠ” model.to(device)ë¥¼ í˜¸ì¶œí•˜ë©´ ì´ë™í•˜ëŠ” ê²ƒë“¤:\n- nn.Parameterë¡œ ë“±ë¡ëœ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°\n- register_buffer()ë¡œ ë“±ë¡ëœ ë²„í¼\n\nì´ë™í•˜ì§€ ì•ŠëŠ” ê²ƒ:\n- ë‹¨ìˆœ í…ì„œ ì†ì„± (self.tensor = torch.zeros(...))\n- ì§€ì—­ ë³€ìˆ˜ë¡œ ìƒì„±ëœ í…ì„œ\n\nself.pe = pe.unsqueeze(0)ì€ ë‹¨ìˆœ ì†ì„± í• ë‹¹ì´ë¯€ë¡œ model.cuda() ì‹œ CPUì— ë‚¨ì•„ìˆê²Œ ë©ë‹ˆë‹¤. forwardì—ì„œ GPU í…ì„œ xì™€ ì—°ì‚°í•˜ë ¤ í•  ë•Œ device ë¶ˆì¼ì¹˜ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.",
            "suggestion": "í•™ìŠµë˜ì§€ ì•Šì§€ë§Œ ëª¨ë¸ê³¼ í•¨ê»˜ ì´ë™í•´ì•¼ í•˜ëŠ” í…ì„œëŠ” self.register_buffer('name', tensor)ë¡œ ë“±ë¡í•˜ì„¸ìš”. ì´ë ‡ê²Œ í•˜ë©´ device ì´ë™ê³¼ state_dict ì €ì¥ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: Positional Encoding, ë§ˆìŠ¤í¬ í…ì„œ ë“± í•™ìŠµë˜ì§€ ì•ŠëŠ” ìƒìˆ˜ í…ì„œë¥¼ ë‹¤ë£° ë•Œ register_bufferë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì•ˆ ê·¸ëŸ¬ë©´ GPU/CPU í˜¼ìš© ë²„ê·¸ë‚˜ ëª¨ë¸ ì €ì¥/ë¡œë“œ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤."
        }
      ]
    },
    {
      "id": "P2",
      "project_title": "í•™ìŠµ ë£¨í”„ ì¹˜ëª…ì  ë²„ê·¸",
      "scenario": "ëª¨ë¸ í•™ìŠµ ì½”ë“œë¥¼ ì‘ì„±í–ˆëŠ”ë°, lossê°€ ì´ìƒí•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤. ê°ì†Œí•´ì•¼ í•  lossê°€ ì¦ê°€í•˜ê±°ë‚˜, í•™ìŠµì´ ì „í˜€ ì•ˆ ë˜ê±°ë‚˜, ê°‘ìê¸° NaNì´ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤.",
      "difficulty": 2,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "Lossê°€ ë°°ì¹˜ë§ˆë‹¤ í­ë°œí•´ìš” (zero_grad ìœ„ì¹˜ ì˜¤ë¥˜)",
          "bug_type": "A",
          "bug_type_name": "Gradient Bug",
          "file_name": "train_loop.py",
          "buggy_code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = SimpleNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(10):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        output = model(data)\n        loss = criterion(output, target)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()  # â† ìœ„ì¹˜ê°€ ì˜ëª»ë¨\n        \n        print(f\"Epoch {epoch}, Batch {batch_idx}: Loss = {loss.item():.4f}\")",
          "correct_code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = SimpleNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(10):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()  # â† ë°˜ë“œì‹œ backward ì „ì—!\n        \n        output = model(data)\n        loss = criterion(output, target)\n        \n        loss.backward()\n        optimizer.step()\n        \n        print(f\"Epoch {epoch}, Batch {batch_idx}: Loss = {loss.item():.4f}\")",
          "error_log": "Epoch 0, Batch 0: Loss = 2.3025\nEpoch 0, Batch 1: Loss = 4.7891\nEpoch 0, Batch 2: Loss = 8.2341\nEpoch 0, Batch 10: Loss = 156.2847\nEpoch 0, Batch 50: Loss = inf\nEpoch 1, Batch 0: Loss = nan",
          "success_log": "Epoch 0, Batch 0: Loss = 2.3025\nEpoch 0, Batch 1: Loss = 2.2891\nEpoch 0, Batch 10: Loss = 1.8542\nEpoch 0, Batch 50: Loss = 0.9823\nEpoch 1, Batch 0: Loss = 0.7654\n[ì •ìƒ] Lossê°€ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ",
          "hint": "zero_grad()ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. backward() ì „ì— í˜¸ì¶œí•´ì•¼ ì´ì „ ë°°ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ëˆ„ì ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "regex",
            "value": "zero_grad\\(\\)[\\s\\S]*?forward|zero_grad\\(\\)[\\s\\S]*?output\\s*=\\s*model",
            "flags": ""
          },
          "error_info": {
            "type": "Gradient Accumulation Bug",
            "description": "PyTorchëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ìœ ì—°ì„±ì„ ìœ„í•´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ëˆ„ì í•©ë‹ˆë‹¤. ì´ëŠ” ëŒ€í˜• ëª¨ë¸ì„ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ  í•™ìŠµí•  ë•Œ(Gradient Accumulation) ìœ ìš©í•˜ì§€ë§Œ, ì˜ë„ì¹˜ ì•Šì€ ëˆ„ì ì€ ë¬¸ì œë¥¼ ì¼ìœ¼í‚µë‹ˆë‹¤.\n\nì˜ëª»ëœ ìˆœì„œ (zero_gradê°€ step í›„):\n1. backward() â†’ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° + ì´ì „ ê·¸ë˜ë””ì–¸íŠ¸ì— ëˆ„ì \n2. step() â†’ ëˆ„ì ëœ (í­ë°œí•œ) ê·¸ë˜ë””ì–¸íŠ¸ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n3. zero_grad() â†’ ì´ˆê¸°í™” (ì´ë¯¸ ëŠ¦ìŒ)\n\në°°ì¹˜ê°€ ìŒ“ì¼ìˆ˜ë¡ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì ¸ loss í­ë°œ â†’ NaN ë°œìƒ",
            "suggestion": "í•™ìŠµ ë£¨í”„ì—ì„œ zero_grad()ëŠ” ë°˜ë“œì‹œ backward() ì „ì— í˜¸ì¶œí•˜ì„¸ìš”. í‘œì¤€ ìˆœì„œ: optimizer.zero_grad() â†’ output = model(x) â†’ loss.backward() â†’ optimizer.step()"
          },
          "coaching": "ğŸ¯ í˜„ì—…: í•™ìŠµ ì½”ë“œì˜ í‘œì¤€ ìˆœì„œë¥¼ ì™¸ì›Œë‘ì„¸ìš”. zero_grad â†’ forward â†’ loss â†’ backward â†’ step. ì´ ìˆœì„œê°€ í‹€ë¦¬ë©´ loss í­ë°œ, NaN, í•™ìŠµ ì•ˆ ë¨ ë“±ì˜ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "LossëŠ” ì¤„ì–´ë“œëŠ”ë° ëª¨ë¸ì´ ì•ˆ ë°”ë€Œì–´ìš” (detach ì‹¤ìˆ˜)",
          "bug_type": "B",
          "bug_type_name": "Graph Break",
          "file_name": "custom_loss.py",
          "buggy_code": "import torch\nimport torch.nn as nn\n\ndef custom_loss(pred, target, model):\n    # L2 regularization ì¶”ê°€\n    mse_loss = nn.MSELoss()(pred, target)\n    \n    # ì •ê·œí™” í•­ ê³„ì‚°\n    l2_reg = 0\n    for param in model.parameters():\n        l2_reg += torch.norm(param)\n    \n    total_loss = mse_loss.detach() + 0.01 * l2_reg\n    return total_loss\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    pred = model(x)\n    loss = custom_loss(pred, y, model)\n    loss.backward()\n    optimizer.step()\n    print(f\"Loss: {loss.item():.4f}\")",
          "correct_code": "import torch\nimport torch.nn as nn\n\ndef custom_loss(pred, target, model):\n    # L2 regularization ì¶”ê°€\n    mse_loss = nn.MSELoss()(pred, target)\n    \n    # ì •ê·œí™” í•­ ê³„ì‚°\n    l2_reg = 0\n    for param in model.parameters():\n        l2_reg += torch.norm(param)\n    \n    total_loss = mse_loss + 0.01 * l2_reg  # detach ì œê±°!\n    return total_loss\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    pred = model(x)\n    loss = custom_loss(pred, y, model)\n    loss.backward()\n    optimizer.step()\n    print(f\"Loss: {loss.item():.4f}\")",
          "error_log": "Epoch 0: Loss = 2.3456\nEpoch 10: Loss = 0.0234  (ì¤„ì–´ë“œëŠ” ê²ƒì²˜ëŸ¼ ë³´ì„)\nEpoch 50: Loss = 0.0012\n\n[ê²€ì¦]\nTrain Accuracy: 12.3% (ë³€í™” ì—†ìŒ)\nTest Accuracy: 11.8% (ë³€í™” ì—†ìŒ)\n[ë¬¸ì œ] LossëŠ” ì¤„ì–´ë“¤ì§€ë§Œ ëª¨ë¸ ì„±ëŠ¥ ë¶ˆë³€",
          "success_log": "Epoch 0: Loss = 2.3456\nEpoch 10: Loss = 0.4521\nEpoch 50: Loss = 0.0234\n\n[ê²€ì¦]\nTrain Accuracy: 98.2%\nTest Accuracy: 94.5%\n[ì •ìƒ] Loss ê°ì†Œì™€ í•¨ê»˜ ì„±ëŠ¥ë„ í–¥ìƒ",
          "hint": ".detach()ëŠ” í…ì„œë¥¼ ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ë¶„ë¦¬í•©ë‹ˆë‹¤. lossì— detachëœ í…ì„œê°€ í¬í•¨ë˜ë©´ ê·¸ ë¶€ë¶„ì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ íë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "mse_loss + 0.01"
            ],
            "forbidden": [
              "mse_loss.detach()"
            ]
          },
          "error_info": {
            "type": "Computational Graph Disconnection",
            "description": "PyTorchì˜ ìë™ ë¯¸ë¶„ì€ ì—°ì‚° ê·¸ë˜í”„ë¥¼ í†µí•´ ì‘ë™í•©ë‹ˆë‹¤. .detach()ë¥¼ í˜¸ì¶œí•˜ë©´:\n\n1. í•´ë‹¹ í…ì„œê°€ ê·¸ë˜í”„ì—ì„œ ë¶„ë¦¬ë¨\n2. ë¶„ë¦¬ëœ í…ì„œë¥¼ ì‚¬ìš©í•œ ì´í›„ ì—°ì‚°ì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ê°€ íë¥´ì§€ ì•ŠìŒ\n\nì´ ì½”ë“œì—ì„œ:\n- mse_loss.detach() â†’ MSE ì†ì‹¤ê³¼ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê°„ì˜ ì—°ê²° ëŠê¹€\n- l2_regë§Œ ê·¸ë˜í”„ì— ì—°ê²°ë¨\n- backward() ì‹œ MSEì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ = 0\n- ê²°ê³¼: ì •ê·œí™” í•­ë§Œ ìµœì†Œí™”ë˜ê³ , ì‹¤ì œ ì˜ˆì¸¡ ì„±ëŠ¥ì€ ê°œì„  ì•ˆ ë¨",
            "suggestion": "detach()ëŠ” íŠ¹ì • í…ì„œì˜ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ì„ ë§‰ê³  ì‹¶ì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”. loss ê³„ì‚°ì—ì„œëŠ” ëŒ€ë¶€ë¶„ detachê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: 'Lossê°€ ì¤„ì–´ë“œëŠ”ë° ì„±ëŠ¥ì´ ì•ˆ ì˜¤ë¥¸ë‹¤'ë©´ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ì„ í™•ì¸í•˜ì„¸ìš”. print(loss.grad_fn)ìœ¼ë¡œ ì—°ì‚° ê·¸ë˜í”„ ì—°ê²° ìƒíƒœë¥¼ ë””ë²„ê¹…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¼ë”ë‹ˆ ì˜¤íˆë ¤ ë§í–ˆì–´ìš” (ìŠ¤ì¼€ì¤„ëŸ¬ ìˆœì„œ ì˜¤ë¥˜)",
          "bug_type": "C",
          "bug_type_name": "Scheduler Bug",
          "file_name": "scheduler.py",
          "buggy_code": "import torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\nmodel = MyModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nfor epoch in range(100):\n    for batch in train_loader:\n        scheduler.step()  # â† ìœ„ì¹˜ê°€ ì˜ëª»ë¨\n        \n        optimizer.zero_grad()\n        output = model(batch['x'])\n        loss = criterion(output, batch['y'])\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch {epoch}: LR = {scheduler.get_last_lr()[0]:.6f}\")",
          "correct_code": "import torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\nmodel = MyModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nfor epoch in range(100):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        output = model(batch['x'])\n        loss = criterion(output, batch['y'])\n        loss.backward()\n        optimizer.step()\n    \n    scheduler.step()  # â† ì—í¬í¬ ëì—ì„œ í•œ ë²ˆë§Œ í˜¸ì¶œ\n    print(f\"Epoch {epoch}: LR = {scheduler.get_last_lr()[0]:.6f}\")",
          "error_log": "Epoch 0: LR = 0.000100 (ì²« ë°°ì¹˜ í›„ ì´ë¯¸ ê¸‰ê°)\nEpoch 1: LR = 0.000001\nEpoch 2: LR = 0.000000\nLossê°€ ìˆ˜ë ´í•˜ì§€ ì•Šê³  ì§„ë™",
          "success_log": "Epoch 0: LR = 0.001000\nEpoch 9: LR = 0.001000\nEpoch 10: LR = 0.000100\nEpoch 19: LR = 0.000100\nEpoch 20: LR = 0.000010\n[ì •ìƒ] 10ì—í¬í¬ë§ˆë‹¤ LRì´ 1/10ë¡œ ê°ì†Œ",
          "hint": "StepLR(step_size=10)ì€ '10ë²ˆì˜ step() í˜¸ì¶œ í›„' LRì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œí•˜ë©´ step_sizeì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.",
          "solution_check": {
            "type": "regex",
            "value": "for batch[\\s\\S]*?optimizer\\.step\\(\\)[\\s\\S]*?\\n\\s*scheduler\\.step\\(\\)",
            "flags": ""
          },
          "error_info": {
            "type": "LR Scheduler Misuse",
            "description": "Learning Rate SchedulerëŠ” í•™ìŠµë¥ ì„ ë™ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í˜¸ì¶œ ìœ„ì¹˜ê°€ ì˜ëª»ë˜ë©´:\n\n**ì˜ëª»ëœ ì‚¬ìš© (ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œ):**\n- StepLR(step_size=10)ì´ 10ë°°ì¹˜ í›„ LR ê°ì†Œ\n- í•œ ì—í¬í¬ì— 100ë°°ì¹˜ë¼ë©´, 10ë°°ì¹˜ë§Œì— LR = 0.0001\n- 20ë°°ì¹˜ í›„ LR = 0.00001... ê¸‰ê²©íˆ 0ì— ìˆ˜ë ´\n\n**ì˜¬ë°”ë¥¸ ì‚¬ìš© (ì—í¬í¬ë§ˆë‹¤ í˜¸ì¶œ):**\n- 10ì—í¬í¬ í›„ LR ê°ì†Œ\n- ì˜ë„í•œ ëŒ€ë¡œ ì ì§„ì  í•™ìŠµë¥  ê°ì†Œ\n\në˜í•œ PyTorch 1.1 ì´í›„ë¶€í„°ëŠ” scheduler.step()ì„ optimizer.step() ì´í›„ì— í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.",
            "suggestion": "ëŒ€ë¶€ë¶„ì˜ ìŠ¤ì¼€ì¤„ëŸ¬(StepLR, ExponentialLR ë“±)ëŠ” ì—í¬í¬ ëì—ì„œ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ì„¸ìš”. ë°°ì¹˜ ë‹¨ìœ„ ìŠ¤ì¼€ì¤„ëŸ¬(OneCycleLR ë“±)ëŠ” ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œí•©ë‹ˆë‹¤. ìŠ¤ì¼€ì¤„ëŸ¬ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ìŠ¤ì¼€ì¤„ëŸ¬ ë²„ê·¸ëŠ” ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. get_last_lr()ë¡œ í˜„ì¬ í•™ìŠµë¥ ì„ ë¡œê¹…í•˜ê³ , ì˜ë„í•œ ìŠ¤ì¼€ì¤„ê³¼ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. íŠ¹íˆ ì¬í•™ìŠµ ì‹œ scheduler.last_epoch ì´ˆê¸°í™”ì— ì£¼ì˜í•˜ì„¸ìš”."
        }
      ]
    },
    {
      "id": "P3",
      "project_title": "ë°ì´í„° ë¡œë” & ì „ì²˜ë¦¬ í•¨ì •",
      "scenario": "ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í–ˆëŠ”ë°, í•™ìŠµ ì†ë„ê°€ ë„ˆë¬´ ëŠë¦¬ê±°ë‚˜, ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜, ì¬í˜„ì´ ì•ˆ ë˜ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.",
      "difficulty": 2,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "GPUëŠ” ë†€ê³  ìˆê³  CPUë§Œ ì¼í•´ìš” (DataLoader ë³‘ëª©)",
          "bug_type": "A",
          "bug_type_name": "I/O Bottleneck",
          "file_name": "dataloader.py",
          "buggy_code": "from torch.utils.data import DataLoader\n\n# ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (50,000ì¥, ê° 224x224)\ntrain_dataset = ImageDataset(root='./data', transform=transform)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True\n)\n\n# Training loop\nfor epoch in range(10):\n    for batch in train_loader:  # ì—¬ê¸°ì„œ ëŒ€ë¶€ë¶„ì˜ ì‹œê°„ ì†Œìš”\n        images = batch['image'].cuda()\n        labels = batch['label'].cuda()\n        # ... í•™ìŠµ ì½”ë“œ",
          "correct_code": "from torch.utils.data import DataLoader\n\n# ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (50,000ì¥, ê° 224x224)\ntrain_dataset = ImageDataset(root='./data', transform=transform)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    num_workers=4,      # ë³‘ë ¬ ë°ì´í„° ë¡œë”©\n    pin_memory=True     # GPU ì „ì†¡ ìµœì í™”\n)\n\n# Training loop\nfor epoch in range(10):\n    for batch in train_loader:\n        images = batch['image'].cuda(non_blocking=True)\n        labels = batch['label'].cuda(non_blocking=True)\n        # ... í•™ìŠµ ì½”ë“œ",
          "error_log": "=== ë°°ì¹˜ ì‹œê°„ ë¶„ì„ ===\nData Loading: 4.52s (90.4%)\nForward Pass: 0.31s (6.2%)\nBackward Pass: 0.17s (3.4%)\nTotal: 5.00s\n\nGPU Utilization: 8%\nCPU Utilization: 12%\n[ë³‘ëª©] ë°ì´í„° ë¡œë”©ì´ ì „ì²´ ì‹œê°„ì˜ 90% ì°¨ì§€",
          "success_log": "=== ë°°ì¹˜ ì‹œê°„ ë¶„ì„ ===\nData Loading: 0.08s (14.3%)\nForward Pass: 0.31s (55.4%)\nBackward Pass: 0.17s (30.3%)\nTotal: 0.56s\n\nGPU Utilization: 92%\nCPU Utilization: 45%\n[ì •ìƒ] GPUê°€ ì¶©ë¶„íˆ í™œìš©ë¨, 8.9ë°° ì†ë„ í–¥ìƒ",
          "hint": "DataLoaderì˜ num_workersì™€ pin_memory ì˜µì…˜ì„ í™•ì¸í•˜ì„¸ìš”. ê¸°ë³¸ê°’ì€ ë³‘ë ¬ ë¡œë”©ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "num_workers=",
              "pin_memory=True"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Data Loading Bottleneck",
            "description": "DataLoaderì˜ ê¸°ë³¸ ì„¤ì •(num_workers=0)ì€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤:\n\n1. GPUê°€ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ\n2. ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ë‹¤ìŒ ë°°ì¹˜ ë¡œë”© ì‹œì‘\n3. GPU ëŒ€ê¸°... (ì´ ì‹œê°„ì´ ë‚­ë¹„)\n4. ë¡œë”© ì™„ë£Œ í›„ GPUì— ì „ì†¡\n5. GPU ì²˜ë¦¬ ì‹œì‘\n\nnum_workers > 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´:\n1. ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ë“¤ì´ ë¯¸ë¦¬ ë‹¤ìŒ ë°°ì¹˜ë“¤ì„ ì¤€ë¹„\n2. GPU ì²˜ë¦¬ ì¤‘ì— ë‹¤ìŒ ë°°ì¹˜ê°€ ì´ë¯¸ ì¤€ë¹„ë¨\n3. GPU ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™”\n\npin_memory=TrueëŠ” CPUâ†’GPU ì „ì†¡ì„ ë¹„ë™ê¸°ë¡œ ë§Œë“¤ì–´ ì¶”ê°€ ìµœì í™”í•©ë‹ˆë‹¤.",
            "suggestion": "num_workersëŠ” CPU ì½”ì–´ ìˆ˜ì˜ 2~4ë°°ë¡œ ì‹œì‘í•˜ì„¸ìš” (ë„ˆë¬´ ë†’ìœ¼ë©´ ì˜¤ë²„í—¤ë“œ). GPU ì‚¬ìš© ì‹œ pin_memory=True, .cuda(non_blocking=True)ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: nvidia-smië¡œ GPU ì‚¬ìš©ë¥ ì„ í™•ì¸í•˜ì„¸ìš”. 30% ë¯¸ë§Œì´ë©´ ë°ì´í„° ë¡œë”© ë³‘ëª©ì„ ì˜ì‹¬í•˜ì„¸ìš”. num_workers ìµœì ê°’ì€ ì‹¤í—˜ìœ¼ë¡œ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "ë˜‘ê°™ì´ í•™ìŠµí–ˆëŠ”ë° ê²°ê³¼ê°€ ë§¤ë²ˆ ë‹¬ë¼ìš” (ì‹œë“œ ë¶ˆì™„ì „)",
          "bug_type": "B",
          "bug_type_name": "Reproducibility",
          "file_name": "seed_fix.py",
          "buggy_code": "import random\nimport numpy as np\nimport torch\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n\nset_seed(42)\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters())\n\n# DataLoader\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Training\nfor epoch in range(10):\n    for batch in train_loader:\n        # ... í•™ìŠµ ì½”ë“œ",
          "correct_code": "import random\nimport numpy as np\nimport torch\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # ë©€í‹° GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters())\n\n# DataLoaderì—ë„ ì‹œë“œ ì ìš©\ng = torch.Generator()\ng.manual_seed(42)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True, generator=g)\n\n# Training\nfor epoch in range(10):\n    for batch in train_loader:\n        # ... í•™ìŠµ ì½”ë“œ",
          "error_log": "=== Run 1 ===\nFinal Test Accuracy: 94.23%\n\n=== Run 2 (ë™ì¼ ì½”ë“œ) ===\nFinal Test Accuracy: 91.87%\n\n=== Run 3 (ë™ì¼ ì½”ë“œ) ===\nFinal Test Accuracy: 93.51%\n\n[ë¬¸ì œ] ì‹¤í—˜ ì¬í˜„ ë¶ˆê°€ëŠ¥",
          "success_log": "=== Run 1 ===\nFinal Test Accuracy: 93.42%\n\n=== Run 2 (ë™ì¼ ì½”ë“œ) ===\nFinal Test Accuracy: 93.42%\n\n=== Run 3 (ë™ì¼ ì½”ë“œ) ===\nFinal Test Accuracy: 93.42%\n\n[ì •ìƒ] ì™„ë²½í•œ ì¬í˜„ì„±",
          "hint": "randomê³¼ numpy ì‹œë“œë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. PyTorch, CUDA, cuDNN, DataLoader ëª¨ë‘ ì‹œë“œ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "torch.manual_seed(",
              "torch.cuda.manual_seed("
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Incomplete Seed Setting",
            "description": "ë”¥ëŸ¬ë‹ í•™ìŠµì—ì„œ ë‚œìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê³³:\n\n1. **Python random**: shuffle, random.choice ë“±\n2. **NumPy**: ë°ì´í„° ì „ì²˜ë¦¬, augmentation\n3. **PyTorch CPU**: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”, Dropout\n4. **PyTorch CUDA**: GPU ì—°ì‚°\n5. **cuDNN**: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ\n6. **DataLoader**: ë°ì´í„° ì…”í”Œë§\n\nëª¨ë“  ì†ŒìŠ¤ì— ì‹œë“œë¥¼ ì„¤ì •í•´ì•¼ ì™„ë²½í•œ ì¬í˜„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nì£¼ì˜: cudnn.deterministic=TrueëŠ” ì„±ëŠ¥ì„ ì•½ê°„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "suggestion": "ì¬í˜„ì„±ì´ ì¤‘ìš”í•œ ì‹¤í—˜ì—ì„œëŠ” ëª¨ë“  ì‹œë“œë¥¼ ê³ ì •í•˜ì„¸ìš”. ë‹¨, cudnn.deterministicì€ ì„±ëŠ¥ ì˜í–¥ì´ ìˆìœ¼ë¯€ë¡œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë„ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ì‹¤í—˜ ì¬í˜„ì„±ì€ ì—°êµ¬ì™€ ë””ë²„ê¹…ì˜ ê¸°ë³¸ì…ë‹ˆë‹¤. ì‹œë“œ ê³ ì • í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ ì¬ì‚¬ìš©í•˜ì„¸ìš”. ë˜í•œ í™˜ê²½ ì •ë³´(PyTorch ë²„ì „, CUDA ë²„ì „)ë„ í•¨ê»˜ ê¸°ë¡í•˜ì„¸ìš”."
        },
        {
          "step": 3,
          "title": "Validationì—ì„œ ë³¸ ë°ì´í„°ê°€ Trainì—ë„ ìˆì–´ìš” (Data Leakage)",
          "bug_type": "C",
          "bug_type_name": "Data Leakage",
          "file_name": "data_split.py",
          "buggy_code": "from sklearn.model_selection import train_test_split\nimport albumentations as A\n\n# ì›ë³¸ ë°ì´í„°: 1000ì¥\noriginal_images = load_images('./data')\n\n# 1. Augmentation ì ìš© (5ë°° ì¦ê°•)\naugmented_images = []\naugment = A.Compose([A.HorizontalFlip(), A.RandomBrightness()])\n\nfor img in original_images:\n    augmented_images.append(img)  # ì›ë³¸\n    for _ in range(4):  # 4ê°œ ì¶”ê°€\n        augmented_images.append(augment(image=img)['image'])\n\n# 2. ì¦ê°•ëœ ì „ì²´ ë°ì´í„°ì—ì„œ ë¶„í•  (5000ì¥)\nX_train, X_val = train_test_split(augmented_images, test_size=0.2)\n\n# ê²°ê³¼: ê°™ì€ ì›ë³¸ì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ trainê³¼ valì— ì„ì„!",
          "correct_code": "from sklearn.model_selection import train_test_split\nimport albumentations as A\n\n# ì›ë³¸ ë°ì´í„°: 1000ì¥\noriginal_images = load_images('./data')\n\n# 1. ë¨¼ì € ì›ë³¸ ë°ì´í„°ë¥¼ ë¶„í• \nX_train_orig, X_val_orig = train_test_split(original_images, test_size=0.2)\n\n# 2. Train ë°ì´í„°ì—ë§Œ Augmentation ì ìš©\naugment = A.Compose([A.HorizontalFlip(), A.RandomBrightness()])\n\nX_train = []\nfor img in X_train_orig:\n    X_train.append(img)  # ì›ë³¸\n    for _ in range(4):  # 4ê°œ ì¶”ê°€\n        X_train.append(augment(image=img)['image'])\n\n# Validationì€ ì›ë³¸ ê·¸ëŒ€ë¡œ (ì¦ê°• ì—†ìŒ)\nX_val = X_val_orig\n\n# ê²°ê³¼: trainê³¼ valì´ ì™„ì „íˆ ë…ë¦½ì ",
          "error_log": "=== í•™ìŠµ ê²°ê³¼ ===\nTrain Accuracy: 98.5%\nValidation Accuracy: 97.8%\n\n=== ì‹¤ì œ ì„œë¹„ìŠ¤ ===\nTest Accuracy: 62.3%\n\n[ë¬¸ì œ] Valê³¼ Test ì„±ëŠ¥ ì°¨ì´ê°€ 35%p ì´ìƒ!\n[ì›ì¸ ë¶„ì„] ê°™ì€ ì›ë³¸ì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ Train/Valì— ë¶„ì‚°ë¨",
          "success_log": "=== í•™ìŠµ ê²°ê³¼ ===\nTrain Accuracy: 95.2%\nValidation Accuracy: 88.4%\n\n=== ì‹¤ì œ ì„œë¹„ìŠ¤ ===\nTest Accuracy: 87.1%\n\n[ì •ìƒ] Valê³¼ Test ì„±ëŠ¥ì´ ìœ ì‚¬\n[ë¶„ì„] Train/Valì´ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ë¶„ë¦¬ë¨",
          "hint": "Augmentationì€ ê°™ì€ ì›ë³¸ì—ì„œ íŒŒìƒëœ ì´ë¯¸ì§€ë“¤ì„ ë§Œë“­ë‹ˆë‹¤. ë¶„í• ì€ augmentation ì „ì— ì›ë³¸ ë°ì´í„°ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "regex",
            "value": "train_test_split\\(original|train_test_split.*\\)\\s*[\\s\\S]*?augment",
            "flags": ""
          },
          "error_info": {
            "type": "Augmentation Data Leakage",
            "description": "Data Augmentation í›„ ë¶„í• í•˜ë©´ ë°œìƒí•˜ëŠ” ë¬¸ì œ:\n\n**ì›ë³¸ ì´ë¯¸ì§€ A**\n- ì¦ê°• â†’ A, A_flip, A_bright, A_rotate, A_crop\n- ì¦ê°• í›„ ë¶„í•  â†’ A, A_flipì€ Train, A_brightëŠ” Valë¡œ...\n\n**ê²°ê³¼:**\n- ëª¨ë¸ì´ Trainì—ì„œ Aë¥¼ í•™ìŠµ\n- Valì˜ A_brightë„ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§\n- Valì—ì„œ ë†’ì€ ì„±ëŠ¥ (í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì™¸ìš´ ê²ƒ)\n- Testì—ì„œ ì™„ì „íˆ ìƒˆë¡œìš´ ì´ë¯¸ì§€ â†’ ì„±ëŠ¥ ê¸‰ë½\n\nì´ê²ƒì´ 'Validation ì„±ëŠ¥ì€ ì¢‹ì€ë° Test ì„±ëŠ¥ì´ ë‚®ì€' ì „í˜•ì ì¸ ì›ì¸ì…ë‹ˆë‹¤.",
            "suggestion": "Train/Val/Test ë¶„í• ì€ í•­ìƒ Augmentation ì „ì—, ì›ë³¸ ë°ì´í„°ë¡œ ìˆ˜í–‰í•˜ì„¸ìš”. ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì‹œê°„ ìˆœì„œë¡œ ë¶„í• í•˜ê³ , ê·¸ë£¹(í™˜ì, ì‚¬ìš©ì ë“±)ì´ ìˆìœ¼ë©´ ê·¸ë£¹ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: Data LeakageëŠ” ëª¨ë¸ì´ 'ë„ˆë¬´ ì˜ ë‚˜ì˜¬ ë•Œ' ê°€ì¥ ë¨¼ì € ì˜ì‹¬í•´ì•¼ í•©ë‹ˆë‹¤. Val ì„±ëŠ¥ê³¼ ì‹¤ì„œë¹„ìŠ¤ ì„±ëŠ¥ ì°¨ì´ê°€ í¬ë‹¤ë©´ ë°ì´í„° ë¶„í•  ê³¼ì •ì„ ì ê²€í•˜ì„¸ìš”."
        }
      ]
    },
    {
      "id": "P4",
      "project_title": "ëª¨ë¸ ì €ì¥ & ë°°í¬ ì‹¤ìˆ˜",
      "scenario": "í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë‚˜ì¤‘ì— ë‹¤ì‹œ ë¶ˆëŸ¬ì™”ëŠ”ë°, ì„±ëŠ¥ì´ ë‹¤ë¥´ê±°ë‚˜ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. ë˜ëŠ” ë‹¤ë¥¸ í™˜ê²½ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
      "difficulty": 3,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "ì €ì¥í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™”ë”ë‹ˆ ì„±ëŠ¥ì´ ë§ê°€ì¡Œì–´ìš” (state_dict í˜¼ë™)",
          "bug_type": "A",
          "bug_type_name": "Save/Load Error",
          "file_name": "save_model.py",
          "buggy_code": "import torch\n\n# í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥\nmodel = MyModel()\n# ... í•™ìŠµ ...\nprint(f\"Before save: Accuracy = {evaluate(model)}\")\n\n# ëª¨ë¸ ì €ì¥\ntorch.save(model.state_dict(), 'model.pth')\n\n# ë‚˜ì¤‘ì— ë¡œë“œ\nloaded_model = torch.load('model.pth')  # ì˜ëª»ëœ ë¡œë“œ ë°©ë²•\nloaded_model.eval()\nprint(f\"After load: Accuracy = {evaluate(loaded_model)}\")",
          "correct_code": "import torch\n\n# í•™ìŠµ ì™„ë£Œ í›„ ì €ì¥\nmodel = MyModel()\n# ... í•™ìŠµ ...\nprint(f\"Before save: Accuracy = {evaluate(model)}\")\n\n# ëª¨ë¸ ì €ì¥\ntorch.save(model.state_dict(), 'model.pth')\n\n# ì˜¬ë°”ë¥¸ ë¡œë“œ ë°©ë²•\nloaded_model = MyModel()  # ë¨¼ì € ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\nloaded_model.load_state_dict(torch.load('model.pth'))  # state_dict ë¡œë“œ\nloaded_model.eval()\nprint(f\"After load: Accuracy = {evaluate(loaded_model)}\")",
          "error_log": "Before save: Accuracy = 94.52%\n\n# ë¡œë“œ ì‹œë„\n>>> loaded_model = torch.load('model.pth')\n>>> type(loaded_model)\n<class 'collections.OrderedDict'>  # ëª¨ë¸ì´ ì•„ë‹ˆë¼ ë”•ì…”ë„ˆë¦¬!\n\n>>> loaded_model.eval()\nAttributeError: 'OrderedDict' object has no attribute 'eval'",
          "success_log": "Before save: Accuracy = 94.52%\n\n# ì˜¬ë°”ë¥¸ ë¡œë“œ\n>>> loaded_model = MyModel()\n>>> loaded_model.load_state_dict(torch.load('model.pth'))\n<All keys matched successfully>\n>>> loaded_model.eval()\n>>> evaluate(loaded_model)\nAfter load: Accuracy = 94.52%",
          "hint": "torch.save(model.state_dict(), ...)ë¡œ ì €ì¥í–ˆë‹¤ë©´, ë¡œë“œí•  ë•Œë„ state_dict ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "load_state_dict(",
              "MyModel()"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "State Dict Confusion",
            "description": "PyTorch ëª¨ë¸ ì €ì¥ ë°©ì‹:\n\n**1. state_dict ì €ì¥ (ê¶Œì¥):**\n- torch.save(model.state_dict(), 'model.pth')\n- ì €ì¥: íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜, ë°”ì´ì–´ìŠ¤)ë§Œ\n- ë¡œë“œ: model = MyModel(); model.load_state_dict(torch.load(...))\n\n**2. ì „ì²´ ëª¨ë¸ ì €ì¥:**\n- torch.save(model, 'model.pth')\n- ì €ì¥: ëª¨ë¸ êµ¬ì¡° + íŒŒë¼ë¯¸í„°\n- ë¡œë“œ: model = torch.load('model.pth')\n- ë‹¨ì : ì €ì¥ ì‹œì ì˜ ì½”ë“œ ê²½ë¡œì— ì˜ì¡´\n\në¬¸ì œ ì½”ë“œëŠ” state_dictë¥¼ ì €ì¥í•˜ê³  ì „ì²´ ëª¨ë¸ ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì„œ ë”•ì…”ë„ˆë¦¬ ê°ì²´ê°€ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "suggestion": "state_dict ë°©ì‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì½”ë“œ ê²½ë¡œ ì˜ì¡´ì„±ì´ ì—†ê³ , ë‹¤ë¥¸ í™˜ê²½ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤. ì €ì¥/ë¡œë“œ ë°©ì‹ì„ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: state_dict ë°©ì‹ì´ í‘œì¤€ì…ë‹ˆë‹¤. ì¶”ê°€ë¡œ optimizer.state_dict()ë„ í•¨ê»˜ ì €ì¥í•˜ë©´ í•™ìŠµì„ ì´ì–´ì„œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œí–ˆë”ë‹ˆ ì„±ëŠ¥ì´ ë–¨ì–´ì¡Œì–´ìš” (Optimizer ìƒíƒœ ëˆ„ë½)",
          "bug_type": "B",
          "bug_type_name": "Checkpoint Error",
          "file_name": "checkpoint.py",
          "buggy_code": "import torch\n\n# ì²´í¬í¬ì¸íŠ¸ ì €ì¥\ndef save_checkpoint(model, optimizer, epoch, loss):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        # optimizer ìƒíƒœ ëˆ„ë½!\n        'loss': loss\n    }, 'checkpoint.pth')\n\n# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\ndef load_checkpoint(model, optimizer):\n    checkpoint = torch.load('checkpoint.pth')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    # optimizer ìƒíƒœ ë³µì› ì•ˆ í•¨!\n    return checkpoint['epoch'], checkpoint['loss']\n\n# í•™ìŠµ ì¬ê°œ\nmodel = MyModel()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nstart_epoch, _ = load_checkpoint(model, optimizer)\n\nfor epoch in range(start_epoch, 100):\n    # í•™ìŠµ...",
          "correct_code": "import torch\n\n# ì²´í¬í¬ì¸íŠ¸ ì €ì¥\ndef save_checkpoint(model, optimizer, epoch, loss):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),  # optimizer ìƒíƒœ í¬í•¨!\n        'loss': loss\n    }, 'checkpoint.pth')\n\n# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\ndef load_checkpoint(model, optimizer):\n    checkpoint = torch.load('checkpoint.pth')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # optimizer ë³µì›!\n    return checkpoint['epoch'], checkpoint['loss']\n\n# í•™ìŠµ ì¬ê°œ\nmodel = MyModel()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nstart_epoch, _ = load_checkpoint(model, optimizer)\n\nfor epoch in range(start_epoch, 100):\n    # í•™ìŠµ...",
          "error_log": "=== ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Epoch 50) ===\nLoss: 0.1523, Accuracy: 94.2%\n\n=== í•™ìŠµ ì¬ê°œ ===\nEpoch 51: Loss = 0.8934 (ê¸‰ì¦!)\nEpoch 52: Loss = 0.6721\nEpoch 53: Loss = 0.5102\n...\nEpoch 60: Loss = 0.3421 (ë‹¤ì‹œ ìˆ˜ë ´ ì‹œì‘)\n\n[ë¬¸ì œ] 10 epoch ë¶„ëŸ‰ì˜ í•™ìŠµì´ ë‚­ë¹„ë¨",
          "success_log": "=== ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Epoch 50) ===\nLoss: 0.1523, Accuracy: 94.2%\n\n=== í•™ìŠµ ì¬ê°œ ===\nEpoch 51: Loss = 0.1498 (ìì—°ìŠ¤ëŸ¬ìš´ ì—°ì†)\nEpoch 52: Loss = 0.1456\nEpoch 53: Loss = 0.1402\n\n[ì •ìƒ] ëŠê¹€ ì—†ì´ í•™ìŠµ ì´ì–´ê°",
          "hint": "Adam, SGD+momentum ë“±ì˜ ì˜µí‹°ë§ˆì´ì €ëŠ” ë‚´ë¶€ ìƒíƒœë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ìƒíƒœë„ í•¨ê»˜ ì €ì¥í•´ì•¼ í•™ìŠµì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "optimizer.state_dict()",
              "optimizer.load_state_dict("
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Incomplete Checkpoint",
            "description": "Optimizer ë‚´ë¶€ ìƒíƒœ ì˜ˆì‹œ (Adam):\n\n```python\n{\n  'param_0': {\n    'step': 50000,           # ì—…ë°ì´íŠ¸ íšŸìˆ˜\n    'exp_avg': tensor(...),   # 1ì°¨ ëª¨ë©˜íŠ¸ (gradient ì´ë™í‰ê· )\n    'exp_avg_sq': tensor(...) # 2ì°¨ ëª¨ë©˜íŠ¸ (gradientÂ² ì´ë™í‰ê· )\n  },\n  ...\n}\n```\n\nì´ ìƒíƒœê°€ ì—†ìœ¼ë©´:\n- Adamì˜ adaptive learning rateê°€ ë¦¬ì…‹\n- íŒŒë¼ë¯¸í„°ë³„ë¡œ ìµœì í™”ëœ í•™ìŠµë¥  ì •ë³´ ì†ì‹¤\n- í•™ìŠµì´ 'ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘'í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë™ì‘\n- ê²°ê³¼: Loss ê¸‰ì¦ â†’ ë‹¤ì‹œ ìˆ˜ë ´ (ì‹œê°„ ë‚­ë¹„)",
            "suggestion": "ì²´í¬í¬ì¸íŠ¸ì—ëŠ” ìµœì†Œí•œ model.state_dict(), optimizer.state_dict(), epochë¥¼ í¬í•¨í•˜ì„¸ìš”. ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš© ì‹œ scheduler.state_dict()ë„ ì¶”ê°€í•˜ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: ì¥ì‹œê°„ í•™ìŠµ(ìˆ˜ ì¼~ìˆ˜ ì£¼)ì—ì„œ ì²´í¬í¬ì¸íŠ¸ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. ì„œë²„ ì¥ì• , ë©”ëª¨ë¦¬ ë¶€ì¡± ë“±ìœ¼ë¡œ í•™ìŠµì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë§¤ N ì—í¬í¬ë§ˆë‹¤ ì™„ì „í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ì„¸ìš”."
        },
        {
          "step": 3,
          "title": "ë‹¤ë¥¸ ì„œë²„ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ìš” (GPU/CPU í˜¸í™˜ì„±)",
          "bug_type": "C",
          "bug_type_name": "Device Compat",
          "file_name": "cross_device.py",
          "buggy_code": "import torch\n\n# === GPU ì„œë²„ì—ì„œ í•™ìŠµ ë° ì €ì¥ ===\nmodel = MyModel().cuda()\n# ... í•™ìŠµ ...\ntorch.save(model.state_dict(), 'model.pth')\n\n# === CPU ì„œë²„ì—ì„œ ë¡œë“œ (ì—ëŸ¬!) ===\nmodel = MyModel()\nmodel.load_state_dict(torch.load('model.pth'))  # ì—ëŸ¬ ë°œìƒ\nmodel.eval()",
          "correct_code": "import torch\n\n# === GPU ì„œë²„ì—ì„œ í•™ìŠµ ë° ì €ì¥ ===\nmodel = MyModel().cuda()\n# ... í•™ìŠµ ...\ntorch.save(model.state_dict(), 'model.pth')\n\n# === CPU ì„œë²„ì—ì„œ ë¡œë“œ ===\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MyModel()\nmodel.load_state_dict(torch.load('model.pth', map_location=device))\nmodel.to(device)\nmodel.eval()",
          "error_log": "# CPU ì„œë²„ì—ì„œ ì‹¤í–‰\n>>> torch.cuda.is_available()\nFalse\n\n>>> model.load_state_dict(torch.load('model.pth'))\nRuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu').",
          "success_log": "# CPU ì„œë²„ì—ì„œ ì‹¤í–‰\n>>> torch.cuda.is_available()\nFalse\n>>> device = torch.device('cpu')\n>>> model.load_state_dict(torch.load('model.pth', map_location=device))\n<All keys matched successfully>\n>>> model.to(device)\n>>> model.eval()\n\n# ì¶”ë¡  ì •ìƒ ë™ì‘",
          "hint": "torch.load()ì˜ map_location íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ì €ì¥ëœ í…ì„œë¥¼ ì›í•˜ëŠ” deviceë¡œ ë§¤í•‘í•˜ì—¬ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "map_location"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Cross-Device Compatibility Error",
            "description": "torch.save()ëŠ” í…ì„œì˜ device ì •ë³´ë„ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.\n\n**GPUì—ì„œ ì €ì¥ ì‹œ:**\n- í…ì„œ ë°ì´í„°: [0.123, 0.456, ...]\n- Device ì •ë³´: 'cuda:0'\n\n**CPUì—ì„œ ë¡œë“œ ì‹œ:**\n- torch.load()ê°€ 'cuda:0'ìœ¼ë¡œ í…ì„œë¥¼ ë³µì›í•˜ë ¤ ì‹œë„\n- CUDA ì—†ìŒ â†’ ì—ëŸ¬\n\n**í•´ê²°ì±…:**\n- map_location='cpu': ëª¨ë“  í…ì„œë¥¼ CPUë¡œ\n- map_location=device: ë™ì ìœ¼ë¡œ device ê²°ì •\n- map_location={'cuda:0': 'cuda:1'}: íŠ¹ì • GPUâ†’ë‹¤ë¥¸ GPU ë§¤í•‘",
            "suggestion": "ë°°í¬ í™˜ê²½ì´ í•™ìŠµ í™˜ê²½ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¡œë“œ ì½”ë“œì—ì„œ í•­ìƒ map_locationì„ ì‚¬ìš©í•˜ì„¸ìš”. íŠ¹íˆ torch.device('cuda' if torch.cuda.is_available() else 'cpu') íŒ¨í„´ì„ í™œìš©í•˜ë©´ ì–´ëŠ í™˜ê²½ì—ì„œë„ ë™ì‘í•©ë‹ˆë‹¤."
          },
          "coaching": "ğŸ¯ í˜„ì—…: GPUì—ì„œ í•™ìŠµí•˜ê³  CPUì—ì„œ ì„œë¹™í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë˜í•œ ë©€í‹° GPUë¡œ í•™ìŠµí•˜ê³  ë‹¨ì¼ GPUë¡œ ë°°í¬í•˜ëŠ” ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤. map_location ì‚¬ìš©ì„ ìŠµê´€í™”í•˜ì„¸ìš”."
        }
      ]
    },
    {
      "id": "P5",
      "project_title": "LLM & Transformer ë””ë²„ê¹…",
      "scenario": "Transformer ê¸°ë°˜ ëª¨ë¸(BERT, GPT ë“±)ì„ Fine-tuningí•˜ê±°ë‚˜ ì¶”ë¡ í•˜ëŠ”ë°, ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¤ê±°ë‚˜ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.",
      "difficulty": 3,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "íŒ¨ë”© í† í°ì´ ë¬¸ì¥ ì˜ë¯¸ë¥¼ ë§ì³ìš” (Attention Mask ëˆ„ë½)",
          "bug_type": "A",
          "bug_type_name": "Attention Mask",
          "file_name": "bert_inference.py",
          "buggy_code": "from transformers import BertModel, BertTokenizer\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()\n\ndef get_embedding(text, max_length=128):\n    # í† í¬ë‚˜ì´ì§•\n    tokens = tokenizer(\n        text,\n        padding='max_length',\n        max_length=max_length,\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    with torch.no_grad():\n        # attention_maskë¥¼ ì „ë‹¬í•˜ì§€ ì•ŠìŒ!\n        outputs = model(input_ids=tokens['input_ids'])\n        \n    # [CLS] í† í°ì˜ ì„ë² ë”© ë°˜í™˜\n    return outputs.last_hidden_state[:, 0, :]\n\n# ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°\nemb_a = get_embedding(\"I love you\")\nemb_b = get_embedding(\"I love you\")\nemb_c = get_embedding(\"I hate everything in this world\")\n\nprint(f\"A-B similarity: {cosine_similarity(emb_a, emb_b)}\")\nprint(f\"A-C similarity: {cosine_similarity(emb_a, emb_c)}\")",
          "correct_code": "from transformers import BertModel, BertTokenizer\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()\n\ndef get_embedding(text, max_length=128):\n    # í† í¬ë‚˜ì´ì§•\n    tokens = tokenizer(\n        text,\n        padding='max_length',\n        max_length=max_length,\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    with torch.no_grad():\n        # attention_mask ì „ë‹¬!\n        outputs = model(\n            input_ids=tokens['input_ids'],\n            attention_mask=tokens['attention_mask']  # íŒ¨ë”© ë§ˆìŠ¤í‚¹\n        )\n        \n    # [CLS] í† í°ì˜ ì„ë² ë”© ë°˜í™˜\n    return outputs.last_hidden_state[:, 0, :]\n\n# ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°\nemb_a = get_embedding(\"I love you\")\nemb_b = get_embedding(\"I love you\")\nemb_c = get_embedding(\"I hate everything in this world\")\n\nprint(f\"A-B similarity: {cosine_similarity(emb_a, emb_b)}\")\nprint(f\"A-C similarity: {cosine_similarity(emb_a, emb_c)}\")",
          "error_log": "A: \"I love you\" (125 padding tokens)\nB: \"I love you\" (125 padding tokens)\nC: \"I hate everything in this world\" (120 padding tokens)\n\nA-B similarity: 0.9234 (OK)\nA-C similarity: 0.9456 (ì´ìƒ! A-Bë³´ë‹¤ ë†’ìŒ)\n\n[ì›ì¸] íŒ¨ë”© í† í°ì´ attentionì— í¬í•¨ë˜ì–´ ì˜ë¯¸ ì™œê³¡",
          "success_log": "A: \"I love you\" (masked: 125 padding tokens)\nB: \"I love you\" (masked: 125 padding tokens)\nC: \"I hate everything in this world\" (masked: 120 padding tokens)\n\nA-B similarity: 1.0000 (ì™„ë²½íˆ ë™ì¼)\nA-C similarity: 0.2341 (ë‚®ìŒ - ì˜ë¯¸ ì°¨ì´ ë°˜ì˜)\n\n[ì •ìƒ] ì‹¤ì œ í† í°ë§Œ attentionì— ë°˜ì˜",
          "hint": "Tokenizerê°€ ë°˜í™˜í•˜ëŠ” attention_maskëŠ” ì‹¤ì œ í† í°=1, íŒ¨ë”©=0ì…ë‹ˆë‹¤. ì´ ë§ˆìŠ¤í¬ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•´ì•¼ íŒ¨ë”©ì´ ë¬´ì‹œë©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "attention_mask"
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Missing Attention Mask",
            "description": "Transformerì˜ Self-Attentionì€ ëª¨ë“  í† í° ê°„ì˜ ê´€ê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:\n\n**Attention mask ì—†ì´:**\n- [CLS] I love you [PAD] [PAD] [PAD] ... (125ê°œ)\n- [CLS]ê°€ ëª¨ë“  [PAD] í† í°ì—ë„ attention\n- íŒ¨ë”©ì´ ë§ì„ìˆ˜ë¡ ì„ë² ë”©ì´ íŒ¨ë”©ì— ì˜í•´ ì™œê³¡\n\n**Attention mask ìˆìœ¼ë©´:**\n- attention_mask = [1, 1, 1, 1, 0, 0, 0, ...]\n- [PAD] ìœ„ì¹˜ì˜ attention score = -âˆ â†’ softmax í›„ 0\n- ì‹¤ì œ í† í°ë§Œ attention ê³„ì‚°ì— ì°¸ì—¬\n\nì§§ì€ ë¬¸ì¥ë¼ë¦¬ íŒ¨ë”©ì´ ë¹„ìŠ·í•˜ë©´ 'íŒ¨ë”© ìœ ì‚¬ë„'ê°€ ë†’ì•„ì ¸ ì˜ë¯¸ ë¹„êµê°€ ì™œê³¡ë©ë‹ˆë‹¤.",
            "suggestion": "HuggingFace ëª¨ë¸ ì‚¬ìš© ì‹œ tokenizerì˜ attention_maskë¥¼ í•­ìƒ ëª¨ë¸ì— ì „ë‹¬í•˜ì„¸ìš”. **tokensë¥¼ ì‚¬ìš©í•˜ë©´ í•œ ë²ˆì— ì „ë‹¬ë©ë‹ˆë‹¤: model(**tokens)"
          },
          "coaching": "ğŸ¯ í˜„ì—…: BERT, GPT ë“± Transformer ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ attention_maskëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. íŠ¹íˆ ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë¬¸ì¥ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ íŒ¨ë”©ì´ ë¶ˆê· í˜•í•´ì§€ë¯€ë¡œ ë°˜ë“œì‹œ ë§ˆìŠ¤í‚¹í•´ì•¼ í•©ë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "Tokenizerì™€ ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ ë§ì„ í•´ìš” (Tokenizer ë¶ˆì¼ì¹˜)",
          "bug_type": "B",
          "bug_type_name": "Tokenizer Mismatch",
          "file_name": "mismatched_tokenizer.py",
          "buggy_code": "from transformers import BertForSequenceClassification, BertTokenizer\nimport torch\n\n# Fine-tuningëœ ëª¨ë¸ ë¡œë“œ\nmodel = BertForSequenceClassification.from_pretrained('./my_finetuned_model')\nmodel.eval()\n\n# ë‹¤ë¥¸ tokenizer ì‚¬ìš© (ì‹¤ìˆ˜!)\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')  # large ë²„ì „!\n\ndef predict(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    probs = torch.softmax(outputs.logits, dim=1)\n    return 'Positive' if probs[0, 1] > 0.5 else 'Negative', probs[0].tolist()\n\nresult, probs = predict(\"The movie was great!\")\nprint(f\"Prediction: {result}, Probabilities: {probs}\")",
          "correct_code": "from transformers import BertForSequenceClassification, BertTokenizer\nimport torch\n\n# Fine-tuningëœ ëª¨ë¸ ë¡œë“œ\nmodel = BertForSequenceClassification.from_pretrained('./my_finetuned_model')\nmodel.eval()\n\n# ê°™ì€ ëª¨ë¸ì˜ tokenizer ì‚¬ìš©!\ntokenizer = BertTokenizer.from_pretrained('./my_finetuned_model')  # ëª¨ë¸ê³¼ ë™ì¼ ê²½ë¡œ\n# ë˜ëŠ”: BertTokenizer.from_pretrained('bert-base-uncased')  # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼\n\ndef predict(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    probs = torch.softmax(outputs.logits, dim=1)\n    return 'Positive' if probs[0, 1] > 0.5 else 'Negative', probs[0].tolist()\n\nresult, probs = predict(\"The movie was great!\")\nprint(f\"Prediction: {result}, Probabilities: {probs}\")",
          "error_log": "Input: \"The movie was great!\"\n\n[ì˜ëª»ëœ Tokenizer: bert-large-uncased]\nTokens: ['the', 'movie', 'was', 'great', '!']\nToken IDs: [1996, 3185, 2001, 2307, 999]\n\nPrediction: Negative (0.01, 0.99)\n[í‹€ë¦¼] ëª¨ë¸ì´ ë³¸ ì  ì—†ëŠ” í† í° ID ë§¤í•‘",
          "success_log": "Input: \"The movie was great!\"\n\n[ì˜¬ë°”ë¥¸ Tokenizer: bert-base-uncased]\nTokens: ['the', 'movie', 'was', 'great', '!']\nToken IDs: [1996, 3185, 2001, 2307, 999]\n\nPrediction: Positive (0.97, 0.03)\n[ì •ë‹µ] í•™ìŠµ ì‹œì™€ ë™ì¼í•œ í† í°í™”",
          "hint": "ëª¨ë¸ê³¼ TokenizerëŠ” í•­ìƒ ì§ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤. Fine-tuningí•œ ëª¨ë¸ì„ ì €ì¥í•  ë•Œ tokenizerë„ ê°™ì€ ê²½ë¡œì— ì €ì¥í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_any": [
              "'./my_finetuned_model'",
              "'bert-base-uncased'"
            ],
            "forbidden": [
              "'bert-large-uncased'"
            ]
          },
          "error_info": {
            "type": "Tokenizer-Model Mismatch",
            "description": "Tokenizerì™€ ëª¨ë¸ì˜ ë¶ˆì¼ì¹˜ ë¬¸ì œ:\n\n**ê°™ì€ ë‹¨ì–´ â†’ ë‹¤ë¥¸ í† í° ID:**\n- bert-base: \"great\" â†’ ID 2307\n- bert-large: \"great\" â†’ ID 2307 (ê°™ì„ ìˆ˜ ìˆì§€ë§Œ...)\n- roberta: \"great\" â†’ ID 372 (ì™„ì „íˆ ë‹¤ë¦„!)\n\n**ë¬¸ì œ:**\n- ëª¨ë¸ì€ í•™ìŠµ ì‹œ íŠ¹ì • vocabularyë¡œ í•™ìŠµ\n- ë‹¤ë¥¸ tokenizerëŠ” ë‹¤ë¥¸ ID ë§¤í•‘\n- ID 2307ì´ í•™ìŠµ ë•Œ \"great\"ì˜€ì§€ë§Œ, ë‹¤ë¥¸ tokenizerì—ì„œëŠ” \"terrible\"ì¼ ìˆ˜ ìˆìŒ\n- ê²°ê³¼: ì™„ì „íˆ ì—‰ëš±í•œ ì˜ˆì¸¡\n\n**íŠ¹íˆ ìœ„í—˜í•œ ê²½ìš°:**\n- ëª¨ë¸ ê³„ì—´ì´ ë‹¤ë¥¸ ê²½ìš° (BERT vs RoBERTa)\n- ê°™ì€ ê³„ì—´ì´ì§€ë§Œ í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš° (base vs large)",
            "suggestion": "ëª¨ë¸ ì €ì¥ ì‹œ tokenizer.save_pretrained(path)ë¡œ tokenizerë„ í•¨ê»˜ ì €ì¥í•˜ì„¸ìš”. ë¡œë“œí•  ë•ŒëŠ” ëª¨ë¸ê³¼ ê°™ì€ ê²½ë¡œì—ì„œ tokenizerë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”."
          },
          "coaching": "ğŸ¯ í˜„ì—…: HuggingFace ëª¨ë¸ ë°°í¬ ì‹œ ëª¨ë¸+tokenizer+configë¥¼ ì„¸íŠ¸ë¡œ ê´€ë¦¬í•˜ì„¸ìš”. ë²„ì „ ë¶ˆì¼ì¹˜ëŠ” ì°¾ê¸° ì–´ë ¤ìš´ ë²„ê·¸ì˜ ì›ì¸ì´ ë©ë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "GPTê°€ ì´ìƒí•œ ë§ë§Œ ë°˜ë³µí•´ìš” (ìƒì„± íŒŒë¼ë¯¸í„° ì˜¤ë¥˜)",
          "bug_type": "C",
          "bug_type_name": "Generation Bug",
          "file_name": "gpt_generate.py",
          "buggy_code": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.eval()\n\ndef generate_text(prompt, max_length=100):\n    inputs = tokenizer(prompt, return_tensors='pt')\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs['input_ids'],\n            max_length=max_length,\n            # íŒŒë¼ë¯¸í„°ê°€ ì—†ê±°ë‚˜ ì˜ëª»ë¨\n        )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nresult = generate_text(\"Once upon a time\")\nprint(result)",
          "correct_code": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.eval()\n\ndef generate_text(prompt, max_length=100):\n    inputs = tokenizer(prompt, return_tensors='pt')\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs['input_ids'],\n            max_length=max_length,\n            do_sample=True,           # ìƒ˜í”Œë§ í™œì„±í™”\n            temperature=0.8,           # ë‹¤ì–‘ì„± ì¡°ì ˆ (0.7~1.0 ê¶Œì¥)\n            top_k=50,                  # ìƒìœ„ 50ê°œ í† í°ì—ì„œ ìƒ˜í”Œë§\n            top_p=0.95,                # ëˆ„ì  í™•ë¥  95%ê¹Œì§€ì˜ í† í°ì—ì„œ ìƒ˜í”Œë§\n            repetition_penalty=1.2,    # ë°˜ë³µ ë°©ì§€ (>1.0)\n            pad_token_id=tokenizer.eos_token_id  # íŒ¨ë”© ê²½ê³  ë°©ì§€\n        )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nresult = generate_text(\"Once upon a time\")\nprint(result)",
          "error_log": "Prompt: \"Once upon a time\"\n\n[Greedy Decoding - ê¸°ë³¸ ì„¤ì •]\nOutput: \"Once upon a time, the the the the the the the the the the the the the the the the...\"\n\n[ë¬¸ì œ] ê°€ì¥ ë†’ì€ í™•ë¥  í† í°ë§Œ ê³„ì† ì„ íƒ â†’ ë°˜ë³µ ë£¨í”„",
          "success_log": "Prompt: \"Once upon a time\"\n\n[Sampling with parameters]\nOutput: \"Once upon a time, there was a young princess who lived in a magnificent castle at the edge of an enchanted forest. Every morning, she would...\"\n\n[ì •ìƒ] ë‹¤ì–‘í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ ìƒì„±",
          "hint": "generate()ì˜ ê¸°ë³¸ ì„¤ì •ì€ Greedy Decoding(do_sample=False)ì…ë‹ˆë‹¤. ì´ ëª¨ë“œì—ì„œëŠ” ê°€ì¥ í™•ë¥  ë†’ì€ í† í°ë§Œ ë°˜ë³µ ì„ íƒí•˜ì—¬ ë°˜ë³µ ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "do_sample=True"
            ],
            "required_any": [
              "temperature=",
              "top_k=",
              "top_p=",
              "repetition_penalty="
            ],
            "forbidden": []
          },
          "error_info": {
            "type": "Text Generation Parameter Error",
            "description": "í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ:\n\n**Greedy Decoding (ê¸°ë³¸):**\n- ë§¤ ìŠ¤í… ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ\n- ê²°ì •ì (deterministic) - í•­ìƒ ê°™ì€ ê²°ê³¼\n- ë¬¸ì œ: ë°˜ë³µ ë£¨í”„, ì§€ë£¨í•œ í…ìŠ¤íŠ¸\n\n**Sampling:**\n- í™•ë¥  ë¶„í¬ì—ì„œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§\n- do_sample=Trueë¡œ í™œì„±í™”\n- temperature: ë¶„í¬ì˜ ë‚ ì¹´ë¡œì›€ ì¡°ì ˆ (ë‚®ìœ¼ë©´ greedyì— ê°€ê¹Œì›€)\n- top_k: ìƒìœ„ kê°œ í† í°ì—ì„œë§Œ ìƒ˜í”Œë§\n- top_p (nucleus): ëˆ„ì  í™•ë¥  pê¹Œì§€ì˜ í† í°ì—ì„œ ìƒ˜í”Œë§\n\n**repetition_penalty:**\n- ì´ë¯¸ ìƒì„±ëœ í† í°ì˜ í™•ë¥ ì„ ë‚®ì¶¤\n- > 1.0: ë°˜ë³µ ì–µì œ\n- 1.0: íš¨ê³¼ ì—†ìŒ",
            "suggestion": "ìš©ë„ì— ë§ê²Œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”. ì°½ì˜ì  ê¸€ì“°ê¸°: temperature=0.9, top_p=0.95. ì •í™•í•œ ë‹µë³€: temperature=0.3, top_k=10. ë°˜ë³µ ë¬¸ì œê°€ ìˆìœ¼ë©´ repetition_penalty=1.2 ì¶”ê°€."
          },
          "coaching": "ğŸ¯ í˜„ì—…: í…ìŠ¤íŠ¸ ìƒì„± í’ˆì§ˆì€ íŒŒë¼ë¯¸í„° íŠœë‹ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. ìš©ë„(ì±—ë´‡, ë²ˆì—­, ìš”ì•½, ì°½ì‘)ì— ë”°ë¼ ìµœì  íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¦…ë‹ˆë‹¤. A/B í…ŒìŠ¤íŠ¸ë¡œ ìµœì ê°’ì„ ì°¾ìœ¼ì„¸ìš”."
        }
      ]
    }
  ]
}
