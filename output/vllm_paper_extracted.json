{
  "title": "vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention",
  "file_name": "2309.06180v1.pdf",
  "pages": [
    {
      "page_number": 1,
      "content": "Efficient Memory Management for Large Language\nModel Serving with PagedAttention\nWoosukKwon1,âˆ— ZhuohanLi1,âˆ— SiyuanZhuang1 YingSheng1,2 LianminZheng1 CodyHaoYu3\nJosephE.Gonzalez1 HaoZhang4 IonStoica1\n1UCBerkeley 2StanfordUniversity 3IndependentResearcher 4UCSanDiego\nAbstract\n40\nHighthroughputservingoflargelanguagemodels(LLMs)\nrequiresbatchingsufficientlymanyrequestsatatime.How-\n30\never,existingsystemsstrugglebecausethekey-valuecache KV\n(KV cache) memory for each request is huge and grows Parameters Cache\n(26GB, 65%) (>30%) 20\nandshrinksdynamically.Whenmanagedinefficiently,this\nmemorycanbesignificantlywastedbyfragmentationand\nredundantduplication,limitingthebatchsize.Toaddress Others\nthisproblem,weproposePagedAttention,anattentional- NVIDIA A100 40GB\ngorithminspiredbytheclassicalvirtualmemoryandpag-\ningtechniquesinoperatingsystems.Ontopofit,webuild\nvLLM,anLLMservingsystemthatachieves(1)near-zero\nwasteinKVcachememoryand(2)flexiblesharingofKV\ncache within and across requests to further reduce mem-\noryusage.OurevaluationsshowthatvLLMimprovesthe\nthroughputofpopularLLMsby2-4Ã—withthesamelevel\nof latency compared to the state-of-the-art systems, such\nasFasterTransformerandOrca.Theimprovementismore\npronouncedwithlongersequences,largermodels,andmore\ncomplexdecodingalgorithms.vLLMâ€™ssourcecodeispublicly\navailableathttps://github.com/vllm-project/vllm.\n1 Introduction\nTheemergenceoflargelanguagemodels(LLMs)likeGPT[5,\n37]andPaLM[9]haveenablednewapplicationssuchaspro-\ngrammingassistants[6,18]anduniversalchatbots[19,35]\nthatarestartingtoprofoundlyimpactourworkanddaily\nroutines.Manycloudcompanies[34,44]areracingtopro-\nvidetheseapplicationsashostedservices.However,running\ntheseapplicationsisveryexpensive,requiringalargenum-\nber of hardware accelerators such as GPUs. According to\nrecentestimates,processinganLLMrequestcanbe10Ã—more\nexpensivethanatraditionalkeywordquery[43].Giventhese\nhighcosts,increasingthethroughputâ€”andhencereducing\nPermissiontomakedigitalorhardcopiesofpartorallofthisworkfor\npersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesare\nnotmadeordistributedforprofitorcommercialadvantageandthatcopies\nbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforthird-\npartycomponentsofthisworkmustbehonored.Forallotheruses,contact\ntheowner/author(s).\nSOSPâ€™23,October23â€“26,2023,Koblenz,Germany\nÂ©2023Copyrightheldbytheowner/author(s).\nACMISBN979-8-4007-0229-7/23/10.\nhttps://doi.org/10.1145/3600006.3613165\n)BG(\negasu\nyromeM\nExisting systems vLLM\nParameter size\n1.2k\n0.8k\n0.4k\n0\n0 10 20 30 40\nBatch size (# requests)\n)s/nekot(\ntuphguorhT\nFigure1.Left:MemorylayoutwhenservinganLLMwith\n13B parameters on NVIDIA A100. The parameters (gray)\npersistinGPUmemorythroughoutserving.Thememory\nfortheKVcache(red)is(de)allocatedperservingrequest.\nA small amount of memory (yellow) is used ephemerally\nforactivation.Right:vLLMsmoothsouttherapidgrowth\ncurveofKVcachememoryseeninexistingsystems[31,60],\nleadingtoanotableboostinservingthroughput.\nthecostperrequestâ€”ofLLMserving systemsisbecoming\nmoreimportant.\nAtthecoreofLLMsliesanautoregressiveTransformer\nmodel[53].Thismodelgenerateswords(tokens),oneata\ntime,basedontheinput(prompt)andtheprevioussequence\noftheoutputâ€™stokensithasgeneratedsofar.Foreachre-\nquest,thisexpensiveprocessisrepeateduntilthemodelout-\nputsaterminationtoken.Thissequentialgenerationprocess\nmakestheworkloadmemory-bound,underutilizingthecom-\nputationpowerofGPUsandlimitingtheservingthroughput.\nImprovingthethroughputispossiblebybatchingmulti-\nplerequeststogether.However,toprocessmanyrequests\nin a batch, the memory space for each request should be\nefficientlymanaged.Forexample,Fig.1(left)illustratesthe\nmemorydistributionfora13B-parameterLLMonanNVIDIA\nA100GPUwith40GBRAM.Approximately65%ofthemem-\noryisallocatedforthemodelweights,whichremainstatic\nduringserving.Closeto30%ofthememoryisusedtostore\nthedynamicstatesoftherequests.ForTransformers,these\nstatesconsistofthekeyandvaluetensorsassociatedwiththe\nattentionmechanism,commonlyreferredtoasKVcache[41],\nwhichrepresentthecontextfromearliertokenstogener-\nate new output tokens in sequence. The remaining small\nâˆ—Equalcontribution.\n1\n3202\npeS\n21\n]GL.sc[\n1v08160.9032:viXra"
    },
    {
      "page_number": 2,
      "content": "100\n80\n60\n40\n20\n0\nOrca Orca Orca vLLM\n(Max) (Pow2) (Oracle)\n)%(\negasu\nehcac\nVK\ndecodingalgorithms, such asparallel sampling andbeam\nToken states Reservation Internal frag. E & x O te t r h n e a r l s frag. search,thatgeneratemultipleoutputsperrequest.Inthese\nscenarios,therequestconsistsofmultiplesequencesthatcan 8.9\npartiallysharetheirKVcache.However,memorysharingis\n41.6 36.6\nnotpossibleintheexistingsystemsbecausetheKVcacheof\n57.3 thesequencesisstoredinseparatecontiguousspaces.\n13.6 25.2 96.3 To address the above limitations, we propose PagedAt-\n17.9\ntention, an attention algorithm inspired by the operating\n13.3\n38.2 systemâ€™s(OS)solutiontomemoryfragmentationandshar-\n20.4 26.8\ning:virtualmemorywithpaging.PagedAttentiondividesthe\nrequestâ€™sKVcacheintoblocks,eachofwhichcancontain\ntheattentionkeysandvaluesofafixednumberoftokens.In\nPagedAttention,theblocksfortheKVcachearenotneces-\nFigure2.Averagepercentageofmemorywastesindifferent\nsarilystoredincontiguousspace.Therefore,wecanmanage\nLLMservingsystemsduringtheexperimentinÂ§6.2.\ntheKVcacheinamoreflexiblewayasinOSâ€™svirtualmem-\nory:onecanthinkofblocksaspages,tokensasbytes,and\npercentageofmemoryisusedforotherdata,includingac- requestsasprocesses.Thisdesignalleviatesinternalfrag-\ntivationsâ€“theephemeraltensorscreatedwhenevaluating mentation by using relatively small blocks and allocating\ntheLLM.Sincethemodelweightsareconstantandtheac- themondemand.Moreover,iteliminatesexternalfragmen-\ntivationsonlyoccupyasmallfractionoftheGPUmemory, tation as all blocks have the same size. Finally, it enables\nthewaytheKVcacheismanagediscriticalindetermining memory sharing at the granularity of a block, across the\nthemaximumbatchsize.Whenmanagedinefficiently,the differentsequencesassociatedwiththesamerequestoreven\nKVcachememorycansignificantlylimitthebatchsizeand acrossthedifferentrequests.\nconsequentlythethroughputoftheLLM,asillustratedin Inthiswork,webuildvLLM,ahigh-throughputdistributed\nFig.1(right). LLMservingengineontopofPagedAttentionthatachieves\nInthispaper,weobservethatexistingLLMservingsys- near-zerowasteinKVcachememory.vLLMusesblock-level\ntems[31,60]fallshortofmanagingtheKVcachememory memorymanagementandpreemptiverequestscheduling\nefficiently.ThisismainlybecausetheystoretheKVcacheof thatareco-designedwithPagedAttention.vLLMsupports\narequestincontiguousmemoryspace,asmostdeeplearning popularLLMssuchasGPT[5],OPT[62],andLLaMA[52]\nframeworks[33,39]requiretensorstobestoredincontigu- withvaryingsizes,includingtheonesexceedingthememory\nousmemory.However,unlikethetensorsinthetraditional capacityofasingleGPU.Ourevaluationsonvariousmodels\ndeeplearningworkloads,theKVcachehasuniquecharac- andworkloadsshowthatvLLMimprovestheLLMserving\nteristics:itdynamicallygrowsandshrinksovertimeasthe throughput by 2-4Ã— compared to the state-of-the-art sys-\nmodelgeneratesnewtokens,anditslifetimeandlengthare tems[31,60],withoutaffectingthemodelaccuracyatall.The\nnotknownapriori.Thesecharacteristicsmaketheexisting improvementsaremorepronouncedwithlongersequences,\nsystemsâ€™approachsignificantlyinefficientintwoways: largermodels,andmorecomplexdecodingalgorithms(Â§4.3).\nFirst,theexistingsystems[31,60]sufferfrominternaland Insummary,wemakethefollowingcontributions:\nexternalmemoryfragmentation.TostoretheKVcacheof â€¢ Weidentifythechallengesinmemoryallocationinserving\narequestincontiguousspace,theypre-allocate acontigu- LLMsandquantifytheirimpactonservingperformance.\nouschunkofmemorywiththerequestâ€™smaximumlength â€¢ WeproposePagedAttention,anattentionalgorithmthat\n(e.g., 2048 tokens). This can result in severe internal frag- operates on KV cache stored in non-contiguous paged\nmentation, since the requestâ€™s actual length can be much memory, which is inspired by the virtual memory and\nshorterthanitsmaximumlength(e.g.,Fig.11).Moreover, paginginOS.\neveniftheactuallengthisknownapriori,thepre-allocation â€¢ WedesignandimplementvLLM,adistributedLLMserving\nisstillinefficient:Astheentirechunkisreservedduringthe enginebuiltontopofPagedAttention.\nrequestâ€™slifetime,othershorterrequestscannotutilizeany â€¢ WeevaluatevLLMonvariousscenariosanddemonstrate\npartofthechunkthatiscurrentlyunused.Besides,external thatitsubstantiallyoutperformsthepreviousstate-of-the-\nmemoryfragmentationcanalsobesignificant,sincethepre- artsolutionssuchasFasterTransformer[31]andOrca[60].\nallocatedsizecanbedifferentforeachrequest.Indeed,our\nprofilingresultsinFig.2showthatonly20.4%-38.2%ofthe\n2 Background\nKVcachememoryisusedtostoretheactualtokenstatesin\ntheexistingsystems. Inthissection,wedescribethegenerationandservingpro-\nSecond,theexistingsystemscannotexploittheopportu- ceduresoftypicalLLMsandtheiteration-levelscheduling\nnitiesformemorysharing.LLMservicesoftenuseadvanced usedinLLMserving.\n2"
    },
    {
      "page_number": 3,
      "content": "2.1 Transformer-BasedLargeLanguageModels Thepromptphasetakesthewholeuserprompt(ğ‘¥\n1\n,...,ğ‘¥ ğ‘›)\nasinputandcomputestheprobabilityofthefirstnewto-\nThetaskoflanguagemodelingistomodeltheprobability\no\nse\nf\nq\na\nu\nl\ne\ni\nn\nst\nti\no\na\nf\nl\nt\no\no\nr\nk\nd\ne\ne\nn\nri\ns\nng\n(ğ‘¥\n, 1 it\n,.\nis\n..\nc\n,\no\nğ‘¥\nm\nğ‘›)\nm\n.\no\nS\nn\nin\nt\nc\no\ne\nf\nl\na\na\nc\nn\nt\ng\no\nu\nri\na\nz\ng\ne\ne\nth\nh\ne\nas\njo\na\nin\nn\nt\na\np\ntu\nro\nra\nb\nl\n-\nk\nat\ne\ne\nn\ns\nğ‘ƒ\nth\n(\ne\nğ‘¥ ğ‘›\nk\n+\ne\n1\ny\n|\nve\nğ‘¥\nc\n1\nt\n,\no\n.\nr\n.\ns\n.,\nğ‘˜\nğ‘¥\n1\nğ‘›\n,\n)\n.\n.\n.\nD\n.,\nu\nğ‘˜\nr\nğ‘›\nin\na\ng\nnd\nth\nv\ni\na\ns\nlu\np\ne\nro\nv\nc\ne\ne\nc\ns\nt\ns\no\n,\nr\na\ns\nls\nğ‘£\no\n1 ,.\ng\n.\ne\n.\nn\n,\ne\nğ‘£ ğ‘›\nr-\n.\nabilityoverthewholesequenceastheproductofconditional Sinceprompttokensğ‘¥ 1 ,...,ğ‘¥ ğ‘› areallknown,thecomputa-\ntionofthepromptphasecanbeparallelizedusingmatrix-\nprobabilities(a.k.a.autoregressivedecomposition[3]):\nmatrixmultiplicationoperations.Therefore,thisphasecan\nğ‘ƒ(ğ‘¥) =ğ‘ƒ(ğ‘¥ 1 )Â·ğ‘ƒ(ğ‘¥ 2 |ğ‘¥ 1 )Â·Â·Â·ğ‘ƒ(ğ‘¥ ğ‘› |ğ‘¥ 1 ,...,ğ‘¥ ğ‘›âˆ’1 ). (1) efficientlyusetheparallelisminherentinGPUs.\nTheautoregressivegenerationphasegeneratesthere-\nTransformers[53]havebecomethedefactostandardar-\nmainingnewtokenssequentially.Atiterationğ‘¡,themodel\nchitectureformodelingtheprobabilityaboveatalargescale. takesonetokenğ‘¥ ğ‘›+ğ‘¡ asinputandcomputestheprobability\nThemostimportantcomponentofaTransformer-basedlan- ğ‘ƒ(ğ‘¥ ğ‘›+ğ‘¡+1 |ğ‘¥ 1 ,...,ğ‘¥ ğ‘›+ğ‘¡)withthekeyvectorsğ‘˜ 1 ,...,ğ‘˜ ğ‘›+ğ‘¡ and\nguagemodelisitsself-attentionlayers.Foraninputhidden valuevectorsğ‘£ 1 ,...,ğ‘£ ğ‘›+ğ‘¡.Notethatthekeyandvaluevectors\nstate sequence (ğ‘¥\n1\n,...,ğ‘¥ ğ‘›) âˆˆ Rğ‘›Ã—ğ‘‘ , a self-attention layer atpositions1toğ‘›+ğ‘¡ âˆ’1arecachedatpreviousiterations,\nfirstapplieslineartransformationsoneachpositionğ‘– toget onlythenewkeyandvaluevectorğ‘˜ ğ‘›+ğ‘¡ andğ‘£ ğ‘›+ğ‘¡ arecom-\nthequery,key,andvaluevectors: putedatthisiteration.Thisphasecompleteseitherwhenthe\nsequencereachesamaximumlength(specifiedbyusersor\nğ‘ ğ‘– =ğ‘Š ğ‘ ğ‘¥ ğ‘– , ğ‘˜ ğ‘– =ğ‘Š ğ‘˜ ğ‘¥ ğ‘– , ğ‘£ ğ‘– =ğ‘Š ğ‘£ ğ‘¥ ğ‘– . (2) limitedbyLLMs)orwhenanend-of-sequence(<eos>)token\nisemitted.Thecomputationatdifferentiterationscannot\nThen,theself-attentionlayercomputestheattentionscore beparallelizedduetothedatadependencyandoftenuses\nğ‘ ğ‘–ğ‘— bymultiplyingthequeryvectoratonepositionwithall matrix-vectormultiplication,whichislessefficient.Asare-\nthekeyvectorsbeforeitandcomputetheoutputğ‘œ ğ‘– asthe sult,thisphaseseverelyunderutilizesGPUcomputationand\nweightedaverageoverthevaluevectors: becomesmemory-bound,beingresponsibleformostportion\nofthelatencyofasinglerequest.\nâˆš\nğ‘ ğ‘–ğ‘— = exp(ğ‘ ğ‘– âŠ¤ğ‘˜ ğ‘—/ ğ‘‘ âˆš ) , ğ‘œ ğ‘– = âˆ‘ï¸ ğ‘– ğ‘ ğ‘–ğ‘— ğ‘£ ğ‘— . (3) 2.3 BatchingTechniquesforLLMs\n(cid:205)ğ‘– ğ‘¡=1 exp(ğ‘ ğ‘– âŠ¤ğ‘˜ ğ‘¡/ ğ‘‘) ğ‘—=1 ThecomputeutilizationinservingLLMscanbeimproved\nbybatchingmultiplerequests.Becausetherequestsshare\nBesidesthecomputationinEq.4,allothercomponents thesamemodelweights,theoverheadofmovingweightsis\nintheTransformermodel,includingtheembeddinglayer, amortizedacrosstherequestsinabatch,andcanbeover-\nfeed-forwardlayer,layernormalization[2],residualconnec- whelmed by the computational overhead when the batch\ntion[22],outputlogitcomputation,andthequery,key,and size is sufficiently large. However, batching the requests\nvaluetransformationinEq.2,areallappliedindependently toanLLMserviceisnon-trivialfortworeasons.First,the\nposition-wiseinaformofğ‘¦ ğ‘– = ğ‘“(ğ‘¥ ğ‘–). requestsmayarriveatdifferenttimes.Anaivebatchingstrat-\negywouldeithermakeearlierrequestswaitforlaterones\n2.2 LLMService&AutoregressiveGeneration\nordelaytheincomingrequestsuntilearlieronesfinish,lead-\nOncetrained,LLMsareoftendeployedasaconditionalgen- ingtosignificantqueueingdelays.Second,therequestsmay\nerationservice(e.g.,completionAPI[34]orchatbot[19,35]). havevastlydifferentinputandoutputlengths(Fig.11).A\nArequesttoanLLMserviceprovidesalistofinputprompt straightforwardbatchingtechniquewouldpadtheinputs\ntokens(ğ‘¥\n1\n,...,ğ‘¥ ğ‘›),andtheLLMservicegeneratesalistof andoutputsoftherequeststoequalizetheirlengths,wasting\noutputtokens(ğ‘¥ ğ‘›+1 ,...,ğ‘¥ ğ‘›+ğ‘‡)accordingtoEq.1.Wereferto GPUcomputationandmemory.\ntheconcatenationofthepromptandoutputlistsassequence. To address this problem, fine-grained batching mecha-\nDuetothedecompositioninEq.1,theLLMcanonlysam- nisms,suchascellularbatching[16]anditeration-levelsched-\npleandgeneratenewtokensonebyone,andthegeneration uling[60],havebeenproposed.Unliketraditionalmethods\nprocessofeachnewtokendependsonalltheprevioustokens thatworkattherequestlevel,thesetechniquesoperateat\ninthatsequence,specificallytheirkeyandvaluevectors.In theiterationlevel.Aftereachiteration,completedrequests\nthissequentialgenerationprocess,thekeyandvaluevectors areremovedfromthebatch,andnewonesareadded.There-\nof existing tokens are often cached for generating future fore, a new request can be processed after waiting for a\ntokens,knownasKVcache.NotethattheKVcacheofone singleiteration,notwaitingfortheentirebatchtocomplete.\ntokendependsonallitsprevioustokens.Thismeansthatthe Moreover,withspecialGPUkernels,thesetechniqueselim-\nKVcacheofthesametokenappearingatdifferentpositions inatetheneedtopadtheinputsandoutputs.Byreducing\ninasequencewillbedifferent. thequeueingdelayandtheinefficienciesfrompadding,the\nGivenarequestprompt,thegenerationcomputationin fine-grainedbatchingmechanismssignificantlyincreasethe\ntheLLMservicecanbedecomposedintotwophases: throughputofLLMserving.\n3"
    },
    {
      "page_number": 4,
      "content": "1 slot for 2 slots future used 1 slot future used\ngenerated token (reserved) External fragmentation (reserved)\nFour score and seven years ago our fathersbrought forth <eos> <resv> â€¦ <resv> You only live once <eos> <resv> â€¦ <resv>\n7 KV cache states for 2038 slots never used 3 KV cache states for 507 slots never used\nrequest Aâ€™s prompt (internal fragmentation) request Bâ€™s prompt (Internal fragmentation)\nRequest A Request B\ncurrent iteration current iteration\nFigure3.KVcachememorymanagementinexistingsystems.Threetypesofmemorywastesâ€“reserved,internalfragmentation,\nandexternalfragmentationâ€“existthatpreventotherrequestsfromfittingintothememory.Thetokenineachmemoryslot\nrepresentsitsKVcache.NotethesametokenscanhavedifferentKVcachewhenatdifferentpositions.\n3 MemoryChallengesinLLMServing Â§6.3)oftheirKVcache,andthesharingpatternevolvesas\nthedecodingprocessadvances.\nAlthoughfine-grainedbatchingreducesthewasteofcom-\nSchedulingforunknowninput&outputlengths.The\nputingandenablesrequeststobebatchedinamoreflexible\nrequeststoanLLMserviceexhibitvariabilityintheirinput\nway,thenumberofrequeststhatcanbebatchedtogetheris\nandoutputlengths.Thisrequiresthememorymanagement\nstillconstrainedbyGPUmemorycapacity,particularlythe\nsystemtoaccommodateawiderangeofpromptlengths.In\nspaceallocatedtostoretheKVcache.Inotherwords,the\naddition,astheoutputlengthofarequestgrowsatdecoding,\nserving systemâ€™s throughput is memory-bound. Overcom-\nthememoryrequiredforitsKVcachealsoexpandsandmay\ningthismemory-boundrequiresaddressingthefollowing\nexhaustavailablememoryforincomingrequestsorongoing\nchallengesinthememorymanagement:\ngenerationforexistingprompts.Thesystemneedstomake\nLargeKVcache.TheKVCachesizegrowsquicklywiththe schedulingdecisions,suchasdeletingorswappingoutthe\nnumberofrequests.Asanexample,forthe13Bparameter KVcacheofsomerequestsfromGPUmemory.\nOPTmodel[62],theKVcacheofasingletokendemands800\nKBofspace,calculatedas2(keyandvaluevectors)Ã—5120\n3.1 MemoryManagementinExistingSystems\n(hidden state size) Ã— 40 (number of layers) Ã— 2 (bytes per\nFP16).SinceOPTcangeneratesequencesupto2048tokens,\nSincemostoperatorsincurrentdeeplearningframeworks\nthememoryrequiredtostoretheKVcacheofonerequest\n[33,39]requiretensorstobestoredincontiguousmemory,\ncanbeasmuchas1.6GB.ConcurrentGPUshavememory\nprevious LLM serving systems [31, 60] also store the KV\ncapacitiesinthetensofGBs.Evenifallavailablememory\ncacheofonerequestasacontiguoustensoracrossthediffer-\nwasallocatedtoKVcache,onlyafewtensofrequestscould\nentpositions.Duetotheunpredictableoutputlengthsfrom\nbeaccommodated.Moreover,inefficientmemorymanage-\ntheLLM,theystaticallyallocateachunkofmemoryfora\nmentcanfurtherdecreasethebatchsize,asshowninFig.2.\nrequestbasedontherequestâ€™smaximumpossiblesequence\nAdditionally,giventhecurrenttrends,theGPUâ€™scomputa-\nlength,irrespectiveoftheactualinputoreventualoutput\ntionspeedgrowsfasterthanthememorycapacity[17].For\nlengthoftherequest.\nexample,fromNVIDIAA100toH100,TheFLOPSincreases\nFig.3illustratestworequests:requestAwith2048max-\nbymorethan2x,buttheGPUmemorystaysat80GBmax-\nimumpossiblesequencelengthandrequestBwithamax-\nimum.Therefore,webelievethememorywillbecomean\nimumof512.Thechunkpre-allocationschemeinexisting\nincreasinglysignificantbottleneck.\nsystemshasthreeprimarysourcesofmemorywastes:re-\nComplexdecodingalgorithms.LLMservicesofferarange served slotsforfuturetokens,internalfragmentationdueto\nofdecodingalgorithmsforuserstoselectfrom,eachwith over-provisioningforpotentialmaximumsequencelengths,\nvaryingimplicationsformemorymanagementcomplexity. andexternalfragmentationfromthememoryallocatorlike\nForexample,whenusersrequestmultiplerandomsamples thebuddyallocator.Theexternalfragmentationwillnever\nfromasingleinputprompt,atypicalusecaseinprogram beusedforgeneratedtokens,whichisknownbeforeserving\nsuggestion [18], the KV cache of the prompt part, which arequest.Internalfragmentationalsoremainsunused,but\naccountsfor12%ofthetotalKVcachememoryinourex- thisisonlyrealizedafterarequesthasfinishedsampling.\nperiment(Â§6.3),canbesharedtominimizememoryusage. Theyarebothpurememorywaste.Althoughthereserved\nOntheotherhand,theKVcacheduringtheautoregressive memoryiseventuallyused,reservingthisspacefortheen-\ngeneration phase should remain unshared due to the dif- tirerequestâ€™sduration,especiallywhenthereservedspace\nferentsampleresultsandtheirdependenceoncontextand islarge,occupiesthespacethatcouldotherwisebeusedto\nposition. The extent of KV cache sharing depends on the processotherrequests.Wevisualizetheaveragepercentage\nspecificdecodingalgorithmemployed.Inmoresophisticated ofmemorywastesinourexperimentsinFig.2,revealing\nalgorithmslikebeamsearch[49],differentrequestbeams thattheactualeffectivememoryinprevioussystemscanbe\ncan share larger portions (up to 55% memory saving, see aslowas20.4%.\n4"
    },
    {
      "page_number": 5,
      "content": "Worker 0 Key and value vectors\nScheduler Cache Model Block 1 years ago our fathers\nEngine Shard 0\nKV Cache Manager Worker 1 Query Block 2 brought forth\nCache Model vector forth\nEngine Shard 1\nâ€¦\nBlock tables\nBlock 0 Four score and seven\nWorker N - 1\nCPU Block GPU Block Cache Model Figure 5. Illustration of the PagedAttention algorithm,\nAllocator Allocator Engine Shard N - 1\nwhere the attention key and values vectors are stored as\nFigure4.vLLMsystemoverview. non-contiguousblocksinthememory.\nAlthoughcompaction[54]hasbeenproposedasapoten- blocksize(ğµ).Denotethekeyblockğ¾ ğ‘— = (ğ‘˜ (ğ‘—âˆ’1)ğµ+1 ,...,ğ‘˜ ğ‘—ğµ)\ntialsolutiontofragmentation,performingcompactionina andvalueblockğ‘‰ ğ‘— = (ğ‘£ (ğ‘—âˆ’1)ğµ+1 ,...,ğ‘£ ğ‘—ğµ).Theattentioncom-\nperformance-sensitive LLM serving system is impractical putationinEq.4canbetransformedintothefollowingblock-\ndue to the massive KV cache. Even with compaction, the wisecomputation:\np s m h r a e a n - r a i a n l g l g o e c m s a p t e e e n c d t ifi c s h c y u s t t n o e k m d s e s p c . o ac d e in f g or a e lg a o ch rit r h eq m u s e i s n tp e r x e i v st e i n n t g s m m e e m m o o r r y y ğ´ ğ‘–ğ‘— = (cid:205) ğ‘¡ âŒˆ = ğ‘–/ 1 e ğµ x âŒ‰ p e ( x ğ‘ p ğ‘– âŠ¤ ( ğ¾ ğ‘ ğ‘– âŠ¤ ğ‘—/ ğ¾ âˆš ğ‘¡ ğ‘‘ 1/ ) âˆš ğ‘‘) , ğ‘œ ğ‘– = âŒˆ âˆ‘ï¸ ğ‘– ğ‘— / = ğµ 1 âŒ‰ ğ‘‰ ğ‘— ğ´ ğ‘– âŠ¤ ğ‘— , (4)\n4 Method whereğ´ ğ‘–ğ‘— = (ğ‘ ğ‘–,(ğ‘—âˆ’1)ğµ+1 ,...,ğ‘ ğ‘–,ğ‘—ğµ)istherowvectorofatten-\ntionscoreon ğ‘—-thKVblock.\nInthiswork,wedevelopanewattentionalgorithm,Page-\nDuring the attention computation, the PagedAttention\ndAttention,andbuildanLLMservingengine,vLLM,totackle\nkernelidentifiesandfetchesdifferentKVblocksseparately.\nthechallengesoutlinedinÂ§3.ThearchitectureofvLLMis\nWeshowanexampleofPagedAttentioninFig.5:Thekey\nshown in Fig. 4. vLLM adopts a centralized scheduler to\nand value vectors are spread across three blocks, and the\ncoordinatetheexecutionofdistributedGPUworkers.The\nthreeblocksarenotcontiguousonthephysicalmemory.At\nKVcachemanager effectivelymanagestheKVcacheina\npagedfashion,enabledbyPagedAttention.Specifically,the eachtime,thekernelmultipliesthequeryvectorğ‘ ğ‘– ofthe\nKVcachemanagermanagesthephysicalKVcachememory querytoken(â€œforthâ€)andthekeyvectorsğ¾ ğ‘— inablock(e.g.,\nkeyvectorsofâ€œFourscoreandsevenâ€forblock0)tocompute\nontheGPUworkersthroughtheinstructionssentbythe\ncentralizedscheduler. theattentionscoreğ´ ğ‘–ğ‘— ,andlatermultipliesğ´ ğ‘–ğ‘—withthevalue\nNext,WedescribethePagedAttentionalgorithminÂ§4.1.\nvectorsğ‘‰\nğ‘—\ninablocktoderivethefinalattentionoutputğ‘œ\nğ‘–\n.\nIn summary, the PagedAttention algorithm allows the\nWiththat,weshowthedesignoftheKVcachemanagerin\nKVblockstobestoredinnon-contiguousphysicalmemory,\nÂ§4.2andhowitfacilitatesPagedAttentioninÂ§4.3,respec-\nwhichenablesmoreflexiblepagedmemorymanagementin\ntively.Then,weshowhowthisdesignfacilitateseffective\nvLLM.\nmemorymanagementforvariousdecodingmethods(Â§4.4)\nandhandlesthevariablelengthinputandoutputsequences 4.2 KVCacheManager\n(Â§4.5). Finally, we show how the system design of vLLM\nThekeyideabehindvLLMâ€™smemorymanagerisanalogous\nworksinadistributedsetting(Â§4.6).\ntothevirtualmemory [25]inoperatingsystems.OSparti-\n4.1 PagedAttention tionsmemoryintofixed-sizedpagesandmapsuserprogramsâ€™\nlogicalpagestophysicalpages.Contiguouslogicalpagescan\nToaddressthememorychallengesinÂ§3,weintroducePage-\ncorrespondtonon-contiguousphysicalmemorypages,al-\ndAttention,anattentionalgorithminspiredbytheclassicidea\nlowinguserprogramstoaccessmemoryasthoughitwere\nofpaging[25]inoperatingsystems.Unlikethetraditional\ncontiguous.Moreover,physicalmemoryspaceneedsnotto\nattentionalgorithms,PagedAttentionallowsstoringcontinu-\nbefullyreservedinadvance,enablingtheOStodynamically\nouskeysandvaluesinnon-contiguousmemoryspace.Specif-\nallocatephysicalpagesasneeded.vLLMusestheideasbe-\nically,PagedAttentionpartitionstheKVcacheofeachse-\nhind virtual memory to manage the KV cache in an LLM\nquenceintoKVblocks.Eachblockcontainsthekeyandvalue\nvectorsforafixednumberoftokens,1whichwedenoteasKV service. Enabled by PagedAttention, we organize the KV\ncacheasfixed-sizeKVblocks,likepagesinvirtualmemory.\n1InTransformer,eachtokenhasasetofkeyandvaluevectorsacrosslayers Arequestâ€™sKVcacheisrepresentedasaseriesoflogical\nandattentionheadswithinalayer.Allthekeyandvaluevectorscanbe KVblocks,filledfromlefttorightasnewtokensandtheirKV\nmanagedtogetherwithinasingleKVblock,orthekeyandvaluevectorsat\ncachearegenerated.ThelastKVblockâ€™sunfilledpositions\ndifferentheadsandlayerscaneachhaveaseparateblockandbemanaged\narereservedforfuturegenerations.OnGPUworkers,ablock\ninseparateblocktables.Thetwodesignshavenoperformancedifference\nandwechoosethesecondoneforeasyimplementation. engine allocates a contiguous chunk of GPU DRAM and\n5"
    },
    {
      "page_number": 6,
      "content": "Physical KV blocks Physical KV blocks\n(on GPU DRAM)\nBlock 0\nReq A uest P O r u o t m pu p t t s : : â€œ â€œ F fa o t u h r e s r c s o â€ r â†’ e a â€œ n b d ro s u e g v h e tâ€ n â†’ ye â€¦ ars ago ourâ€ B B l l o o c c k k 0 1 1years 1ago 1our 2fathers\nLogic\nR\na\ne\nl\nq\nK\nA u\nV\nes\nb\nt\nlocks\nB\nB\nl\nl\no\no\nc\nc\nk\nk\n1\n2\nye\no\na\nf\nrs\nti\na\nm\ng\ne\no\ns\nour fathers\nLogic\nR\na\ne\nl\nq\nK\nB u\nV\nes\nb\nt\nlocks\nLogical KV blocks Block 2 Block 0 Four score and seven Block 3 brought Block 0 It was the best\nB B l l o o c c k k 0 1 1 1y F e o a u r r s 1 1 s a c g o o re 1 1 a o n u d r f 1 2 s a e th v e e r n s 1 1 Phy n s u 1 i B c m 7 a l b o l e b c r lo k c k T ab # l 1 e fi 4 lled 1 3 B B l l o o c c k k 3 4 b3rought B B l l o o c c k k 1 2 b y ro e u a g rs ht ago our fathers B B l l o o c c k k 4 5 It was the best B B l l o o c c k k 1 2 of times\nB B l l o o c c k k 2 3 b3rought 3 1 3 1 3 â€“ 13 3 â†’ 1 â€“ 42 1 B B l l o o c c k k 5 6 Block 3 B B l l o o c c k k 6 7 Four score and seven\nBlock 8\nBlock 7 1Four 1score 1and 1seven\nBlock 8 Figure7.StoringtheKVcacheoftworequestsatthesame\ntimeinvLLM.\nFigure6.BlocktabletranslationinvLLM.\nrequestsandthelatesttokensforgenerationphaserequests)\ndividesitintophysicalKVblocks(thisisalsodoneonCPU\nasonesequenceandfeedsitintotheLLM.DuringLLMâ€™s\nRAMforswapping;seeÂ§4.5).TheKVblockmanager also\ncomputation,vLLMusesthePagedAttentionkerneltoaccess\nmaintains block tablesâ€”the mapping between logical and\nthepreviousKVcachestoredintheformoflogicalKVblocks\nphysicalKVblocksofeachrequest.Eachblocktableentry\nandsavesthenewlygeneratedKVcacheintothephysical\nrecordsthecorrespondingphysicalblocksofalogicalblock\nKVblocks.StoringmultipletokenswithinaKVblock(block\nandthenumberoffilledpositions.Separatinglogicaland\nsize>1)enablesthePagedAttentionkerneltoprocessthe\nphysicalKVblocksallowsvLLMtodynamicallygrowthe\nKVcacheacrossmorepositionsinparallel,thusincreasing\nKVcachememorywithoutreservingitforallpositionsin\nthehardwareutilizationandreducinglatency.However,a\nadvance,whicheliminatesmostmemorywasteinexisting\nlargerblocksizealsoincreasesmemoryfragmentation.We\nsystems,asinFig.2.\nstudytheeffectofblocksizeinÂ§7.2.\n4.3 DecodingwithPagedAttentionandvLLM Again,vLLMdynamicallyassignsnewphysicalblocksto\nNext,wewalkthroughanexample,asinFig.6,todemon- logicalblocksasmoretokensandtheirKVcachearegener-\nstratehowvLLMexecutesPagedAttentionandmanagesthe ated.Asalltheblocksarefilledfromlefttorightandanew\nmemoryduringthedecodingprocessofasingleinputse- physical block is only allocated when all previous blocks\nquence:â—‹1 AsinOSâ€™svirtualmemory,vLLMdoesnotrequire are full, vLLMlimits all the memory wastes fora request\nreservingthememoryforthemaximumpossiblegenerated withinoneblock,soitcaneffectivelyutilizeallthememory,\nsequencelengthinitially.Instead,itreservesonlythenec- asshowninFig.2.Thisallowsmorerequeststofitintomem-\nessaryKVblockstoaccommodatetheKVcachegenerated oryforbatchingâ€”henceimprovingthethroughput.Oncea\nduringpromptcomputation.Inthiscase,Theprompthas7 requestfinishesitsgeneration,itsKVblockscanbefreedto\ntokens,sovLLMmapsthefirst2logicalKVblocks(0and storetheKVcacheofotherrequests.InFig.7,weshowan\n1) to 2 physical KV blocks (7 and 1, respectively). In the exampleofvLLMmanagingthememoryfortwosequences.\nprefillstep,vLLMgeneratestheKVcacheoftheprompts Thelogicalblocksofthetwosequencesaremappedtodiffer-\nandthefirstoutputtokenwithaconventionalself-attention entphysicalblockswithinthespacereservedbytheblock\nalgorithm(e.g.,[13]).vLLMthenstorestheKVcacheofthe engineinGPUworkers.Theneighboringlogicalblocksof\nfirst4tokensinlogicalblock0andthefollowing3tokens bothsequencesdonotneedtobecontiguousinphysicalGPU\nin logical block 1. The remaining slot is reserved for the memoryandthespaceofphysicalblockscanbeeffectively\nsubsequentautoregressivegenerationphase.â—‹2 Inthefirst utilizedbybothsequences.\nautoregressivedecodingstep,vLLMgeneratesthenewtoken\n4.4 ApplicationtoOtherDecodingScenarios\nwiththePagedAttentionalgorithmonphysicalblocks7and\nÂ§4.3showshowPagedAttentionandvLLMhandlebasicde-\n1.Sinceoneslotremainsavailableinthelastlogicalblock,\ncodingalgorithms,suchasgreedydecodingandsampling,\nthenewlygeneratedKVcacheisstoredthere,andtheblock\nthattakeoneuserpromptasinputandgenerateasingleout-\ntableâ€™s#filledrecordisupdated.â—‹3 Attheseconddecoding\nputsequence.InmanysuccessfulLLMapplications[18,34],\nstep,asthelastlogicalblockisfull,vLLMstoresthenewly\nanLLMservicemustoffermorecomplexdecodingscenarios\ngeneratedKVcacheinanewlogicalblock;vLLMallocatesa\nthatexhibitcomplexaccessingpatternsandmoreopportuni-\nnewphysicalblock(physicalblock3)foritandstoresthis\ntiesformemorysharing.Weshowthegeneralapplicability\nmappingintheblocktable.\nofvLLMontheminthissection.\nGlobally,foreachdecodingiteration,vLLMfirstselects\na set of candidate sequences for batching (more in Â§4.5), Parallelsampling.InLLM-basedprogramassistants[6,18],\nand allocates the physical blocks for the newly required anLLMgeneratesmultiplesampledoutputsforasinglein-\nlogicalblocks.Then,vLLMconcatenatesalltheinputtokens putprompt;userscanchooseafavoriteoutputfromvarious\nof the current iteration (i.e., all tokens for prompt phase candidates.Sofarwehaveimplicitlyassumedthatarequest\n6"
    },
    {
      "page_number": 7,
      "content": "Physical KV blocks\nSample Block 0 Ref count: 2 â†’ 1 Sample Beam candidate 0 Block 5 Block 9\nA1 Block 1 years ago our mothers A2\nLogical KV blocks Block 2Copy-on-write Logical KV blocks Beam candidate 1 Block 0 Block 1 Block 3 Block 6 Block 10\nBlock 0 Four score and seven Block 3 years ago our fathers Block 0 Four score and seven\nBlock 1 years ago our fathers Block 4 Block 1 years ago our mothers Beam candidate 2 Block 7 Block 11\nBlock 5\nBeam candidate 3 Block 2 Block 4 Block 8 Block 12\nBlock 6\nBlock 7 Four score and seven\nFigure9.Beamsearchexample.\nBlock 8\nFigure8.Parallelsamplingexample. samplespace.Thealgorithmreliesonthebeamwidth pa-\nrameterğ‘˜,whichdeterminesthenumberoftopcandidates\ngeneratesasinglesequence.Intheremainderofthispaper, retained at every step. During decoding, beam search ex-\nweassumethemoregeneralcaseinwhicharequestgener- pandseachcandidatesequenceinthebeambyconsidering\natesmultiplesequences.Inparallelsampling,onerequest allpossibletokens,computestheirrespectiveprobabilitiesus-\nincludesmultiplesamplessharingthesameinputprompt, ingtheLLM,andretainsthetop-ğ‘˜ mostprobablesequences\nallowingtheKVcacheoftheprompttobesharedaswell.Via outofğ‘˜ Â·|ğ‘‰|candidates,where|ğ‘‰|isthevocabularysize.\nitsPagedAttentionandpagedmemorymanagement,vLLM Unlikeparalleldecoding,beamsearchfacilitiessharing\ncanrealizethissharingeasilyandsavememory. notonlytheinitialpromptblocksbutalsootherblocksacross\nFig.8showsanexampleofparalleldecodingfortwoout- differentcandidates,andthesharingpatternsdynamically\nputs. Since both outputs share the same prompt, we only changeasthedecodingprocessadvances,similartothepro-\nreservespaceforonecopyofthepromptâ€™sstateattheprompt cesstreeintheOScreatedbycompoundforks.Fig.9shows\nphase;thelogicalblocksforthepromptsofbothsequences how vLLM manages the KV blocks for a beam search ex-\naremappedtothesamephysicalblocks:thelogicalblock0 ample withğ‘˜ = 4. Prior to the iteration illustrated as the\nand1ofbothsequencesaremappedtophysicalblocks7and dotted line, each candidate sequence has used 4 full logi-\n1,respectively.Sinceasinglephysicalblockcanbemapped calblocks.Allbeamcandidatessharethefirstblock0(i.e.,\ntomultiplelogicalblocks,weintroduceareferencecount for prompt).Candidate3digressesfromothersfromthesecond\neach physical block. In this case, the reference counts for block.Candidates0-2sharethefirst3blocksanddivergeat\nphysicalblocks7and1areboth2.Atthegenerationphase, thefourthblock.Atsubsequentiterations,thetop-4prob-\nthe two outputs sample different output tokens and need able candidates all originate from candidates 1 and 2. As\nseparatestorageforKVcache.vLLMimplementsacopy-on- the original candidates 0 and 3 are no longer among the\nwritemechanismattheblockgranularityforthephysical topcandidates,theirlogicalblocksarefreed,andtherefer-\nblocksthatneedmodificationbymultiplesequences,similar encecountsofcorrespondingphysicalblocksarereduced.\ntothecopy-on-writetechniqueinOSvirtualmemory(e.g., vLLMfreesallphysicalblockswhosereferencecountsreach\nwhenforkingaprocess).Specifically,inFig.8,whensample 0(blocks2,4,5,8).Then,vLLMallocatesnewphysicalblocks\nA1needstowritetoitslastlogicalblock(logicalblock1), (blocks9-12)tostorethenewKVcachefromthenewcan-\nvLLMrecognizesthatthereferencecountofthecorrespond- didates.Now,allcandidatesshareblocks0,1,3;candidates\ning physical block (physical block 1) is greater than 1; it 0and1shareblock6,andcandidates2and3furthershare\nallocatesanewphysicalblock(physicalblock3),instructs block7.\ntheblockenginetocopytheinformationfromphysicalblock PreviousLLMservingsystemsrequirefrequentmemory\n1,anddecreasesthereferencecountto1.Next,whensample copiesoftheKVcacheacrossthebeamcandidates.Forexam-\nA2writestophysicalblock1,thereferencecountisalready ple,inthecaseshowninFig.9,afterthedottedline,candidate\nreducedto1;thusA2directlywritesitsnewlygeneratedKV 3wouldneedtocopyalargeportionofcandidate2â€™sKV\ncachetophysicalblock1. cachetocontinuegeneration.Thisfrequentmemorycopy\nIn summary, vLLM enables the sharing of most of the overheadissignificantlyreducedbyvLLMâ€™sphysicalblock\nspaceusedtostorethepromptsâ€™KVcacheacrossmultiple sharing.InvLLM,mostblocksofdifferentbeamcandidates\noutputsamples,withtheexceptionofthefinallogicalblock, canbeshared.Thecopy-on-writemechanismisappliedonly\nwhichismanagedbyacopy-on-writemechanism.Bysharing whenthenewlygeneratedtokensarewithinanoldshared\nphysicalblocksacrossmultiplesamples,memoryusagecan block,asinparalleldecoding.Thisinvolvesonlycopying\nbegreatlyreduced,especiallyforlonginputprompts. oneblockofdata.\nBeamsearch.InLLMtaskslikemachinetranslation[59], Sharedprefix.Commonly,theLLMuserprovidesa(long)\ntheusersexpectthetop-ğ‘˜ mostappropriatetranslationsout- descriptionofthetaskincludinginstructionsandexample\nputbytheLLM.Beamsearch[49]iswidelyusedtodecode inputsandoutputs,alsoknownassystemprompt [36].The\nthemostprobableoutputsequencefromanLLM,asitmiti- descriptionisconcatenatedwiththeactualtaskinputtoform\ngatesthecomputationalcomplexityoffullytraversingthe thepromptoftherequest.TheLLMgeneratesoutputsbased\n7"
    },
    {
      "page_number": 8,
      "content": "Sequence A Sequence B context:(1)Whichblocksshoulditevict?(2)Howtorecover\nPrompt Prompt\nevictedblocksifneededagain?Typically,evictionpolicies\nTranslate English to French: Translate English to French:\nShared prefix â€œ â€œ s p e e a p p o e tt r e m râ€ in = tâ€ > = â€œ > lo u â€œm tre e n d t e h e m p e o r i â€ vrÃ©eâ€ â€œ â€œ s p e e a p p o e tt r e m râ€ in = tâ€ > = â€œ > lo u â€œm tre e n d t e h e m p e o r i â€ vrÃ©eâ€ useheuristicstopredictwhichblockwillbeaccessedfur-\nâ€œplush girafeâ€ => â€œgirafe en pelucheâ€ â€œplush girafeâ€ => â€œgirafe en pelucheâ€\nthestinthefutureandevictthatblock.Sinceinourcasewe\nTask input â€œcheeseâ€ => â€œI love youâ€ =>\nknowthatallblocksofasequenceareaccessedtogether,we\nSequence A Sequence B\nLLM output LLM output implementanall-or-nothingevictionpolicy,i.e.,eitherevict\nTask output â€œfromageâ€ â€œJe tâ€™amieâ€\nallornoneoftheblocksofasequence.Furthermore,multi-\nFigure10.Sharedpromptexampleformachinetranslation. plesequenceswithinonerequest(e.g.,beamcandidatesin\nTheexamplesareadoptedfrom[5]. onebeamsearchrequest)aregang-scheduledasasequence\ngroup.Thesequenceswithinonesequencegrouparealways\nonthefullprompt.Fig.10showsanexample.Moreover,the preemptedorrescheduledtogetherduetopotentialmemory\nsharedprefixcanbefurthertuned,viapromptengineering, sharingacrossthosesequences.Toanswerthesecondques-\ntoimprovetheaccuracyofthedownstreamtasks[26,27]. tion of how to recover an evicted block, we consider two\nForthistypeofapplication,manyuserpromptssharea techniques:\nprefix,thustheLLMserviceprovidercanstoretheKVcache\nSwapping.Thisistheclassictechniqueusedbymostvirtual\noftheprefixinadvancetoreducetheredundantcomputa-\nmemoryimplementationswhichcopytheevictedpagestoa\ntionspentontheprefix.InvLLM,thiscanbeconveniently\nswapspaceonthedisk.Inourcase,wecopyevictedblocksto\nachievedbyreservingasetofphysicalblocksforasetof\ntheCPUmemory.AsshowninFig.4,besidestheGPUblock\npredefinedsharedprefixesbytheLLMserviceprovider,as\nallocator,vLLMincludesaCPUblockallocatortomanage\nhowOShandlessharedlibraryacrossprocesses.Auserin-\nthe physical blocks swapped to CPU RAM. When vLLM\nputpromptwiththesharedprefixcansimplymapitslogi-\nexhaustsfreephysicalblocksfornewtokens,itselectsaset\ncalblockstothecachedphysicalblocks(withthelastblock\nofsequencestoevictandtransfertheirKVcachetotheCPU.\nmarkedcopy-on-write).Thepromptphasecomputationonly\nOnce it preempts a sequence and evicts its blocks, vLLM\nneedstoexecuteontheuserâ€™staskinput.\nstopsacceptingnewrequestsuntilallpreemptedsequences\nMixed decoding methods. The decoding methods dis- arecompleted.Oncearequestcompletes,itsblocksarefreed\ncussedearlierexhibitdiversememorysharingandaccess- frommemory,andtheblocksofapreemptedsequenceare\ningpatterns.Nonetheless,vLLMfacilitatesthesimultane- broughtbackintocontinuetheprocessingofthatsequence.\nousprocessingofrequestswithdifferentdecodingprefer- Notethatwiththisdesign,thenumberofblocksswappedto\nences,whichexistingsystemscannot efficientlydo.Thisis theCPURAMneverexceedsthenumberoftotalphysical\nbecause vLLM conceals the complex memory sharing be- blocksintheGPURAM,sotheswapspaceontheCPURAM\ntweendifferentsequencesviaacommonmappinglayerthat isboundedbytheGPUmemoryallocatedfortheKVcache.\ntranslates logical blocks to physical blocks. The LLM and\nRecomputation.Inthiscase,wesimplyrecomputetheKV\nits execution kernel only see a list of physical block IDs\ncachewhenthepreemptedsequencesarerescheduled.Note\nforeachsequenceanddonotneedtohandlesharingpat-\nthatrecomputationlatencycanbesignificantlylowerthan\nternsacrosssequences.Comparedtoexistingsystems,this\nthe original latency, as the tokens generated at decoding\napproachbroadensthebatchingopportunitiesforrequests\ncanbeconcatenatedwiththeoriginaluserpromptasanew\nwithdifferentsamplingrequirements,ultimatelyincreasing\npromptâ€”theirKVcacheatallpositionscanbegeneratedin\nthesystemâ€™soverallthroughput.\nonepromptphaseiteration.\n4.5 SchedulingandPreemption Theperformancesofswappingandrecomputationdepend\nonthebandwidthbetweenCPURAMandGPUmemoryand\nWhen the request traffic surpasses the systemâ€™s capacity,\nthecomputationpoweroftheGPU.Weexaminethespeeds\nvLLMmustprioritizeasubsetofrequests.InvLLM,weadopt\nofswappingandrecomputationinÂ§7.3.\nthe first-come-first-serve (FCFS) scheduling policy for all\nrequests,ensuringfairnessandpreventingstarvation.When\n4.6 DistributedExecution\nvLLMneedstopreemptrequests,itensuresthattheearliest\narrivedrequestsareservedfirstandthelatestrequestsare ManyLLMshaveparametersizesexceedingthecapacityofa\npreemptedfirst. singleGPU[5,9].Therefore,itisnecessarytopartitionthem\nLLMservicesfaceauniquechallenge:theinputprompts acrossdistributedGPUsandexecutetheminamodelparallel\nforanLLMcanvarysignificantlyinlength,andtheresulting fashion[28,63].Thiscallsforamemorymanagercapableof\noutputlengthsarenotknownapriori,contingentonboth handlingdistributedmemory.vLLMiseffectiveindistributed\ntheinputpromptandthemodel.Asthenumberofrequests settingsbysupportingthewidelyusedMegatron-LMstyle\nandtheiroutputsgrow,vLLMcanrunoutoftheGPUâ€™sphys- tensormodelparallelismstrategyonTransformers[47].This\nical blocks to store the newly generated KV cache. There strategyadherestoanSPMD(SingleProgramMultipleData)\naretwoclassicquestionsthatvLLMneedstoanswerinthis executionschedule,whereinthelinearlayersarepartitioned\n8"
    },
    {
      "page_number": 9,
      "content": "Table1.Modelsizesandserverconfigurations.\n2.0\nModelsize 13B 66B 175B 1.5\nGPUs A100 4Ã—A100 8Ã—A100-80GB\n1.0\nTotalGPUmemory 40GB 160GB 640GB\nParametersize 26GB 132GB 346GB 0.5\nMemoryforKVcache 12GB 21GB 264GB\n0.0\nMax.#KVcacheslots 15.7K 9.7K 60.1K 0 500 1000 1500 2000\n# Tokens\nto perform block-wise matrix multiplication, and the the\nGPUsconstantlysynchronizeintermediateresultsviaanall-\nreduceoperation.Specifically,theattentionoperatorissplit\nontheattentionheaddimension,eachSPMDprocesstakes\ncareofasubsetofattentionheadsinmulti-headattention.\nWeobservethatevenwithmodelparallelexecution,each\nmodelshardstillprocessesthesamesetofinputtokens,thus\nrequiringtheKVCacheforthesamepositions.Therefore,\nvLLMfeaturesasingleKVcachemanagerwithinthecen-\ntralizedscheduler,asinFig.4.DifferentGPUworkersshare\nthemanager,aswellasthemappingfromlogicalblocksto\nphysicalblocks.ThiscommonmappingallowsGPUworkers\ntoexecutethemodelwiththephysicalblocksprovidedby\ntheschedulerforeachinputrequest.AlthougheachGPU\nworkerhasthesamephysicalblockIDs,aworkeronlystores\naportionoftheKVcacheforitscorrespondingattention\nheads.\nIneachstep,theschedulerfirstpreparesthemessagewith\ninputtokenIDsforeachrequestinthebatch,aswellasthe\nblocktableforeachrequest.Next,theschedulerbroadcasts\nthiscontrolmessagetotheGPUworkers.Then,theGPU\nworkersstarttoexecutethemodelwiththeinputtokenIDs.\nIntheattentionlayers,theGPUworkersreadtheKVcache\naccordingtotheblocktableinthecontrolmessage.During\nexecution,theGPUworkerssynchronizetheintermediate\nresultswiththeall-reducecommunicationprimitivewithout\nthecoordinationofthescheduler,asin[47].Intheend,the\nGPUworkerssendthesampledtokensofthisiterationback\nto the scheduler. In summary, GPU workers do not need\ntosynchronizeonmemorymanagementastheyonlyneed\ntoreceiveallthememorymanagementinformationatthe\nbeginning of each decoding iteration along with the step\ninputs.\n5 Implementation\nvLLMisanend-to-endservingsystemwithaFastAPI[15]\nfrontendandaGPU-basedinferenceengine.Thefrontend\nextends the OpenAI API [34] interface, allowing users to\ncustomize sampling parameters for each request, such as\nthemaximumsequencelengthandthebeamwidthğ‘˜.The\nvLLMengineiswrittenin8.5KlinesofPythonand2Klinesof\nC++/CUDAcode.Wedevelopcontrol-relatedcomponentsin-\ncludingtheschedulerandtheblockmanagerinPythonwhile\ndevelopingcustomCUDAkernelsforkeyoperationssuchas\nPagedAttention.Forthemodelexecutor,weimplementpop-\nularLLMssuchasGPT[5],OPT[62],andLLaMA[52]using\nytisneD\n1eâˆ’2 8\nInput (mean: 161.31)\nOutput (mean: 337.99) 6\n4\n2\n0\n0 500 1000 1500 2000\n# Tokens\n(a)ShareGPT\nytisneD\n1eâˆ’2\nInput (mean: 19.31)\nOutput (mean: 58.45)\n(b)Alpaca\nFigure11.Inputandoutputlengthdistributionsofthe(a)\nShareGPTand(b)Alpacadatasets.\nPyTorch[39]andTransformers[58].WeuseNCCL[32]for\ntensorcommunicationacrossthedistributedGPUworkers.\n5.1 Kernel-levelOptimization\nSincePagedAttentionintroducesmemoryaccesspatterns\nthatarenotefficientlysupportedbyexistingsystems,we\ndevelopseveralGPUkernelsforoptimizingit.(1)Fusedre-\nshapeandblockwrite.IneveryTransformerlayer,thenew\nKVcachearesplitintoblocks,reshapedtoamemorylayout\noptimizedforblockread,thensavedatpositionsspecified\nbytheblocktable.Tominimizekernellaunchoverheads,we\nfusethemintoasinglekernel.(2)Fusingblockreadandatten-\ntion.WeadapttheattentionkernelinFasterTransformer[31]\ntoreadKVcacheaccordingtotheblocktableandperform\nattentionoperationsonthefly.Toensurecoalescedmemory\naccess, we assign a GPU warp to read each block. More-\nover,weaddsupportforvariablesequencelengthswithina\nrequestbatch.(3)Fusedblockcopy.Blockcopyoperations,\nissued by the copy-on-write mechanism, may operate on\ndiscontinuousblocks.Thiscanleadtonumerousinvocations\nof small data movements if we use the cudaMemcpyAsync\nAPI.Tomitigatetheoverhead,weimplementakernelthat\nbatchesthecopyoperationsfordifferentblocksintoasingle\nkernellaunch.\n5.2 SupportingVariousDecodingAlgorithms\nvLLMimplementsvariousdecodingalgorithmsusingthree\nkey methods: fork, append, and free. The fork method\ncreatesanewsequencefromanexistingone.Theappend\nmethodappendsanewtokentothesequence.Finally,the\nfree method deletes the sequence. For instance, in paral-\nlelsampling,vLLMcreatesmultipleoutputsequencesfrom\nthe single input sequence using the fork method. It then\naddsnewtokenstothesesequencesineveryiterationwith\nappend,anddeletessequencesthatmeetastoppingcondi-\ntionusingfree.Thesamestrategyisalsoappliedinbeam\nsearchandprefixsharingbyvLLM.Webelievefuturedecod-\ningalgorithmscanalsobesupportedbycombiningthese\nmethods.\n6 Evaluation\nInthissection,weevaluatetheperformanceofvLLMunder\navarietyofworkloads.\n9"
    },
    {
      "page_number": 10,
      "content": "1.0 1.0 1.0\n0.5 0.5 0.5\n0.0 0.0 0.0\n0.0 0.5 1.0 1.5 2.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 2.5\nRequest rate (req/s) Request rate (req/s) Request rate (req/s)\n(a) OPT-13B, 1 GPU, ShareGPT (b) OPT-66B, 4 GPUs, ShareGPT (c) OPT-175B, 8 GPUs, ShareGPT\nycnetal\ndezilamroN\n)nekot/s(\nFasterTransformer Orca (Max) Orca (Pow2) Orca (Oracle) vLLM\n1.0 1.0 1.0\n0.5 0.5 0.5\n0.0 0.0 0.0\n0 10 20 30 0 5 10 15 20 0 5 10 15 20\nRequest rate (req/s) Request rate (req/s) Request rate (req/s)\n(d) OPT-13B, 1 GPU, Alpaca (e) OPT-66B, 4 GPUs, Alpaca (f) OPT-175B, 8 GPUs, Alpaca\nycnetal\ndezilamroN\n)nekot/s(\nFigure12.SinglesequencegenerationwithOPTmodelsontheShareGPTandAlpacadataset\n35\n30\n25\n20\n15\n10\n5\n0\nOrca Orca Orca vLLM\n(Max) (Pow2) (Oracle)\nstseuqer\ndehctaB\n#\n150 30.42\n125\n100\n75\n13.62\n9.81 50 7.00\n25\n0\nOrca Orca Orca vLLM\n(Max) (Pow2) (Oracle)\n(a)ShareGPT\nstseuqer\ndehctaB\n#\nAsFasterTransformerdoesnothaveitsownscheduler,we 132.44\nimplement a custom scheduler with a dynamic batching\nmechanismsimilartotheexistingservingsystemssuchas\n72.75 Triton[30].Specifically,wesetamaximumbatchsizeğµas\n43.24 largeaspossibleforeachexperiment,accordingtotheGPU\n7.00 memorycapacity.Theschedulertakesuptoğµ numberof\nearliestarrivedrequestsandsendsthebatchtoFasterTrans-\n(b)Alpaca formerforprocessing.\nFigure13.Averagenumberofbatchedrequestswhenserv- Baseline2:Orca.Orca[60]isastate-of-the-artLLMserving\ning OPT-13B for the ShareGPT (2 reqs/s) and Alpaca (30 systemoptimizedforthroughput.SinceOrcaisnotpublicly\nreqs/s)traces. availableforuse,weimplementourownversionofOrca.We\nassumeOrcausesthebuddyallocationalgorithmtodeter-\n6.1 ExperimentalSetup\nminethememoryaddresstostoreKVcache.Weimplement\nModelandserverconfigurations.WeuseOPT[62]mod- threeversionsofOrcabasedonhowmuchitover-reserves\nelswith13B,66B,and175BparametersandLLaMA[52]with thespaceforrequestoutputs:\n13Bparametersforourevaluation.13Band66Barepopular â€¢ Orca(Oracle).Weassumethesystemhastheknowledge\nsizesforLLMsasshowninanLLMleaderboard[38],while ofthelengthsoftheoutputsthatwillbeactuallygenerated\n175BisthesizeofthefamousGPT-3[5]model.Forallof fortherequests.Thisshowstheupper-boundperformance\nourexperiments,weuseA2instanceswithNVIDIAA100 ofOrca,whichisinfeasibletoachieveinpractice.\nGPUsonGoogleCloudPlatform.Thedetailedmodelsizes â€¢ Orca(Pow2).Weassumethesystemover-reservesthe\nandserverconfigurationsareshowninTable1. spaceforoutputsbyatmost2Ã—.Forexample,ifthetrue\noutputlengthis25,itreserves32positionsforoutputs.\nWorkloads.WesynthesizeworkloadsbasedonShareGPT[51]\nâ€¢ Orca(Max).Weassumethesystemalwaysreservesthe\nandAlpaca[50]datasets,whichcontaininputandoutput\nspaceuptothemaximumsequencelengthofthemodel,\ntextsofrealLLMservices.TheShareGPTdatasetisacollec-\ni.e.,2048tokens.\ntionofuser-sharedconversationswithChatGPT[35].The\nAlpacadatasetisaninstructiondatasetgeneratedbyGPT- Keymetrics.Wefocusonservingthroughput.Specifically,\n3.5withself-instruct[57].Wetokenizethedatasetsanduse usingtheworkloadswithdifferentrequestrates,wemea-\ntheirinputandoutputlengthstosynthesizeclientrequests. surenormalizedlatency ofthesystems,themeanofevery\nAsshowninFig.11,theShareGPTdatasethas8.4Ã—longer requestâ€™s end-to-end latency divided by its output length,\ninputpromptsand5.8Ã—longeroutputsonaveragethanthe asinOrca[60].Ahigh-throughputservingsystemshould\nAlpacadataset,withhighervariance.Sincethesedatasetsdo retain low normalized latency against high request rates.\nnotincludetimestamps,wegeneraterequestarrivaltimes Formostexperiments,weevaluatethesystemswith1-hour\nusingPoissondistributionwithdifferentrequestrates. traces. As an exception, we use 15-minute traces for the\nOPT-175Bmodelduetothecostlimit.\nBaseline1:FasterTransformer.FasterTransformer[31]is\nadistributedinferenceenginehighlyoptimizedforlatency.\n10"
    },
    {
      "page_number": 11,
      "content": "1.0 1.0 1.0\n0.5 0.5 0.5\n0.0 0.0 0.0\n0 5 10 15 0 2 4 6 8 10 0 2 4 6\nRequest rate (req/s) Request rate (req/s) Request rate (req/s)\n(a) parallel generation (parallel size = 2) (b) parallel generation (parallel size = 4) (c) parallel generation (parallel size = 6)\nycnetal\ndezilamroN\n)nekot/s(\nOrca (Max) Orca (Pow2) Orca (Oracle) vLLM\n1.0 1.0 1.0\n0.5 0.5 0.5\n0.0 0.0 0.0\n0 5 10 15 0 2 4 6 8 10 0 2 4 6\nRequest rate (req/s) Request rate (req/s) Request rate (req/s)\n(d) beam search (beam width = 2) (e) beam search (beam width = 4) (f) beam search (beam width = 6)\nycnetal\ndezilamroN\n)nekot/s(\nFigure14.ParallelgenerationandbeamsearchwithOPT-13BontheAlpacadataset.\n6.2 BasicSampling 12\n8\nWeevaluatetheperformanceofvLLMwithbasicsampling 4\n(onesampleperrequest)onthreemodelsandtwodatasets.\n0\nThefirstrowofFig.12showstheresultsontheShareGPT 2 4 6\n# Output sequences\ndataset. The curves illustrate that as the request rate in-\ncreases,thelatencyinitiallyincreasesatagradualpacebut\nthensuddenlyexplodes.Thiscanbeattributedtothefact\nthatwhentherequestratesurpassesthecapacityoftheserv-\ningsystem,thequeuelengthcontinuestogrowinfinitely\nandsodoesthelatencyoftherequests.\nOntheShareGPTdataset,vLLMcansustain1.7Ã—â€“2.7Ã—\nhigherrequestratescomparedtoOrca(Oracle)and2.7Ã—â€“8Ã—\ncomparedtoOrca(Max),whilemaintainingsimilarlaten-\ncies.ThisisbecausevLLMâ€™sPagedAttentioncanefficiently\nmanagethememoryusageandthusenablebatchingmore\nrequeststhanOrca.Forexample,asshowninFig.13a,for\nOPT-13BvLLMprocesses2.2Ã—morerequestsatthesame\ntimethanOrca(Oracle)and4.3Ã—morerequeststhanOrca\n(Max).ComparedtoFasterTransformer,vLLMcansustainup\nto22Ã—higherrequestrates,asFasterTransformerdoesnot\nutilizeafine-grainedschedulingmechanismandinefficiently\nmanagesthememorylikeOrca(Max).\nThesecondrowofFig.12andFig.13bshowstheresults\nontheAlpacadataset,whichfollowsasimilartrendtothe\nShareGPTdataset.OneexceptionisFig.12(f),wherevLLMâ€™s\nadvantageoverOrca(Oracle)andOrca(Pow2)islesspro-\nnounced.Thisisbecausethemodelandserverconfiguration\nforOPT-175B(Table1)allowsforlargeGPUmemoryspace\navailable to store KV cache, while the Alpaca dataset has\nshortsequences.Inthissetup,Orca(Oracle)andOrca(Pow2)\ncanalsobatchalargenumberofrequestsdespitetheinef-\nficiencies in their memory management. As a result, the\nperformanceofthesystemsbecomescompute-boundrather\nthanmemory-bound.\n)%(\ngnivas\nyromeM\n9.79 60 8.53\n6.09 40\n20\n0\n2 4 6\nBeam width\n(a)Parallelsampling\n)%(\ngnivas\nyromeM\n53.13 55.16\n37.56\n(b)Beamsearch\nFigure15.Averageamountofmemorysavingfromsharing\nKVblocks,whenservingOPT-13BfortheAlpacatrace.\n6.3 ParallelSamplingandBeamSearch\nWeevaluatetheeffectivenessofmemorysharinginPage-\ndAttention with two popular sampling methods: parallel\nsamplingandbeamsearch.Inparallelsampling,allparal-\nlelsequencesinarequestcansharetheKVcacheforthe\nprompt.AsshowninthefirstrowofFig.14,withalarger\nnumberofsequencestosample,vLLMbringsmoreimprove-\nmentovertheOrcabaselines.Similarly,thesecondrowof\nFig.14showstheresultsforbeamsearchwithdifferentbeam\nwidths.Sincebeamsearchallowsformoresharing,vLLM\ndemonstrates even greater performance benefits. The im-\nprovementofvLLMoverOrca(Oracle)onOPT-13Bandthe\nAlpacadatasetgoesfrom1.3Ã—inbasicsamplingto2.3Ã—in\nbeamsearchwithawidthof6.\nFig.15plotstheamountofmemorysaving,computedby\nthenumberofblockswesavedbysharingdividedbythe\nnumberoftotalblockswithoutsharing.Weshow6.1%-9.8%\nmemorysavingonparallelsamplingand37.6%-55.2%on\nbeamsearch.InthesameexperimentswiththeShareGPT\ndataset, we saw 16.2% - 30.5% memory saving on parallel\nsamplingand44.3%-66.3%onbeamsearch.\n6.4 Sharedprefix\nWeexploretheeffectivenessofvLLMforthecaseaprefix\nis shared among different input prompts, as illustrated in\n11"
    },
    {
      "page_number": 12,
      "content": "1.0 1.0\n0.5 0.5\n0.0 0.0\n0 20 40 0 20 40\nRequest rate (req/s) Request rate (req/s)\n(a) 1-shot prefix prompt (b) 5-shot prefix prompt\nycnetal\ndezilamroN\n)nekot/s(\nOrca (Oracle) vLLM\nFigure16.Translationworkloadwheretheinputprompts\nshareacommonprefix.Theprefixincludes(a)1example\nwith80tokensor(b)5exampleswith341tokens.\n1.0\n0.5\n0.0\n0.0 0.2 0.4 0.6 0.8\nRequest rate (req/s)\nycnetal\ndezilamroN\n)nekot/s(\n250\n200\n150\n100\n50\n0\n64 128 256 Context length\nOrca (Max) Orca (Pow2) Orca (Oracle) vLLM\nFigure17.Performanceonchatbotworkload.\nFig.10.Forthemodel,weuseLLaMA-13B[52],whichismul-\ntilingual.Fortheworkload,weusetheWMT16[4]English-\nto-Germantranslationdatasetandsynthesizetwoprefixes\nthatincludeaninstructionandafewtranslationexamples.\nThe first prefix includes a single example (i.e., one-shot)\nwhiletheotherprefixincludes5examples(i.e.,few-shot).As\nshowninFig.16(a),vLLMachieves1.67Ã—higherthrough-\nputthanOrca(Oracle)whentheone-shotprefixisshared.\nFurthermore,whenmoreexamplesareshared(Fig.16(b)),\nvLLMachieves3.58Ã—higherthroughputthanOrca(Oracle).\n6.5 Chatbot\nAchatbot[8,19,35]isoneofthemostimportantapplications\nofLLMs.Toimplementachatbot,weletthemodelgenerate\na response by concatenating the chatting history and the\nlastuserqueryintoaprompt.Wesynthesizethechatting\nhistoryanduserqueryusingtheShareGPTdataset.Dueto\nthelimitedcontextlengthoftheOPT-13Bmodel,wecutthe\nprompttothelast1024tokensandletthemodelgenerate\natmost1024tokens.WedonotstoretheKVcachebetween\ndifferentconversationroundsasdoingthiswouldoccupythe\nspaceforotherrequestsbetweentheconversationrounds.\nFig.17showsthatvLLMcansustain2Ã—higherrequest\nratescomparedtothethreeOrcabaselines.SincetheShareGPT\ndatasetcontainsmanylongconversations,theinputprompts\nformostrequestshave1024tokens.Duetothebuddyallo-\ncationalgorithm,theOrcabaselinesreservethespacefor\n1024tokensfortherequestoutputs,regardlessofhowthey\npredicttheoutputlengths.Forthisreason,thethreeOrca\nbaselinesbehavesimilarly.Incontrast,vLLMcaneffectively\n)su(\nycnetal\nlenreK\nvLLM (bs 8) vLLM (bs 32)\nFT (bs 8) FT (bs 32) 17.5 15.0\n12.5\n10.0\n7.5 5.0\n2.5\n0.0\n1 2 4 8 16 32 64 128256 Block size\n(a)Latencyofattentionkernels.\n)nekot/s(\nycnetal\ndezilamroN\nShareGPT Alpaca\n(b)End-to-endlatencywithdif-\nferentblocksizes.\nFigure18.Ablationexperiments.\nhandle the long prompts, as PagedAttention resolves the\nproblemofmemoryfragmentationandreservation.\n7 AblationStudies\nInthissection,westudyvariousaspectsofvLLMandevalu-\natethedesignchoiceswemakewithablationexperiments.\n7.1 KernelMicrobenchmark\nThedynamicblockmappinginPagedAttentionaffectsthe\nperformanceoftheGPUoperationsinvolvingthestoredKV\ncache,i.e.,blockread/writesandattention.Comparedtothe\nexistingsystems,ourGPUkernels(Â§5)involveextraover-\nheadsofaccessingtheblocktable,executingextrabranches,\nandhandlingvariablesequencelengths.AsshowninFig.18a,\nthisleadsto20â€“26%higherattentionkernellatency,com-\nparedtothehighly-optimizedFasterTransformerimplemen-\ntation. We believe the overhead is small as it only affects\nthe attention operator but not the other operators in the\nmodel,suchasLinear.Despitetheoverhead,PagedAttention\nmakesvLLMsignificantlyoutperformFasterTransformerin\nend-to-endperformance(Â§6).\n7.2 ImpactofBlockSize\nThechoiceofblocksizecanhaveasubstantialimpactonthe\nperformanceofvLLM.Iftheblocksizeistoosmall,vLLM\nmaynotfullyutilizetheGPUâ€™sparallelismforreadingand\nprocessing KV cache. If the block size is too large, inter-\nnalfragmentationincreasesandtheprobabilityofsharing\ndecreases.\nInFig.18b,weevaluatetheperformanceofvLLMwithdif-\nferentblocksizes,usingtheShareGPTandAlpacatraceswith\nbasicsamplingunderfixedrequestrates.IntheShareGPT\ntrace,blocksizesfrom16to128leadtothebestperformance.\nIn the Alpaca trace, while the block size 16 and 32 work\nwell,largerblocksizessignificantlydegradetheperformance\nsincethesequencesbecomeshorterthantheblocksizes.In\npractice,wefindthattheblocksize16islargeenoughto\nefficientlyutilizetheGPUandsmallenoughtoavoidsignifi-\ncantinternalfragmentationinmostworkloads.Accordingly,\nvLLMsetsitsdefaultblocksizeas16.\n12"
    },
    {
      "page_number": 13,
      "content": "140\n120\n100 80\n60\n40\n20\n0\n1 2 4 8 16 32 64 128256\nBlock size\n)sm(\nemiT\nRecompute 2.5\nSwap in Swap out 2.0\nSwap in + out 1.5\n1.0\n0.5\n0.0\n1 2 4 8 16 32 64 128256\nBlock size\n(a)Microbenchmark\n)nekot/s(\nycnetal\ndezilamroN\nswap-outpolicy,whichexploitsthefactthatprocessinga\nRecompute\nSwap requestrequiresallofitscorrespondingtokenstatestobe\nstoredinGPUmemory.Anotherexampleistherecomputa-\ntionmethodtorecovertheevictedblocks,whichisnotfeasi-\nbleinOS.Besides,vLLMmitigatestheoverheadofmemory\nindirectioninpagingbyfusingtheGPUkernelsformemory\naccessoperationswiththoseforotheroperationssuchas\n(b)End-to-endperformance\nattention.\nFigure19.(a)Overheadofrecomputationandswappingfor\ndifferentblocksizes.(b)PerformancewhenservingOPT-13B 9 RelatedWork\nwiththeShareGPTtracesatthesamerequestrate.\nGeneralmodelservingsystems.Modelservinghasbeen\n7.3 ComparingRecomputationandSwapping anactiveareaofresearchinrecentyears,withnumerous\nsystemsproposedtotacklediverseaspectsofdeeplearning\nvLLMsupportsbothrecomputationandswappingasitsre-\nmodeldeployment.Clipper[11],TensorFlowServing[33],\ncoverymechanisms.Tounderstandthetradeoffsbetween\nNexus [45], InferLine [10], and Clockwork [20] are some\nthetwomethods,weevaluatetheirend-to-endperformance\nearlier general model serving systems. They study batch-\nandmicrobenchmarktheiroverheads,aspresentedinFig.19.\ning,caching,placement,andschedulingforservingsingle\nOurresultsrevealthatswappingincursexcessiveoverhead\nor multiple models. More recently, DVABatch [12] intro-\nwithsmallblocksizes.Thisisbecausesmallblocksizesoften\nducesmulti-entrymulti-exitbatching.REEF[21]andShep-\nresultinnumeroussmalldatatransfersbetweenCPUand\nherd[61]proposepreemptionforserving.AlpaServe[28]\nGPU,whichlimitstheeffectivePCIebandwidth.Incontrast,\nutilizesmodelparallelismforstatisticalmultiplexing.How-\ntheoverheadofrecomputationremainsconstantacrossdif-\never,thesegeneralsystemsfailtotakeintoaccounttheauto-\nferentblocksizes,asrecomputationdoesnotutilizetheKV\nregressivepropertyandtokenstateofLLMinference,result-\nblocks.Thus,recomputationismoreefficientwhentheblock\ninginmissedopportunitiesforoptimization.\nsizeissmall,whileswappingismoreefficientwhentheblock\nsizeislarge,thoughrecomputationoverheadisneverhigher Specializedservingsystemsfortransformers.Dueto\nthan20%ofswappingâ€™slatency.Formediumblocksizesfrom thesignificanceofthetransformerarchitecture,numerous\n16to64,thetwomethodsexhibitcomparableend-to-end specializedservingsystemsforithavebeendeveloped.These\nperformance. systemsutilizeGPUkerneloptimizations[1,29,31,56],ad-\nvancedbatchingmechanisms[14,60],modelparallelism[1,\n8 Discussion\n41, 60], and parameter sharing [64] for efficient serving.\nAmongthem,Orca[60]ismostrelevanttoourapproach.\nApplyingthevirtualmemoryandpagingtechniqueto\notherGPUworkloads.Theideaofvirtualmemoryand Comparison to Orca. The iteration-level scheduling in\npagingiseffectiveformanagingtheKVcacheinLLMserving Orca[60]andPagedAttentioninvLLMarecomplementary\nbecausetheworkloadrequiresdynamicmemoryallocation techniques: While both systems aim to increase the GPU\n(sincetheoutputlengthisnotknownapriori)anditsperfor- utilizationandhencethethroughputofLLMserving,Orca\nmanceisboundbytheGPUmemorycapacity.However,this achievesitbyschedulingandinterleavingtherequestsso\ndoesnotgenerallyholdforeveryGPUworkload.Forexam- thatmorerequestscanbeprocessedinparallel,whilevLLM\nple,inDNNtraining,thetensorshapesaretypicallystatic, is doing so by increasing memory utilization so that the\nandthusmemoryallocationcanbeoptimizedaheadoftime. workingsetsofmorerequestsfitintomemory.Byreducing\nForanotherexample,inservingDNNsthatarenotLLMs, memory fragmentation and enabling sharing, vLLM runs\nanincreaseinmemoryefficiencymaynotresultinanyper- more requests in a batch in parallel and achieves a 2-4Ã—\nformanceimprovementsincetheperformanceisprimarily speedupcomparedtoOrca.Indeed,thefine-grainedsched-\ncompute-bound.Insuchscenarios,introducingthevLLMâ€™s uling and interleaving of the requests like in Orca makes\ntechniquesmayratherdegradetheperformanceduetothe memorymanagementmorechallenging,makingthetech-\nextraoverheadofmemoryindirectionandnon-contiguous niquesproposedinvLLMevenmorecrucial.\nblockmemory.However,wewouldbeexcitedtoseevLLMâ€™s\nMemoryoptimizations.Thewideninggapbetweenthe\ntechniquesbeingappliedtootherworkloadswithsimilar\ncomputecapabilityandmemorycapacityofacceleratorshas\npropertiestoLLMserving.\ncaused memory to become a bottleneck for both training\nLLM-specificoptimizationsinapplyingvirtualmem- andinference.Swapping[23,42,55],recomputation[7,24]\noryandpaging.vLLMre-interpretsandaugmentstheidea andtheircombination[40]havebeenutilizedtoreducethe\nofvirtualmemoryandpagingbyleveragingtheapplication- peakmemoryoftraining.Notably,FlexGen[46]studieshow\nspecificsemantics.OneexampleisvLLMâ€™sall-or-nothing toswapweightsandtokenstatesforLLMinferencewith\n13"
    },
    {
      "page_number": 14,
      "content": "limitedGPUmemory,butitdoesnottargettheonlineserv- Joseph,GregBrockman,etal.2021.Evaluatinglargelanguagemodels\ningsettings.OLLA[48]optimizesthelifetimeandlocation trainedoncode.arXivpreprintarXiv:2107.03374(2021).\noftensorstoreducefragmentation,butitdoesnotdofine- [7] TianqiChen,BingXu,ChiyuanZhang,andCarlosGuestrin.2016.\nTraining deep nets with sublinear memory cost. arXiv preprint\ngrainedblock-levelmanagementoronlineserving.FlashAt-\narXiv:1604.06174(2016).\ntention[13]appliestilingandkerneloptimizationstoreduce\n[8] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,Hao\nthepeakmemoryofattentioncomputationandreduceI/O Zhang,LianminZheng,SiyuanZhuang,YonghaoZhuang,JosephE.\ncosts.Thispaperintroducesanewideaofblock-levelmem- Gonzalez,IonStoica,andEricP.Xing.2023.Vicuna:AnOpen-Source\norymanagementinthecontextofonlineserving. ChatbotImpressingGPT-4with90%*ChatGPTQuality. https://lmsys.\norg/blog/2023-03-30-vicuna/\n10 Conclusion [9] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,\nGauravMishra,AdamRoberts,PaulBarham,HyungWonChung,\nThispaperproposesPagedAttention,anewattentionalgo- CharlesSutton,SebastianGehrmann,etal.2022. Palm:Scalinglan-\nrithm that allows attention keys and values to be stored guagemodelingwithpathways.arXivpreprintarXiv:2204.02311(2022).\n[10] DanielCrankshaw,Gur-EyalSela,XiangxiMo,CoreyZumar,Ion\nin non-contiguous paged memory, and presents vLLM, a\nStoica,JosephGonzalez,andAlexeyTumanov.2020.InferLine:latency-\nhigh-throughputLLMservingsystemwithefficientmem-\nawareprovisioningandscalingforpredictionservingpipelines.In\nory management enabled by PagedAttention. Inspired by Proceedingsofthe11thACMSymposiumonCloudComputing.477â€“491.\noperatingsystems,wedemonstratehowestablishedtech- [11] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin,\nniques,suchasvirtualmemoryandcopy-on-write,canbe JosephEGonzalez,andIonStoica.2017. Clipper:ALow-Latency\nOnlinePredictionServingSystem.In14thUSENIXSymposiumon\nadaptedtoefficientlymanageKVcacheandhandlevarious\nNetworkedSystemsDesignandImplementation(NSDI17).613â€“627.\ndecodingalgorithmsinLLMserving.Ourexperimentsshow\n[12] WeihaoCui,HanZhao,QuanChen,HaoWei,ZiruiLi,DezeZeng,\nthatvLLMachieves2-4Ã—throughputimprovementsoverthe\nChaoLi,andMinyiGuo.2022. DVABatch:Diversity-awareMulti-\nstate-of-the-artsystems. EntryMulti-ExitBatchingforEfficientProcessingofDNNServices\nonGPUs.In2022USENIXAnnualTechnicalConference(USENIXATC\nAcknowledgement 22).183â€“198.\n[13] TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRÃ©.\nWewouldliketothankXiaoxuanLiu,ZhifengChen,Yan-\n2022.Flashattention:Fastandmemory-efficientexactattentionwith\npingHuang,anonymousSOSPreviewers,andourshepherd, io-awareness.AdvancesinNeuralInformationProcessingSystems35\nLidongZhou,fortheirinsightfulfeedback.Thisresearchis (2022),16344â€“16359.\npartlysupportedbygiftsfromAndreessenHorowitz,Anyscale, [14] JiaruiFang,YangYu,ChengduoZhao,andJieZhou.2021.TurboTrans-\nformers:anefficientGPUservingsystemfortransformermodels.In\nAstronomer,Google,IBM,Intel,Lacework,Microsoft,Mo-\nProceedingsofthe26thACMSIGPLANSymposiumonPrinciplesand\nhamedBinZayedUniversityofArtificialIntelligence,Sam-\nPracticeofParallelProgramming.389â€“402.\nsungSDS,Uber,andVMware. [15] FastAPI.2023.FastAPI.https://github.com/tiangolo/fastapi.\n[16] PinGao,LingfanYu,YongweiWu,andJinyangLi.2018.Lowlatency\nReferences rnninferencewithcellularbatching.InProceedingsoftheThirteenth\nEuroSysConference.1â€“15.\n[1] RezaYazdaniAminabadi,SamyamRajbhandari,MinjiaZhang,Am-\n[17] AmirGholami,ZheweiYao,SehoonKim,MichaelWMahoney,and\nmarAhmadAwan,ChengLi,DuLi,EltonZheng,JeffRasley,Shaden\nKurtKeutzer.2021.Aiandmemorywall.RiseLabMediumPost1(2021),\nSmith,OlatunjiRuwase,etal.2022.DeepSpeedInference:Enabling\n6.\nEfficientInferenceofTransformerModelsatUnprecedentedScale.\n[18] Github.2022. https://github.com/features/copilot\narXivpreprintarXiv:2207.00032(2022).\n[19] Google.2023. https://bard.google.com/\n[2] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton.2016.Layer\n[20] ArpanGujarati,RezaKarimi,SafyaAlzayat,WeiHao,AntoineKauf-\nnormalization.arXivpreprintarXiv:1607.06450(2016).\nmann,YmirVigfusson,andJonathanMace.2020.Serving{DNNs}like\n[3] YoshuaBengio,RÃ©jeanDucharme,andPascalVincent.2000.Aneural\nClockwork:PerformancePredictabilityfromtheBottomUp.In14th\nprobabilisticlanguagemodel.Advancesinneuralinformationprocess-\nUSENIXSymposiumonOperatingSystemsDesignandImplementation\ningsystems13(2000).\n(OSDI20).443â€“462.\n[4] OndrejBojar,RajenChatterjee,ChristianFedermann,YvetteGra-\n[21] Mingcong Han, Hanze Zhang, Rong Chen, and Haibo Chen.\nham,BarryHaddow,MatthiasHuck,AntonioJimenoYepes,Philipp\n2022. Microsecond-scale Preemption for Concurrent {GPU-\nKoehn,VarvaraLogacheva,ChristofMonz,MatteoNegri,Aurelie\naccelerated}{DNN}Inferences.In16thUSENIXSymposiumonOper-\nNeveol,MarianaNeves,MartinPopel,MattPost,RaphaelRubino,Car-\natingSystemsDesignandImplementation(OSDI22).539â€“558.\nolinaScarton,LuciaSpecia,MarcoTurchi,KarinVerspoor,andMarcos\n[22] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deep\nZampieri.2016.Findingsofthe2016ConferenceonMachineTrans-\nresiduallearningforimagerecognition.InProceedingsoftheIEEE\nlation.InProceedingsoftheFirstConferenceonMachineTranslation.\nconferenceoncomputervisionandpatternrecognition.770â€“778.\nAssociationforComputationalLinguistics,Berlin,Germany,131â€“198.\n[23] Chien-ChinHuang,GuJin,andJinyangLi.2020.Swapadvisor:Push-\nhttp://www.aclweb.org/anthology/W/W16/W16-2301\ningdeeplearningbeyondthegpumemorylimitviasmartswapping.\n[5] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredD\nInProceedingsoftheTwenty-FifthInternationalConferenceonArchi-\nKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,Girish\ntecturalSupportforProgrammingLanguagesandOperatingSystems.\nSastry,AmandaAskell,etal.2020. Languagemodelsarefew-shot\n1341â€“1355.\nlearners.Advancesinneuralinformationprocessingsystems33(2020),\n[24] ParasJain,AjayJain,AniruddhaNrusimha,AmirGholami,Pieter\n1877â€“1901.\nAbbeel,JosephGonzalez,KurtKeutzer,andIonStoica.2020.Check-\n[6] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePonde\nmate:Breakingthememorywallwithoptimaltensorrematerialization.\ndeOliveiraPinto,JaredKaplan,HarriEdwards,YuriBurda,Nicholas\n14"
    },
    {
      "page_number": 15,
      "content": "ProceedingsofMachineLearningandSystems2(2020),497â€“511. LanguageModelswithaSingleGPU.arXivpreprintarXiv:2303.06865\n[25] TomKilburn,DavidBGEdwards,MichaelJLanigan,andFrankH (2023).\nSumner.1962.One-levelstoragesystem.IRETransactionsonElectronic [47] MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,\nComputers2(1962),223â€“235. JaredCasper,andBryanCatanzaro.2019.Megatron-lm:Trainingmulti-\n[26] BrianLester,RamiAl-Rfou,andNoahConstant.2021. Thepower billionparameterlanguagemodelsusingmodelparallelism. arXiv\nof scale for parameter-efficient prompt tuning. arXiv preprint preprintarXiv:1909.08053(2019).\narXiv:2104.08691(2021). [48] BenoitSteiner,MostafaElhoushi,JacobKahn,andJamesHegarty.2022.\n[27] XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontin- OLLA:OptimizingtheLifetimeandLocationofArraystoReducethe\nuouspromptsforgeneration.arXivpreprintarXiv:2101.00190(2021). MemoryUsageofNeuralNetworks.(2022). https://doi.org/10.48550/\n[28] ZhuohanLi,LianminZheng,YinminZhong,VincentLiu,YingSheng, arXiv.2210.12924\nXinJin,YanpingHuang,ZhifengChen,HaoZhang,JosephEGonzalez, [49] IlyaSutskever,OriolVinyals,andQuocVLe.2014. Sequencetose-\netal.2023.AlpaServe:StatisticalMultiplexingwithModelParallelism quencelearningwithneuralnetworks.Advancesinneuralinformation\nforDeepLearningServing.arXivpreprintarXiv:2302.11665(2023). processingsystems27(2014).\n[29] LingxiaoMa,ZhiqiangXie,ZhiYang,JilongXue,YoushanMiao,Wei [50] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,Xuechen\nCui,WenxiangHu,FanYang,LintaoZhang,andLidongZhou.2020. Li,CarlosGuestrin,PercyLiang,andTatsunoriB.Hashimoto.2023.\nRammer:Enablingholisticdeeplearningcompileroptimizationswith StanfordAlpaca:AnInstruction-followingLLaMAmodel. https://\nrtasks.InProceedingsofthe14thUSENIXConferenceonOperating github.com/tatsu-lab/stanford_alpaca.\nSystemsDesignandImplementation.881â€“897. [51] ShareGPTTeam.2023. https://sharegpt.com/\n[30] NVIDIA.[n.d.].TritonInferenceServer.https://developer.nvidia.com/ [52] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-\nnvidia-triton-inference-server. AnneLachaux,TimothÃ©eLacroix,BaptisteRoziÃ¨re,NamanGoyal,Eric\n[31] NVIDIA. 2023. FasterTransformer. https://github.com/NVIDIA/ Hambro,FaisalAzhar,etal.2023.Llama:Openandefficientfoundation\nFasterTransformer. languagemodels.arXivpreprintarXiv:2302.13971(2023).\n[32] NVIDIA.2023.NCCL:TheNVIDIACollectiveCommunicationLibrary. [53] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,Llion\nhttps://developer.nvidia.com/nccl. Jones,AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017.At-\n[33] ChristopherOlston,NoahFiedel,KirilGorovoy,JeremiahHarmsen,Li tentionisallyouneed. Advancesinneuralinformationprocessing\nLao,FangweiLi,VinuRajashekhar,SukritiRamesh,andJordanSoyke. systems30(2017).\n2017. Tensorflow-serving:Flexible,high-performancemlserving. [54] JingWang,YouyouLu,QingWang,MinhuiXie,KejiHuang,andJiwu\narXivpreprintarXiv:1712.06139(2017). Shu.2022. Pacman:AnEfficientCompactionApproachfor {Log-\n[34] OpenAI.2020. https://openai.com/blog/openai-api Structured}{Key-Value}StoreonPersistentMemory.In2022USENIX\n[35] OpenAI.2022. https://openai.com/blog/chatgpt AnnualTechnicalConference(USENIXATC22).773â€“788.\n[36] OpenAI.2023. https://openai.com/blog/custom-instructions-for- [55] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuai-\nchatgpt wenLeonSong,ZenglinXu,andTimKraska.2018.Superneurons:Dy-\n[37] OpenAI.2023.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL] namicGPUmemorymanagementfortrainingdeepneuralnetworks.\n[38] LMSYSORG.2023. ChatbotArenaLeaderboardWeek8:Introduc- InProceedingsofthe23rdACMSIGPLANsymposiumonprinciplesand\ningMT-BenchandVicuna-33B. https://lmsys.org/blog/2023-06-22- practiceofparallelprogramming.41â€“53.\nleaderboard/. [56] XiaohuiWang,YingXiong,YangWei,MingxuanWang,andLeiLi.\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James 2021.LightSeq:AHighPerformanceInferenceLibraryforTransform-\nBradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia ers.InProceedingsofthe2021ConferenceoftheNorthAmericanChapter\nGimelshein,LucaAntiga,etal.2019. Pytorch:Animperativestyle, oftheAssociationforComputationalLinguistics:HumanLanguageTech-\nhigh-performancedeeplearninglibrary.Advancesinneuralinforma- nologies:IndustryPapers.113â€“120.\ntionprocessingsystems32(2019). [57] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahA\n[40] ShishirGPatil,ParasJain,PrabalDutta,IonStoica,andJosephGon- Smith,DanielKhashabi,andHannanehHajishirzi.2022.Self-Instruct:\nzalez.2022.POET:TrainingNeuralNetworksonTinyDeviceswith AligningLanguageModelwithSelfGeneratedInstructions. arXiv\nIntegratedRematerializationandPaging.InInternationalConference preprintarXiv:2212.10560(2022).\nonMachineLearning.PMLR,17573â€“17583. [58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\n[41] ReinerPope,SholtoDouglas,AakankshaChowdhery,JacobDevlin, ClementDelangue,AnthonyMoi,PierricCistac,TimRault,RÃ©miLouf,\nJamesBradbury,AnselmLevskaya,JonathanHeek,KefanXiao,Shivani MorganFuntowicz,etal.2020.Transformers:State-of-the-artnatural\nAgrawal,andJeffDean.2022.EfficientlyScalingTransformerInference. languageprocessing.InProceedingsofthe2020conferenceonempirical\narXivpreprintarXiv:2211.05102(2022). methodsinnaturallanguageprocessing:systemdemonstrations.38â€“45.\n[42] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji [59] YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,Mohammad\nRuwase,ShuangyanYang,MinjiaZhang,DongLi,andYuxiongHe. Norouzi,WolfgangMacherey,MaximKrikun,YuanCao,QinGao,\n2021.ZeRO-Offload:DemocratizingBillion-ScaleModelTraining..In KlausMacherey,etal.2016. Googleâ€™sneuralmachinetranslation\nUSENIXAnnualTechnicalConference.551â€“564. system:Bridgingthegapbetweenhumanandmachinetranslation.\n[43] Reuters.2023. https://www.reuters.com/technology/tech-giants-ai- arXivpreprintarXiv:1609.08144(2016).\nlike-bing-bard-poses-billion-dollar-search-problem-2023-02-22/ [60] Gyeong-InYu,JooSeongJeong,Geon-WooKim,SoojeongKim,and\n[44] AmazonWebServices.2023. https://aws.amazon.com/bedrock/ Byung-GonChun.2022. Orca:ADistributedServingSystemfor\n[45] HaichenShen,LequnChen,YuchenJin,LiangyuZhao,BingyuKong, {Transformer-Based}GenerativeModels.In16thUSENIXSymposium\nMatthaiPhilipose,ArvindKrishnamurthy,andRaviSundaram.2019. onOperatingSystemsDesignandImplementation(OSDI22).521â€“538.\nNexus:AGPUclusterengineforacceleratingDNN-basedvideoanal- [61] HongZhang,YupengTang,AnuragKhandelwal,andIonStoica.2023.\nysis.InProceedingsofthe27thACMSymposiumonOperatingSystems SHEPHERD:ServingDNNsintheWild.In20thUSENIXSymposiumon\nPrinciples.322â€“337. NetworkedSystemsDesignandImplementation(NSDI23).USENIXAs-\n[46] YingSheng,LianminZheng,BinhangYuan,ZhuohanLi,MaxRyabinin, sociation,Boston,MA,787â€“808. https://www.usenix.org/conference/\nDanielYFu,ZhiqiangXie,BeidiChen,ClarkBarrett,JosephEGon- nsdi23/presentation/zhang-hong\nzalez,etal.2023. High-throughputGenerativeInferenceofLarge\n15"
    },
    {
      "page_number": 16,
      "content": "[62] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen, EricPXing,etal.2022. Alpa:AutomatingInter-andIntra-Operator\nShuohuiChen,ChristopherDewan,MonaDiab,XianLi,XiVictoria ParallelismforDistributedDeepLearning.In16thUSENIXSymposium\nLin,etal.2022.Opt:Openpre-trainedtransformerlanguagemodels. onOperatingSystemsDesignandImplementation(OSDI22).559â€“578.\narXivpreprintarXiv:2205.01068(2022). [64] ZheZhou,XuechaoWei,JiejingZhang,andGuangyuSun.2022.PetS:\n[63] LianminZheng,ZhuohanLi,HaoZhang,YonghaoZhuang,Zhifeng AUnifiedFrameworkforParameter-EfficientTransformersServing.In\nChen,YanpingHuang,YidaWang,YuanzhongXu,DanyangZhuo, 2022USENIXAnnualTechnicalConference(USENIXATC22).489â€“504.\n16"
    }
  ]
}