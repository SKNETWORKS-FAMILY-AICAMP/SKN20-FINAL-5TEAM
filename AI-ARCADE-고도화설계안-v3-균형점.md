# AI-ARCADE 고도화 설계안 v3
> 작성일: 2026-02-20
> 핵심 원칙: 좋은 제품이 먼저, 기술은 수단. 단 기술 선택이 포트폴리오에도 어필되게.
> 검토 요청: 이 설계의 문제점, 대안, 개선 방향에 대한 비판적 평가를 요청합니다.

---

## 0. 설계 철학

### v1, v2와 다른 점
```
v1, v2: MCP + 멀티 에이전트를 먼저 설계 → 기술에 끌려다님
v3:     사용자 문제를 먼저 정의 → 거기에 기술을 자연스럽게 녹임
```

### 균형점 원칙
- 사용자(AI 엔지니어 초년생)의 실제 문제를 해결하는 게 1순위
- MCP, 멀티 에이전트는 그 해결책으로 들어가는 수단
- 단, 기술 선택의 이유를 면접에서 명확히 설명할 수 있어야 함

---

## 1. 프로젝트 현황

### 플랫폼 개요
- **AI-ARCADE**: AI 엔지니어 초년생 대상 게임형 학습 플랫폼
- **기술 스택**: Vue 3, Django REST, PostgreSQL, OpenAI API (gpt-4o-mini)
- **현재 Unit**
  - Unit 1: 의사코드 실습 (데이터 파이프라인 설계)
  - Unit 2: 버그헌트 (디버깅)
  - Unit 3: 시스템 아키텍처 설계

### 현재 구조의 한계
```
현재 흐름:
문제 제공 → 사용자 풀이 → 단일 LLM 평가 → 결과 표시 → 끝

사용자가 떠나는 이유:
"피드백 받았는데 왜 틀렸는지 모르겠다"
"다음에 뭘 해야 할지 모르겠다"
"내가 성장하고 있는지 모르겠다"
→ 평가 후 "그래서 어떻게 해야 하나"가 없음
```

---

## 2. 해결할 사용자 문제 3가지

```
문제 1: "피드백 받았는데 왜 틀렸는지 모르겠다"
→ 대화형 피드백 (Phase 1, 멀티 에이전트 적용)

문제 2: "다음에 뭘 해야 할지 모르겠다"
→ 학습 경로 추천 (Phase 2, MCP 적용)

문제 3: "내가 성장하고 있는지 모르겠다"
→ 진행감 시각화 (Phase 2, DB + 프론트)
```

기술이 아니라 문제가 먼저이고, 기술은 각 문제의 해결책으로 들어감.

---

## 3. Phase 1: 대화형 피드백 + 멀티 에이전트 (3~4주)

### 왜 여기에 멀티 에이전트인가

단순 챗봇이 아니라 "풀이 내용을 아는 AI가 여러 관점으로 대화"하는 게 핵심.
단일 LLM은 관점이 하나라 깊이 있는 대화가 어려움.

```
사용자: "왜 내 설계가 틀렸어?"

단일 LLM:
"edge_case 처리가 부족했습니다." (끝)

멀티 에이전트:
기술Agent: "null이 들어왔을 때 처리 로직이 없어요"
실무Agent: "이런 케이스가 실제 서비스 장애로 이어져요"
개선Agent: "이렇게 방어 코드를 추가해보세요"
→ 훨씬 입체적이고 실무적인 대화
```

### 구조

```
사용자 질문 입력
        ↓
Orchestrator
"어떤 관점의 답변이 필요한가 판단"
(기술 설명 / 실무 맥락 / 개선 방향)
        ↓
병렬 실행:
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│  기술Agent   │  │  실무Agent   │  │  개선Agent   │
│              │  │              │  │              │
│  왜 틀렸나   │  │  실제 영향이 │  │  어떻게      │
│  기술적 설명 │  │  뭔가        │  │  고칠까      │
└──────────────┘  └──────────────┘  └──────────────┘
        ↓
통합Agent
"세 관점을 자연스러운 대화로 통합"
        ↓
사용자에게 대화 형태로 표시
(멀티턴 지원, 풀이 컨텍스트 유지)
```

### 토큰 관리

```python
# 각 Agent 출력에 summary 필드 강제 포함
agent_output = {
    "full_result": "...",    # 내부 처리용
    "summary": "한 줄 요약"  # 다음 Agent 전달용
}

# 통합Agent에는 summary만 전달
통합Agent_input = {
    "기술": 기술Agent["summary"],   # ~100 토큰
    "실무": 실무Agent["summary"],   # ~100 토큰
    "개선": 개선Agent["summary"],   # ~100 토큰
    "user_question": "...",
    "user_submission": "..."        # 풀이 원문
}
# full_result 전달 X → 토큰 폭증 방지
```

### 파일럿 계획

```
대상: 10~20명 사용자
기간: 2~3주
측정:
- 대화 평균 턴 수 (많을수록 참여도 높음)
- 피드백 후 재도전율
- "이해됐다" 자가 평가 점수
```

### 포트폴리오 어필

```
"평가 후 대화형 피드백 시스템 구현
 - Orchestrator가 질문 의도를 파악해 Agent 선택
 - 기술/실무/개선 관점 Agent 병렬 실행
 - 풀이 컨텍스트를 유지한 멀티턴 대화 구현
 - summary 필드로 토큰 폭증 방지 (Agent 간 전달)"
```

---

## 4. Phase 2: 학습 경로 추천 + MCP (3~4주)

### 왜 여기에 MCP인가

학습 경로 추천에는 두 가지 데이터 소스가 필요함.
```
1. 이 사람의 풀이 이력 (내부 DB)
2. 개념 간 선행 관계 (지식 그래프)
```

에이전트가 상황에 따라 필요한 데이터 소스를 자율 선택하게 하는 것이 MCP의 실질적 이점.

```
Django 내부 함수로 하면:
→ 항상 DB + 지식 그래프 둘 다 조회
→ 불필요한 조회 발생

MCP로 하면:
→ 에이전트가 상황 판단
→ 이력이 충분하면 DB만 조회
→ 개념 gap이 의심되면 지식 그래프도 조회
→ 응답 시간 단축, 비용 절감
```

이것이 "왜 MCP를 썼냐"에 대한 면접 답변.

### MCP 서버 2개

**서버 1: AI-ARCADE DB MCP 서버**
```python
@mcp_tool
def get_user_history(user_id: int):
    """사용자 풀이 이력 + 약점 패턴 반환"""

@mcp_tool
def get_repeated_mistakes(user_id: int):
    """2회 이상 반복된 실수 유형 반환"""

@mcp_tool
def save_learning_path(user_id: int, path: dict):
    """추천된 학습 경로 저장"""
```

**서버 2: 지식 그래프 MCP 서버**
```python
@mcp_tool
def get_concept_prerequisites(concept: str):
    """개념의 선행 학습 항목 반환"""

@mcp_tool
def get_search_keywords(concept: str):
    """개념 공부를 위한 검색 키워드 반환"""

@mcp_tool
def get_next_concepts(concept: str):
    """이 개념 이해 후 다음 학습 단계 반환"""
```

지식 그래프 초안은 LLM으로 생성 후 관리자 검토:
```python
prompt = """
AI 엔지니어 초년생이 자주 겪는 약점 15개와
각 약점의 선행 개념, 검색 키워드, 다음 단계를
JSON으로 생성해줘.
Unit 1 (데이터 파이프라인), Unit 3 (시스템 아키텍처) 범위.
"""
# 결과를 knowledge_graph.json으로 저장
# 관리자 검토 후 사용
```

### 학습 경로 추천 구조

```
학습 경로 요청
        ↓
경로Agent
        ↓ MCP 자율 호출
┌─────────────────┐  ┌─────────────────┐
│ AI-ARCADE       │  │ 지식 그래프     │
│ DB MCP 서버     │  │ MCP 서버        │
│                 │  │                 │
│ 풀이 이력       │  │ 선행 개념       │
│ 반복 실수       │  │ 검색 키워드     │
│ 약점 패턴       │  │ 다음 단계       │
└─────────────────┘  └─────────────────┘
        ↓
사용자에게 표시:

📍 현재 위치
"Data Pipeline 설계는 이해하고 있지만
 예외 상황 처리 개념이 부족합니다"

🔍 왜 이런 약점이 생기는가
"예외처리를 에러 핸들링으로만 이해하고 있어서
 데이터 관점의 예외를 설계 단계에서 고려하지 않는 패턴"

📚 지금 필요한 것 (순서대로)
1. Defensive Programming 개념
2. 검색어: "Python data validation edge case"
3. 이해했다면 → Unit 1 재도전

⏭️ 이걸 마스터하면 다음은
→ Pipeline Validation 설계
```

### 진행감 시각화 (추가)

```
"지난주보다 edge_case 처리가 15% 향상됐어요"
"design 영역은 이제 마스터 수준이에요"
"여기까지 왔으면 Unit 3 도전해볼 수 있어요"
```

DB 쿼리 + 프론트 구현. LLM 호출 없이 가능.

### 포트폴리오 어필

```
"MCP 서버 2개 직접 설계 및 구현
 - AI-ARCADE DB MCP: 풀이 이력/약점 조회
 - 지식 그래프 MCP: 개념 선행 관계
 - 에이전트가 상황에 따라 필요한 도구 자율 선택
 - Django 단순 호출 대비 불필요한 조회 제거"
```

---

## 5. Phase 3: Self-improving 루프 (조건부 활성화)

### 비판 반영: 처음부터 활성화하지 않음

이전 설계안에서 지적된 문제:
- Bias Amplification: 쉬운 문제가 높은 점수 → 더 쉬운 문제 생성
- Mode Collapse: 특정 유형만 계속 생성
- 관리자 1명 검토는 rubber stamping

### 조건부 활성화 조건

```
Phase 1~2 운영 후:
✅ 사용자 100명 이상 데이터 축적
✅ 면접관 AI 평가 정확도 검증 완료
✅ 관리자 2명 이상 검토 체계 확립
✅ 메트릭 대시보드 완성

위 조건 충족 후에만 루프 활성화
```

### 다양성 강제 로직

```python
# Bias Amplification 방지
REQUIRED_DISTRIBUTION = {
    "edge_case": (0.25, 0.35),    # 25~35% 유지
    "design": (0.25, 0.35),
    "implementation": (0.15, 0.25),
    "abstraction": (0.15, 0.25)
}

def validate_problem_distribution(pool):
    for problem_type, (min_ratio, max_ratio) in REQUIRED_DISTRIBUTION.items():
        ratio = count(pool, problem_type) / len(pool)
        if not min_ratio <= ratio <= max_ratio:
            raise ValueError(f"{problem_type} 비율 이상: {ratio}")
            # 자동 생성 중단 → 관리자 알림
```

---

## 6. 평가 메트릭

Self-improving이 실제로 작동하는지, 제품이 좋아지는지 측정.

### 학습 효과 지표
```
- 재도전 시 점수 향상률 (목표: 평균 +15점)
- 같은 약점 반복 비율 (목표: 2회 이상 반복 < 30%)
- 대화형 피드백 후 이해도 자가 평가 (5점 만점)
```

### 시스템 품질 지표
```
- AI 면접관 평가 vs 사람 평가 일치율 (목표: > 80%)
- 문제 생성 관리자 승인률 (목표: > 70%)
- 평균 응답 시간 (목표: < 5초)
- 평가 1회당 비용 (목표: < $0.01)
```

### 사용자 행동 지표
```
- 피드백 대화 평균 턴 수 (목표: > 3턴)
- 플랫폼 주간 재방문율
- 문제 완료율
- 학습 경로 추천 클릭률
```

---

## 7. 비용 구조

gpt-4o-mini 기준 (input $0.15/1M, output $0.60/1M 토큰)

### Phase 1: 대화형 피드백 (사용자 1회 대화당)
```
Orchestrator:    ~500 토큰
Agent 3개 병렬:  ~4,500 토큰
통합Agent:       ~1,000 토큰
합계:            ~6,000 토큰
비용:            약 $0.005 / 대화
```

### Phase 2: 학습 경로 추천 (1회당)
```
경로Agent + MCP 조회: ~3,000 토큰
비용: 약 $0.003 / 추천
```

### 스케일업 시나리오
```
사용자 100명, 1인당 하루 2회 대화:
→ $0.005 × 200 = $1 / 일 → 월 $30

사용자 1,000명:
→ 월 $300

비용 최적화:
- 동일 패턴 응답 캐싱 → 호출 40% 절감
- Orchestrator는 gpt-4o-mini, 깊은 분석만 gpt-4o
- 학습 경로는 24시간 캐싱 (자주 바뀌지 않음)
```

---

## 8. 구현 로드맵

### Phase 1 (3~4주): 대화형 피드백
```
1주: 멀티 에이전트 설계 + 프롬프트 튜닝
2주: Orchestrator + 병렬 실행 구현
3주: 대화 UI 연동 (Vue 컴포넌트)
4주: 파일럿 10~20명, 메트릭 수집
```

### Phase 2 (3~4주): 학습 경로 + MCP
```
1주: MCP 서버 2개 구현 + 테스트
2주: 지식 그래프 초안 생성 + 관리자 검토
3주: 경로Agent + MCP 연결
4주: 진행감 시각화 UI + 검증
```

### Phase 3 (조건 충족 후): Self-improving
```
조건 충족 확인
→ 다양성 강제 로직 구현
→ 관리자 2명 검토 체계 확립
→ 점진적 활성화
```

---

## 9. 이전 설계안 비판 반영 내역

| 비판 내용 | 반영 여부 | 반영 방법 |
|----------|----------|----------|
| MCP over-engineering | 부분 반영 | MCP가 실질적으로 필요한 곳(학습 경로)에만 적용. Django로 충분한 곳은 MCP 제거 |
| Self-improving 루프 위험 | 완전 반영 | 조건 충족 후 점진적 활성화. 다양성 강제 로직 추가 |
| 관리자 1명 rubber stamping | 완전 반영 | 관리자 2명 + 명확한 검토 기준 수립 |
| 구현 기간 비현실적 | 완전 반영 | 4주 → Phase별 3~4주, 총 8~12주로 현실화 |
| 평가 메트릭 부재 | 완전 반영 | 학습효과/시스템품질/사용자행동 3가지 축 메트릭 정의 |
| 면접관 AI 효과성 미검증 | 반영 | 파일럿 10~20명으로 먼저 검증 후 확대 |

---

## 10. 미해결 문제 및 검토 요청

### 설계상 결정이 필요한 것

**1. 대화형 피드백 UX**
- 평가 결과 페이지에서 바로 대화가 시작되는가?
- 아니면 별도 채팅 화면으로 이동하는가?
- 대화 기록을 얼마나 유지할 것인가?

**2. 지식 그래프의 신뢰성**
- LLM으로 생성한 초안을 관리자 몇 명이 검토해야 충분한가?
- 틀린 선행 개념 정보가 학습 경로를 망치면 어떻게 감지하는가?

**3. MCP 에이전트 자율 선택의 실제 구현**
- 에이전트가 정말 상황에 따라 도구를 다르게 선택하는가?
- 아니면 결국 매번 둘 다 호출하게 되는가?

**4. 파일럿 10~20명 모집**
- 어떻게 모집할 것인가?
- 피드백을 어떻게 수집할 것인가?

### 비판적으로 검토해주셨으면 하는 부분
- 대화형 피드백이 실제로 초년생의 이해도를 높이는가?
- MCP를 학습 경로 추천에 쓰는 것이 실질적 이점이 있는가?
- Phase 1 → 2 → 3 순서가 올바른가? 바꿔야 할 게 있는가?
- 이 설계로 "좋은 제품 + 포트폴리오 어필" 두 가지를 동시에 달성할 수 있는가?

---

## 11. 포트폴리오 최종 정리

### 이력서 한 줄
```
"AI 교육 플랫폼에 멀티 에이전트 대화형 피드백 시스템 +
 MCP 기반 학습 경로 추천 엔진 설계 및 구현"
```

### 기술 면접 설명 포인트

**멀티 에이전트 선택 이유:**
"단일 LLM은 관점이 하나라 '왜 틀렸는지'에 대한 깊이 있는 대화가 어렵습니다.
 기술/실무/개선 관점을 분리해 병렬 실행하고 통합함으로써
 초년생이 실제로 이해할 수 있는 수준의 피드백을 만들었습니다."

**MCP 선택 이유:**
"학습 경로 추천에는 내부 DB와 지식 그래프 두 가지가 필요한데,
 상황에 따라 필요한 것만 조회해야 합니다.
 MCP로 에이전트가 도구를 자율 선택하게 해서
 불필요한 조회를 줄이고 응답 시간을 단축했습니다."

**트레이드오프 설명:**
"Self-improving 루프는 Bias Amplification 리스크 때문에
 데이터가 충분히 쌓인 후 점진적으로 활성화했습니다.
 다양성 강제 로직으로 특정 유형만 생성되는 Mode Collapse를 방지했습니다."

---

*이 문서는 Claude (Anthropic)와의 대화를 통해 작성되었습니다.*
*v1(기술 중심) → v2(완전 자동화) → v3(좋은 제품 + 포트폴리오 균형)으로 발전했습니다.*
*비판적 검토와 대안 제시를 환영합니다.*
